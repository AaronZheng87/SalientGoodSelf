---
title             : "Positive bias in perceptual matching may reflect an spontaneous self-referential processing"
shorttitle        : "Positive-bias as the spontaneous self-referential processing"

author: 
  - name          : "Hu Chuan-Peng"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Langenbeckstr. 1, Neuroimaging Center, University Medical Center Mainz, 55131 Mainz, Germany"
    email         : "hcp4715@gmail.com"
  - name          : "Kaiping Peng"
    affiliation   : "1"
  - name          : "Jie Sui"
    affiliation   : "1,3"

affiliation:
  - id            : "1"
    institution   : "Tsinghua University, 100084 Beijing, China"
  - id            : "2"
    institution   : "Leibniz Institute for Resilience Research, 55131 Mainz, Germany"
  - id            : "3"
    institution   : "University of Aberdeen, Aberdeen, Scotland"

authornote: |
  Hu Chuan-Peng, Department of Psychology, Tsinghua University, 100084 Beijing, China.
  Kaiping Peng, Department of Psychology, Tsinghua University, 100084 Beijing, China.
  Jie Sui, School of Psychology, University of Aberdeen, Aberdeen, Scotland.

  Authors contriubtion: HCP, JS, & KP design the study, HCP collected the data, HCP analyzed the data and drafted the manuscript. KP & JS supported this project.

abstract: |
  To navigate in a complex social world, individual has learnt to prioritize valuable information. Previous studies suggested the moral related stimuli was prioritized (Anderson, Siegel, et al., 2011, Science; Gantman & Van Bavel, 2014, Cognition). Using social associative learning paradigm, we found that when geometric shapes, without soical meaning, were associated with different moral valence (morally good, neutral, or bad), the shapes that associated with positive moral valence were prioritized in moral matching task. This patterns of results were robust across different procedures. Further, we tested whether this positive effect was modulated by self-relevance by manipulating the self-referential explicitly and found that the positive bias showed a large effect when positive valued stimuli were related to the self. This effect exist also when the self related information were presented as a task-irrelevant information. We also tested the specificity of the positive valence and found that this effect was not limited to moral domain. Interestingly, the better performance in reaction time is not correpsonding to self-rated psychological distance between self and a morally good-person, but with distance between self and morall bad-person. These results may suggest that our participants (College students in two different cities in China) have a positive moral self bias in perceptual processing, which drive the facilitated processing of morally good stimuli because of the spontaneous self-referential processing, and this trendency is not correlated with explicit rating of moral self.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Perceptual decision-making, Self, positive bias, morality"
wordcount         : "X"

bibliography      : 
  - r-references.bib
  - endnote.bib

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
figsintext        : no

documentclass     : "apa6"
classoption       : "man"
output            : 
  papaja::apa6_pdf:
    latex_engine  : xelatex

---

```{r setup, include = FALSE}
#rm(list = ls())
Sys.setlocale("LC_ALL", "English")  # set local encoding to English
Sys.setenv(LANG = "en") # set the feedback language to English
options(scipen = 999)   # force R to output in decimal instead of scientifc notion
options(digits=5)       # limit the number of reporting

pkgTest <- function(x)
{
        if (!require(x,character.only = TRUE))
        {
                install.packages(x,dep = TRUE)
                if(!require(x,character.only = TRUE)) stop("Package not found")
        }
}

pkgNeeded <- (c("tidyverse", 'metafor',
                "corrplot","readr", 
                "Hmisc",'mosaic', 'here'))

lapply(pkgNeeded,pkgTest)
rm('pkgNeeded') # remove the variable 'pkgNeeded';

#windowsFonts(Times=windowsFont("TT Times New Roman")) # explicit mapping to "times"
apatheme = theme_bw()+
        theme(panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.background = element_blank(),
              panel.border = element_blank(),
              text=element_text(family='Times'),
              legend.title=element_blank(),
              legend.text = element_text(size =12),
              #legend.position='top',
              plot.title = element_text(lineheight=.8, face="bold", size = 16),
#              plot.title = element_text(lineheight=.8, face="bold", size = 16, hjust = 0.5),
              axis.text = element_text (size = 14, color = 'black'),
#              axis.text.x = element_text(angle = 45, vjust = 0.5),   # x-axis's label font
              axis.title = element_text (size = 14),
              axis.line.x = element_line(color='black', size = 1),   # increase the size of font
              axis.line.y = element_line(color='black', size = 1),   # increase the size of font
              axis.title.x = element_text(margin=margin(10,0,0,0)),  # increase the sapce betwen title and x axis
              axis.title.y = element_text(margin=margin(0,12,0,0)))  # increase the space between title and y axis

curDir = here::here() #dirname(rstudioapi::getActiveDocumentContext()$path)
figDir = paste(curDir, '/figures', sep = '')

# Seed for random number generation
set.seed(42)
options(tinytex.verbose = T) # debug the tex
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

```

# Introduction
XXXX

# Methods
## Participants.
Most experiments (1a ~ 6b, except experiment 3b) reported in the current study were first finished between 2014 to 2016 in Tsinghua University, Beijing. Participants of these experiments were recruited in the local community. To increase the sample size so that each experiment has 50 or more valid data [@Simmons_2013_life], we recruited additional participants in Wenzhou University, Wenzhou, China in 2017 for experiment 1a, 1b, 4a, and 4b. Experiment 3b was finished in Wenzhou University in 2017. To have a better estimation of the effect size, we included the data from two experiments (experiment 7a, 7b) that were reported in @Hu_2020_GoodSelf (See Table 1 for overview of these experiments). 
All participant received informed consent and compensated for their time. These experiments were approved by the ethic board in the Department of Tsinghua University. 

 <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Design and Procedure
This series of experiments started to test the effect of instantly acquired moral valence on perceptual decision-making. For this purpose, we used the social associative learning paradigm (or self-tagging paradigm)[@Sui_2012_JEPHPP], in which participants first learned the associations between geometric shapes and labels of person with different moral valence (e.g., in first three studies, the triangle, square, and circle and good person, neutral person, and bad person, respectively). The associations of the shapes and label were counterbalanced across participants. After remembered the associations, participants finished a practice phase to familiar with the task, in which they viewed one of the shapes upon the fixation while one of the labels below the fixation and judged whether the shape and the label matched the association they learned. When participants reached 60% or higher accuracy at the end of the practicing session, they started the experimental task which was the same as in the practice phase. 
The experiment 1a, 1b, 1c, 2, and 6a shared a 2 (matchness: matched vs. mismatched) by 3 (moral valence: good vs. neutral vs. bad) within-subject design. The experiment 1a was the first one of the whole series studies and 1b, 1c, and 2 were conducted to exclude the potential confounding factors. More specifically, experiment 1b used different Chinese words as label to test whether the effect only occure with certain familiar words. Experiment 1c manipulated the moral valence indirectly: participants first learnt to associate different moral behaviors with different neutral names, after remembered the association, they then performed the perceptual matching task by associating names with different shapes. Experiment 2 further tested whether the way we presented the stimuli influence the effect of valence, by sequently presenting labels and shapes. Note that part of participants of experiment 2 were from experiment 1a because we originally planned a cross task comparison. Experiment 6a, which shared the same design as experiment 2, was an EEG experiment which aimed at exploring the neural correlates of the effect. But we will focus on the behavioral results of experiment 6a in the current manuscript.
For experiment 3a, 3b, 4a, 4b, 6b, 7a, and 7b, we included self-relevance as another within-subject variable in the experimental design. For example, the experiment 3a directly extend the desing of experiment 1a into a 2 (matchness: matched vs. mismatched) by 2 (reference: self vs. other) by 3 (moral valence: good vs. neutral vs. bad) within-subject design. Thus in experiment 3a, there were six conditions (good-self, neutral-self, bad-self, good-other, neutral-other, and bad-other) and six shapes (triangle, square, circle, diamond, pentagon, and trapezois). The experiment 6b was an EEG experiment extended from experiment 3a but presented the lable and shape sequentially. Because of the relatively high working memory load (six label-shape pairs), experiment 6b were conducted in two days: the first day participants finished perceptual matching task as a practice, and the second day, they finished the task again while the EEG signals were recorded. Experiment 3b was designed to separate the self-referential trials and other-referential trials. That is, participants finished two different blocks: in the self-referential blocks, they only response to good-self, neutral-self, and bad-self, with half of the trials was matched and half was not; for the other-reference blocks, they only reponded to good-other, neutral-other, and bad-other. Experiment 4a and 4b were design to test the automaticity of the binding between self/other and moral valence. In 4a, we used only two labels (self vs. other) and two shapes (circle, square). To manipulate the moral valence, we added labels within the shape and instructed participants to ignore the words in the shape during the task. In 4b, we reversed the role of self-relevance and valence in the task: participant learnt three labels (good-person, neutral-person, and bad-person) and three shapes (circle, square, and triangle), and the words for self-relevance, "self" or "other", were presented in the shapes. As in 4a, participants were told to ignore the words inside the shape during the task. Experiment 7a and 7b were designed to test the cross task robustness of the effect we observed in the aforementioned experiments [@Hu_2020_GoodSelf]. As we found that the neutral and bad conditions constantly show nonsignificant results, we only used two conditions of moral valence, i.e., good vs. bad, in experiment 7a and 7b.

Finally, experiment 5 was design to test the specificity of the moral valence. We extended experiment 1a with an additional independnet variable: domains of the valence words. More specifically, besides the moral valence, we also added valence from other domains: appearance of person (beautiful, neutral, ugly), apperance of a scene (beautiful, neutral, ugly), and emotion (happy, neutral, and sad). Label-shape pairs from different domains were separated into different blocks. 

E-prime 2.0 was used for presenting stimuli and collecting behavioral responses, except that experiment 7a and 7b used Matlab psychtoolbox [@Brainard_1997;@Pelli_1997]. For participants recruited in Tsinghua University, they finished the experiment individually in a dim-lighted chamber, stimuli were presented on 22-inch CRT monitors and their head were fixed by a chin-rest brace. The distance between participants' eyes and the screen was about 60 cm. The visual angle of geometric shapes was about 3.7º × 3.7º, the fixation cross is of (0.8º × 0.8º of visual angle) at the center of the screen. The words were of 3.6º × 1.6º visual angle. The distance between the center of the shape or the word and the fixation cross was 3.5º of visual angle. For participants recruited in Wenzhou University, they finished the experiment in a group consisted of 3 ~ 12 participants in a dim-lighted testing room. Participants were required to finished the whole experiment independently. Also, they were instructed to start the experiment at the same time, so that the distraction between participants were minimized. The stimuli were presented on 19-inch CRT monitor. The visual angles are could not be exactly controlled because participants’s chin were not fixed.

```{r 'Table1_exp_info', ehco = FALSE, results = 'asis'}
exp_table <- read.csv('Exp_info_all.csv') %>%
  dplyr::rename(ExpID = 1)
# knitr::kable(exp_table, caption = "Information about all experiments")
apa_table(
  exp_table
  , caption = "Information about all experiments."
  , note = "DV = dependent variables; Valence = how valence was manipulated; Shape & Label = how shapes & labels were presented."
  , escape = TRUE
)
```
In most of these experiments, participant were also asked to fill a battery of questionnaire after they finish the behavioral tasks. All the questionnaire data are open [see, dataset 4 in @Liu_2020_JOPD]. See Table 1 for a summary information about all the experiments reported here. 

## Data analysis
We reported all the measurements, analyses, and results in all the experiments in the current study. Participants whose overall accuracy lower than 60% were excluded from analysis. Also, the accurate responses with less than 200ms reaction times were excluded from the analysis.

All data were first pre-processed using `r cite_r("r-references.bib")`. We applied Bayesian hierarchical model to analyze both reaction times (RTs) and accuracy, the results from frequentist approach is available in the supplementary materials. 

We analyzed accuracy performanace using signal detection theory approach. The performance in each match condition was combined with that in the nonmatching condition with the same shape to form a measure of *d’*, the match trials were regarded as signal while the mismatching trials were regarded as noise [@Sui_2012_JEPHPP]. Given that the matching and mismatching trials are presented in the same way and had same number of trials across all studies, we assume that participants' inner distribution of these two types of trials had equal variance but may had different means. That is, we used the equal variance Gaussian SDT model (EVSDT) here []. Trials without response were excluded from the analysis. 

Previous, we used the maximum likelihood estimate of the EVSDT paramters, separately for each participant in the each experiment [@Hu_2020_GoodSelf, @Sui_2012_JEPHPP], and then used *d'* of each condition from each particiapnt as dependent variable for repeated measure ANOVAs. The *d'* is caculated as the difference of the standardized hit and false alarm rats [@Stanislaw_Todorov_1999]: 
$$ d' = zHR - zFR = \Phi^{-1}(HR) - \Phi^{-1}(FAR)$$
Here the standardized hit and false alarm rates as $zHR$ and $zFAR$, respectively. These two $z$-scores were converted from proportion (i.e., hit rate or false alarm rate) by inverse cumulative normal density function, $\Phi^{-1}$ ($\Phi$ is the cumulative normal density function, and is used convert $z$ score into probabilities). The response criterion $c$ is given by the negative standardized false alarm rate [@DeCarlo_1998]: $-zFAR$.

However, the maximum likelihood estimate of parameters of EVSDT may ignored the uncertainty in estimates of the parameters [@Rouder_2005_BHM_SDT]. Here we adopted a hierarchical Bayesian generalized linear model (GLM), implemented by BRMs [@Bürkner_2017, @(Bürkner 2017; @Carpenter_2017_stan] to model the data. 

In the GLM model, we assume that the outcome of each trial is Bernoulli distributed (binomial with 1 trial), with probability $p_{i}$ that $y_{i} = 1$. 

$$ y_{i} \sim Bernoulli(p_{i})$$
In a simple matching task, the probability $p_{i}$ can then be modelled as a function of the trial type:

$$ \Phi(p_{i}) =  \beta_{0} + \beta_{1}IsMatch_{i}$$
The outcomes $y_{i}$ are 0 if the participant responded "mismatch" on trial $i$, 1 if they responded "match". The probability of the "match" resonse for trial $i$ for a participant is $p_{i}$. We then write the generalized linear model on the probits (z-scores; $\Phi$, "Phi") of ps. $\Phi$ is the cumulative normal density function and maps $z$ scores to probabilities. Given this parameterization, the intercept of the mode ($\beta_0$) is going to be the standardized false alarm rate (probability of saying 1 when predictor is 0), which we take as our criterion $c$. The slope of the model ($\beta_1$) is the increase of saying 1 when predictor is 1, in $z$-scores,  which is another way of saying $d'$. Therefore, $c$ = -$z$HR = $-\beta_0$, and $d' = \beta_1$.

In each experiment, we had multiple participants, then we need also consider the variations between subjects, i.e., a hierachical mode in which individual's paramter and the the population level parameter are estimated simultaneously. We assume that the outcome of each trial is Bernoulli distributed (binomial with 1 trial), with probability $p_{ij}$ that $y_{ij} = 1$. 

$$ y_{ij} \sim Bernoulli(p_{ij})$$
Similarly, the generalized linear model was extended to two levels:
$$ \Phi(p_{ij}) =  \beta_{0j} + \beta_{1j}IsMatch_{ij}$$
The outcomes $y_{ij}$ are 0 if participant $j$ responded "mismatch" on trial $i$, 1 if they responded "match". The probability of the "match" resonse for trial $i$ for subject $j$ is $p_{ij}$. We again can write the generalized linear model on the probits (z-scores; $\Phi$, "Phi") of ps. 

The subjective-specific intercepts ($\beta_0 = -zFAR$) and slopes ($\beta_1 = d'$) are describe by multvariate normal with means and a covariance matrix for the parameters.
$$\begin{bmatrix}\beta_{0j}\\
\beta_{1j}\\
\end{bmatrix} \sim N(\begin{bmatrix}\theta_{0}\\
\theta_{1}\\
\end{bmatrix}, \sum)$$

We can further consider the participant is nested in each experiment, and each experiment, because of its variation in stimuli or stimuli presentation, may have different intercept and slope. In this case, we expand to a nested hierachical model:
$$ y_{ijk} \sim Bernoulli(p_{ijk})$$
the linear model
$$ \Phi(p_{ijk}) =  \beta_{0jk} + \beta_{1jk}IsMatch_{ijk}$$
The outcomes $y_{ijk}$ are 0 if participant $j$ in experiment k responded "mismatch" on trial $i$, 1 if they responded "match". 

$$\begin{bmatrix}\beta_{0jk}\\
\beta_{1jk}\\
\end{bmatrix} \sim N(\begin{bmatrix}\theta_{0k}\\
\theta_{1k}\\
\end{bmatrix}, \sum)$$

and the experiment level parameter $mu_{0k}$ and $mu_{1k}$ is from a higher order distribution:

$$\begin{bmatrix}\theta_{0k}\\
\theta_{1k}\\
\end{bmatrix} \sim N(\begin{bmatrix}\mu_{0}\\
\mu_{1}\\
\end{bmatrix}, \sum)$$
in which $mu_{0}$ and $mu_{1}$ means the population level parameter.

Using the Bayesian hierarchial model, we can directly estimate the over-all effect of valence on $d'$ across all experiments, instead of using a two-step approach where we first estimate the $d'$ for each participant and then use a random effect model meta-analysis [@Goh_2016_mini] to synthesize the overall. effect.

For the reaction time, there are many criticism about using the mean RTs as the representation of each participant [@Rousselet_2019], to better catpure a representative parameter for RTs, we used shifted log normal distribution (https://lindeloev.github.io/shiny-rt/#34_(shifted)_log-normal). This distribution has three parameters: shift, $\mu$, and $\sigma$. $\mu$ is the mean of the logNormal distribution, and $\sigma$ is the disperse of the distribution, while shift is the earliest possible response.

$$y_{i} = \beta_{0} + \beta_{1}*IsMatch_{i} * Valence_{i}$$

Shifted log-normal distriubtion:
$$ log(y_{ij} - \theta_{j}) \sim N(\mu_{j}, \sigma_{j})$$ 
$y_{ij}$ is the RT of the $i$th trial of the $j$th participants.

$$\mu_{j} \sim N(\mu, \sigma)$$

$$\sigma_{j} \sim Cauchy()$$

$$\theta_{j} \sim Cauchy()$$

This model can be easily expand to three-level model in which participants and experiments are two group level variable and participants were nested in the experiments.
$$ log(y_{ijk} - \theta_{jk}) \sim N(\mu_{jk}, \sigma_{jk})$$ 

$y_{ijk}$ is the RT of the $i$th trial of the $j$th participants in the $k$th experiment.

$$\mu_{jk} \sim N(\mu_{k}, \sigma_{k})$$

$$\sigma_{jk} \sim Cauchy()$$

$$\theta_{jk} \sim Cauchy()$$

$$\mu_{k} \sim N(\mu, \sigma)$$

We reported the synthesized results in five parts: valence effect, valence-self-relevance interaction, generalizability, specificity of valence effect, and behavior-questionnaire correlation analysis. The overall effect of each effect was estimated by hierarichal bayisan model. Note that to avoid the cases that some participants participated more than one experiments, we inspected the all available information of participants and only included participants' results from their first participation. As mentioned above, 24 participants were intentionally recruited to participate both exp 1a and exp 2, we only included their results from exp 1a in the meta-analysis.

### Valence effect
We synthesized effect size of *d* prime and RT from experiment 1a, 1b, 1c, 2, 5 and 6a for the valence effect. We reported the synthesized the effect across all experiments that tested the valence effect, using Bayesian hierarchical model. 

### Valence-self-relevance interaction
The results from experiment 3a, 3b, 6b, 7a, and 7b. These experiments explicitedly included both moral valence and self-reference. 

### Implicit coupling between valence and self-relevance
In the third part, we examined the change of effect size brought by change of design, with a focus on 4a and 4b, which were designed to examine the implicit effect of the interaction between moral valence and self-referential processing. We are interested in one particular question: will self-referential and morally positive valence had a mutual facilitation effect. That is, when moral valence (experiment 4a) or self-referential (experiment 4a) was presented as task-irrelevant stimuli, whether they would facilitate self-referential or valence effect on perceptual decision-making. For experiment 4a, we report the comparisons between different valence conditions under the self-referential task, not the other-referential task; for experiment 4b, we reported the comparison between the self- vs. other-referential conditions for positive moral condition, not for the neutral or negative conditions. Note that the results were also analyzed in a standard repeated measure ANOVAs (see supplementary materials).

### Specificity of the valence effect
In this part, we reported the data from experiment 5, which included positive, neutral, and negative valence from four different domains: morality, aesthetic of person, aesthetic of scence, and emotion. This experiment was design to test whether the positive bias is specific to morality.

### Behavior-Questionnaire correlation

Finally, we explored correlation between results from behavioral results and self-reported measures. 
For the behavioral task part, we derived different indices. First, we used the mean and SD of the RT data from each participants of each condition. We included the RT variation because it has been shown to be meaningful as individual differences [Jensen, 1992; Ouyang et al., 2017]. Second, we used drift diffusion model to estimate four parameters of DDM for each participants. 
The DDM analyses were finished by HDDM, as reported in Hu et al., (2019: https://psyarxiv.com/9fczh/). That is, we used the reponse code approach, matched response were coded as 1 and mismatched responses were coded as 0. To fully explore all parameters, we allow all four parameters of DDM free to vary. We then extracted the estimation of all the four parameters for each participants for the correlation analyses.

For the questinnaire part, we are most interested in the self-rated distance between different person and self-evaluation related questionnaires: self-esteem, moral-self identity, and moral self-image. Other questionnaires (e.g., personality) were not planned to correlated with behavioral data were not included. Note that all data were reported in [@Liu_2020_JOPD].

```{r loadingData,echo=FALSE,results='hide'}
load("AllData.RData")

### expclude the repeated subj from the raw data

# No repeating subj
df1a.v_meta <- df1a.v

# No repeating subj
df1b.v_meta <- df1b.v

# exclude participant from exp 1a
df1c.v_meta <- df1c.v %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210))

# exclude participant from exp 1a
df2.v_meta <- df2.v %>% dplyr::filter(Subject > 2000)    

# exclude participants from ex1b, 1c, and 2
df3a.v_meta <- df3a.v %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) 

# No repeating subj
df3b.v_meta <- df3b.v

# No repeating subj
df4a.v_meta <- df4a.v

# exclude participants from ex1b, 1c, and 2
df4b.v_meta <- df4b.v %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201))   

# exclude participants from ex1b, 1c, and 2
df5.v_meta <- df5.v %>% dplyr::filter(!Subject %in% c(5201))   

# exclude participants from ex1b, 1c, and 2
df6a.v_meta <- df6a.v %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131))   

# exclude participants from ex1b, 1c, and 2
df6b.v_meta <- df6b_d1.v %>% dplyr::filter(!Subject %in% c(6217))   

# exclude participants from ex1b, 1c, and 2
df7a.v_meta <- df7a_m.v %>% dplyr::filter(!Subject %in% c(7020))   

# No repeating subj
df7b.v_meta <- df7b_m.v

# remove all unnecessary variables
var_list <- c('df1a.v_meta', 'df1b.v_meta', 'df1c.v_meta', 'df2.v_meta', 'df3a.v_meta', 'df3b.v_meta',
              'df4a.v_meta', 'df4b.v_meta', 'df5.v_meta', 'df6a.v_meta', 'df6b.v_meta', 'df7a.v_meta', 'df7b.v_meta',
              'apatheme','exp_table', 'curDir', 'figDir')
rm(list=ls()[! ls() %in% var_list])

```

# Results
```{r first meta,echo=FALSE,results='hide'}

### try meta-analysis 1a, 1b, 1c, 2, 5 and 6a
#selected_columns <- c('Subject','Age', 'Sex')
df1a.v_meta$ExpID <- 'Exp1a'
df1b.v_meta$ExpID <- 'Exp1b'
df1c.v_meta$ExpID <- 'Exp1c'
df2.v_meta$ExpID <- 'Exp2'
df5.v_meta$ExpID <- 'Exp5'
df6a.v_meta$ExpID <- 'Exp6a'

selected_columns <- c('ExpID', 'Site', 'Subject','Age', 'Sex', 'Matchness','Valence', 'RESP', 'ACC','RT')
df_moral <- dplyr::bind_rows(df1a.v_meta[selected_columns],
                             df1b.v_meta[selected_columns],
                             df1c.v_meta[selected_columns],
                             df2.v_meta[selected_columns],
                             df5.v_meta[selected_columns],
                             df6a.v_meta[selected_columns]) %>%
  dplyr::mutate(ExpID_new = paste(ExpID, Site, sep = "_")) %>%
  dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good')))

df_moral_subj <- df_moral %>%
  dplyr::group_by(ExpID_new, Site) %>%
  dplyr::summarize(N = n_distinct(Subject))

df_moral <- df_moral %>%
  dplyr::filter(!is.na(RESP)) %>% # filter trials without response
  dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1,0),
                saymatch = ifelse((Matchness == 'Match' & ACC == 1) | (Matchness == 'Mismatch' & ACC == 0), 1,0)) %>%
  dplyr::select(ExpID_new, Subject, Valence, Matchness, RESP, ACC, ismatch, saymatch) %>%
  dplyr::mutate(ismatch_num = ifelse(Matchness == 'Match', 0.5, -0.5))

# plot the nested structure of the data
with(df_moral, table(Subject, ExpID_new)) %>%
  image(
    col = grey.colors(80, start = 1, end = 0), 
    axes = TRUE, 
    xlab = "Subject", 
    ylab = "ExpID"
  )

library(brms)
# fit the model for all valence effect, didn't specify the prior
std_val_m1 <- brms::brm(saymatch ~ 0 + Valence + Valence:ismatch_num + 
                         (0 + Valence + Valence:ismatch_num | ExpID_new) + 
                         (0 + Valence + Valence:ismatch_num  | ExpID_new:Subject),
                       family = bernoulli(link="probit"),
                       data = df_moral,
                       control = list(adapt_delta = .95),
                       cores = parallel::detectCores(),
                       file = here::here("glmmModels/sdt_val_EffectCode_3_level"))

summary(std_val_m1)
#stancode(fitglmm_6)

plot(hypothesis(std_val_m1,
                "ValenceBad:ismatch_num > ValenceNeutral:ismatch_num"))

plot(hypothesis(std_val_m1,
                "ValenceGood:ismatch_num > ValenceNeutral:ismatch_num"))

# Get the variables in the model
var_name_m1 <- tidybayes::get_variables(std_val_m1)

df_m1_posterior_pop_1 <- std_val_m1 %>% 
  tidybayes::spread_draws(b_ValenceBad) %>%
  dplyr::mutate(condition = 'Population',
                term = 'ValenceBad') %>%
  dplyr::rename(value = b_ValenceBad)

df_m1_posterior_pop_2 <- std_val_m1 %>% 
  tidybayes::spread_draws(b_ValenceNeutral) %>%
  dplyr::mutate(condition = 'Population',
                term = 'ValenceNeutral') %>%
  dplyr::rename(value = b_ValenceNeutral)

df_m1_posterior_pop_3 <- std_val_m1 %>% 
  tidybayes::spread_draws(b_ValenceGood) %>%
  dplyr::mutate(condition = 'Population',
                term = 'ValenceGood') %>%
  dplyr::rename(value = b_ValenceGood)

df_m1_posterior_pop_4 <- std_val_m1 %>% 
  tidybayes::spread_draws(`b_ValenceBad:ismatch_num`) %>%
  dplyr::mutate(condition = 'Population',
                term = 'ValenceBad:ismatch_num') %>%
  dplyr::rename(value = `b_ValenceBad:ismatch_num`)

df_m1_posterior_pop_5 <- std_val_m1 %>% 
  tidybayes::spread_draws(`b_ValenceNeutral:ismatch_num`) %>%
  dplyr::mutate(condition = 'Population',
                term = 'ValenceNeutral:ismatch_num') %>%
  dplyr::rename(value = `b_ValenceNeutral:ismatch_num`)

df_m1_posterior_pop_6 <- std_val_m1 %>% 
  tidybayes::spread_draws(`b_ValenceGood:ismatch_num`) %>%
  dplyr::mutate(condition = 'Population',
                term = 'ValenceGood:ismatch_num') %>%
  dplyr::rename(value = `b_ValenceGood:ismatch_num`)

df_m1_posterior_pop <- rbind(df_m1_posterior_pop_1, df_m1_posterior_pop_2, df_m1_posterior_pop_3, df_m1_posterior_pop_4, df_m1_posterior_pop_5, df_m1_posterior_pop_6) %>%
  dplyr::select(condition, term, value, everything())

df_m1_posterior_exp <- std_val_m1 %>% 
  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
  dplyr::rename(value = r_ExpID_new)

# combine the posterior for both exp and population
df_m1_posterior <- rbind(as.data.frame(df_m1_posterior_pop), as.data.frame(df_m1_posterior_exp))

pop_mean <- std_val_m1 %>%
  tidybayes::gather_draws(b_ValenceBad, b_ValenceNeutral, b_ValenceGood,
                          `b_ValenceBad:ismatch_num`, `b_ValenceNeutral:ismatch_num`, 
                          `b_ValenceGood:ismatch_num`) %>%
  #head(10)
  group_by(.variable) %>%    # this line not necessary (done automatically by spread_draws)
  tidybayes::mean_hdci(.value)

# extract the population level parameters
tmp <- std_val_m1 %>% 
  tidybayes::gather_draws(b_ValenceBad, b_ValenceNeutral, b_ValenceGood,
                          `b_ValenceBad:ismatch_num`, `b_ValenceNeutral:ismatch_num`, 
                          `b_ValenceGood:ismatch_num`) %>%
  dplyr::rename(term = .variable,
                pop_mean = .value) %>%
  #tidyr::separate(term, c(NA, 'term'), "_") 
  dplyr::ungroup() %>%
  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
tmp2 <- merge(tmp, df_m1_posterior_exp, by = c('term','.chain','.iteration', '.draw'), all = T) %>%
  dplyr::mutate(mean_value = pop_mean + value) %>%
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>%
  dplyr::rename(value = mean_value)

# Merge the group parameter with experimetal level data 
tmp3 <- tmp %>%
  dplyr::mutate(condition = 'Overall') %>%
  dplyr::rename(value = pop_mean) %>%
  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
  dplyr::bind_rows(., tmp2) %>%
  dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
                                                         "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
                                                         "Exp5_THU",  "Exp6a_THU","Overall")),
                condition = forcats::fct_rev(condition),
                term = mosaic::derivedFactor("c_bad" = (term == "ValenceBad"),
                                             "c_neutral" = (term == "ValenceNeutral"),
                                             "c_good" = (term == "ValenceGood"),
                                             "dprime_bad" = (term == "ValenceBad:ismatch_num"),
                                             "dprime_neutral" = (term == "ValenceNeutral:ismatch_num"),
                                             "dprime_good" = (term == "ValenceGood:ismatch_num"),
                                             .method ="first", .default = NA),
                term = factor(term, levels = c("c_bad", "c_neutral", "c_good",
                                               "dprime_bad", "dprime_neutral", "dprime_good")))

tmp3 %>% 
  dplyr::group_by(condition, term, .chain) %>%
  dplyr::tally()

# plot the posterior of c
tmp3 %>%
  dplyr::filter(str_detect(term, 'c_')) %>%
  ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x < -.09511|x > .06172))) +
  tidybayes::stat_halfeyeh() +
  geom_vline(xintercept = c(-0.09511, 0.06172), linetype = "dashed") +
  scale_fill_manual(values = c('gray80', 'skyblue')) + 
  facet_wrap( ~ term,
               scales = "free_y", nrow = 1,
               labeller = label_parsed)

# plot the posterior of dprime
tmp3 %>%
  dplyr::filter(str_detect(term, 'dprime_')) %>%
  ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x < 1.8309|x > 2.52905))) +
  tidybayes::stat_halfeyeh() +
  geom_vline(xintercept = c(1.8309, 2.52905), linetype = "dashed") +
  scale_fill_manual(values = c('gray80', 'skyblue')) + 
  facet_wrap( ~ term,
               scales = "free_y", nrow = 1,
               labeller = label_parsed)

# 
mcmc_plot(std_val_m1, pars = 1:4, type = "dens")

# posterior predictive check
pp_check(std_val_m1)
pp_std_val_m1 <- 
  brms::pp_check(std_val_m1, nsamples = 1e2) + 
  ggtitle("PPC std_val_m1") +
  theme_bw (base_size = 10) + 
  theme(legend.position = "none") +
  xlim(-0.5, 1.5)

```

```{r second meta,echo=FALSE,results='hide'}
# Results part 2: with self-referential, included experiments: 3a, 3b, 6b, 7a, 7b

# Combine the data  ----
df.meta_d_2 <- rbind(df3a.meta.d, df3b.meta.d, df6b.meta.d, df7a_m.meta.d, df7b_m.meta.d) 
df.meta_rt_2 <- rbind(df3a.meta.rt, df3b.meta.rt, df6b.meta.rt, df7a_m.meta.rt, df7b_m.meta.rt)

# Calculate the mean, sd, n, and r ----
# for estimating the effect size and SE of effect size.
effectList_2 <- c('Good_Bad_S','Good_Neut_S','Neut_Bad_S',
                'Good_Bad_O','Good_Neut_O','Neut_Bad_O')

df.ES_2 <- data.frame(matrix(, nrow=length(unique(df.meta_d_2$ExpID))*length(effectList_2)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df.meta_d_2$ExpID))*length(effectList_2)),
                ExpID  = rep(rep(unique(df.meta_d_2$ExpID), each = length(effectList_2)), 2),
                Effect = rep(effectList_2, length(unique(df.meta_d_2$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df.meta_rt_2 %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df.meta_d_2 %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_2){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Good_Bad_S'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Good_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Neut_Bad_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Bad_O'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Neut_Bad_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
      }
      
      M1  <- mean(dataCond1$Value) -> df.ES_2$M1[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_2$SD1[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_2$M2[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_2$SD2[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_2$N[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_2$r[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_2$ES[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] <- tmp2[1,1]
      df.ES_2$ES.var[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] <- tmp2[1,2]
    }
  }
}

# Do the meta ----
# info about participants
df.ES_2_sum <- df.ES_2 %>% 
  dplyr::group_by(DVtype, Effect) %>% 
  tidyr::drop_na() %>% 
  dplyr::summarise(Nexp = length(unique(ExpID)), Nsubj = sum(N, na.rm = T))

df.res.meta_2 <- data.frame(matrix(, nrow= (2*3)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = (2*3)),
                Effect = rep(effectList_2, 2),
                N_exp = NA, Cohen_d = NA, se = NA, CI_low = NA, CI_upp = NA, pval = NA)

# meta -analysis
for (DVName in c('RT','dprime')){
  for (effectName in effectList_2){
    df.res.meta <- df.ES_2 %>%
      dplyr::filter(DVtype == DVName & Effect == effectName) %>%
      tidyr::drop_na()
  
    tmp.meta.res <- metafor::rma(yi = df.res.meta$ES,
                           vi = df.res.meta$ES.var,
                           slab = df.res.meta$ExpID)
    df.res.meta_2$N_exp[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$k
    df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$beta
    df.res.meta_2$se[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$se
    df.res.meta_2$CI_low[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$ci.lb
    df.res.meta_2$CI_upp[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$ci.ub
    df.res.meta_2$pval[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$pval
  }
}

# plot the effect size  ----
df.res.meta_2 <- df.res.meta_2 %>%
  dplyr::mutate(Identity = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Neut_S" | Effect == "Neut_Bad_S",
                                  "Self-Ref.", "Other-Ref."),
                EffectType = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Bad_O", "Good_Bad",
                                    ifelse(Effect == "Good_Neut_S" | Effect == "Good_Neut_O", "Good_Neut", "Neut_Bad")))

df.res_meta_pdata <- rbind(df.res.meta_1, df.res.meta_2) %>%
  dplyr::mutate(Identity = factor(Identity, levels = c("No-Ref.", "Self-Ref.", "Other-Ref.")),
                EffectType = factor(EffectType, levels = c("Good_Bad", "Good_Neut", "Neut_Bad" )))

```

## Effect of moral valence

```{r plot-all-effect, fig.cap="Effect size (Cohen's *d*) of Valence.", fig.width=12, warning=FALSE}
#p_meta_val <- 
  df.res_meta_pdata %>%
  #dplyr::filter(Identity == "Self") %>%
  ggplot(., aes(x=DVtype, y=Cohen_d, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=Cohen_d - 1.96*se, ymax = Cohen_d + 1.96*se), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  #ggtitle('Valence effect') +
  #geom_bar(stat="identity", color=NA, 
  #         position=position_dodge()) +
  #geom_errorbar(aes(ymin=Cohen_d - 1.96*se, ymax = Cohen_d + 1.96*se), width=.2,
  #               position=position_dodge(.9)) +
  coord_cartesian(ylim=c(-1.5, 1.5))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  apatheme +
  facet_wrap( ~ Identity, nrow = 1)
```
In this part, we synthesized results from experiment 1a, 1b, 1c, 2, 5 and 6a. Data from 192 participants were included in these analysis. We found differences between positive and negative conditions on RT was Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']`]; on *d'* was Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']`]; *d'*: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']`]; *d'*: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']`]. See Figure \@ref(fig:plot-all-effect) left panel.

## Interaction between valence and self-reference
In this part, we combined the experiments that explicitly manipulated the self-reference and valence, which includes 3a, 3b, 6b, 7a, and 7b. For the positive versus negative contrast, data were from five experiments whith 178 participants; for positive versus neutral and neutral versus negative contrasts, data were from three experiments with 108 participants.

In most of these experiments, the interaction between self-reference and valence was signficant (see results of each experiment in supplementary materials). In the mini-meta-analysis, we analyzed the valence effect for self-referential condition and other-referential condition separately.

For the self-referential condition, we found the same pattern as in the first part of results. That is we found significant differences between positive and neutral as well as positive and negative, but not neutral and negative. The effect size of RT between positive and negative is Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']`]; on *d'* was Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']`]. See Figure \@ref(fig:plot-all-effect) the middle panel.

For the other-referential condition, we found that only the difference between positive and negative on RT was significant, all the other conditions were not. The effect size of RT between positive and negative is Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']`]; on *d'* was Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']`]. See Figure \@ref(fig:plot-all-effect) right panel.

## Generalizibility of the valence effect
In this part, we reported the results from experiment 4 in which either moral valence or self-reference were manipulated as task-irrelevant stimuli. 

```{r analyzing exp4a, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
df.ES_4a <- data.frame(matrix(, nrow=length(unique(df4a.meta.d$ExpID))*length(effectList_2)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df4a.meta.d$ExpID))*length(effectList_2)),
                ExpID  = rep(rep(unique(df4a.meta.d$ExpID), each = length(effectList_2)), 2),
                Effect = rep(effectList_2, length(unique(df4a.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df4a.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df4a.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_2){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Good_Bad_S'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Good_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Neut_Bad_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Bad_O'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Neut_Bad_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
      }
      
      M1  <- mean(dataCond1$Value) -> df.ES_4a$M1[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_4a$SD1[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_4a$M2[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_4a$SD2[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_4a$N[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_4a$r[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_4a$ES[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] <- tmp2[1,1]
      df.ES_4a$ES.var[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_4a <- df.ES_4a %>%
  dplyr::mutate(Identity = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Neut_S" | Effect == "Neut_Bad_S",
                                  "Self-ref.", "Other-ref."),
                EffectType = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Bad_O", "Good_Bad",
                                    ifelse(Effect == "Good_Neut_S" | Effect == "Good_Neut_O", "Good_Neut", "Neut_Bad")),
                Identity = factor(Identity, levels = c("Self-ref.", "Other-ref.")))

p_df_4a <- df.ES_4a %>%
  #dplyr::filter(Identity == "Self") %>%
  ggplot(., aes(x=DVtype, y=ES, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  ggtitle('A: Valence effect') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  apatheme +
  facet_wrap(~Identity, nrow = 1)

#p_df_4a
```

```{r 'plot_exp4a_effect', fig.cap="Effect size (Cohen's *d*) of Valence in Exp4a.", warning=FALSE}
#multiplot(p_meta_val_1, p_meta_val_2_self,p_meta_val_2_other, cols = 3)
p_df_4a
```
For exmperiment 4a, when self-reference was the target and moral valence was task-irrelevant, we found that only under the implicit self-referential condition, i.e., when the moral words were presented as task irrelevant stimuli, there was the main effect of valence and interaction between valence and reference for both *d* prime and RT (See supplementary resuls for the detailed statistics). For *d* prime, we found good-self condition (`r df.ES_4a$M1[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD1[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']`) had higher *d* prime than bad-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']`); good self condition was also higher than neutral self (`r df.ES_4a$M2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Neut_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Neut_S']`) but there was not statistically significant, while the neutral-self condition was higher than bad self condition and not significant neither. For reaction times, good-self condition (`r df.ES_4a$M1[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD1[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']`) were faster relative to bad-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']`), and over neutral-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Neut_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Neut_S']`). The difference between neutral-self and bad-self conditions were not significant. However, for the other-referential condition, there was no significant differences between different valence conditions.

```{r analyzing exp4b, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
effectList_3 <- c('Self_Other_G','Self_Other_N', 'Self_Other_B')

df.ES_4b <- data.frame(matrix(, nrow=length(unique(df4b.meta.d$ExpID))*length(effectList_3)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df4b.meta.d$ExpID))*length(effectList_3)),
                ExpID  = rep(rep(unique(df4b.meta.d$ExpID), each = length(effectList_3)), 2),
                Effect = rep(effectList_3, length(unique(df4b.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df4b.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df4b.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_3){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Self_Other_G'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Self_Other_N'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Self_Other_B'){
        if (!all(is.na(tmpdata$Identity)) & 'Bad' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }

      
      M1  <- mean(dataCond1$Value) -> df.ES_4b$M1[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_4b$SD1[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_4b$M2[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_4b$SD2[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_4b$N[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_4b$r[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_4b$ES[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] <- tmp2[1,1]
      df.ES_4b$ES.var[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_4b <- df.ES_4b %>%
  dplyr::mutate(Val = ifelse(Effect == "Self_Other_G", "Good",
                                  ifelse(Effect == "Self_Other_N", 'Neutral', 'Bad')),
                EffectType = 'Self_Other',
                Val = factor(Val, levels = c("Good", "Neutral", "Bad")))

p_df_4b_val <- df.ES_4b %>%
  #dplyr::filter(Identity == "Self") %>%
  ggplot(., aes(x=DVtype, y=ES, color=Val, fill=Val)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  ggtitle('Self-ref effect') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  apatheme

p_df_4b_val
```
For experiemnt 4b, when valence was the target and the reference was task-irrelevant, we found a strong valence effect (see supplementary results). In this experiment, the advantage of good-self conition can only be distangled by comparing the self-referential and other-referential conditions while controling the valence condition. We only found this modulation effect on RT. The RT of good-self (`r df.ES_4b$M1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']` $\pm$ `r df.ES_4b$SD1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']`) were faster relative to good-other condition (`r df.ES_4b$M2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']` $\pm$ `r df.ES_4b$SD2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']`), Cohen's *d* = `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']`, 95% CI[`r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'] -  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'])` `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'] +  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'])`]. However, neutral-self (`r df.ES_4b$M1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']` $\pm$ `r df.ES_4b$SD1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']`) were faster relative to good-other condition (`r df.ES_4b$M2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']` $\pm$ `r df.ES_4b$SD2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']`), Cohen's *d* = `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']`, 95% CI[`r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'] -  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'])` `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'] +  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'])`]. The difference between bad-self and bad-other was not significant. All the differences between self-referential and other-referential were not significant for *d* prime.

## Specificity of valence effect

```{r analyzing exp5, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
effectList_exp5 <- c('Good_Bad_Mrl','Good_Neut_Mrl','Neut_Bad_Mrl',
                     'Good_Bad_BP','Good_Neut_BP','Neut_Bad_BP',
                     'Good_Bad_BS','Good_Neut_BS','Neut_Bad_BS',
                     'Good_Bad_Emo','Good_Neut_Emo','Neut_Bad_Emo')

df.ES_5 <- data.frame(matrix(, nrow=length(unique(df5.meta.d$ExpID))*length(effectList_exp5)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df5.meta.d$ExpID))*length(effectList_exp5)),
                ExpID  = rep(rep(unique(df5.meta.d$ExpID), each = length(effectList_exp5)), 2),
                Effect = rep(effectList_exp5, length(unique(df4a.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df5.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df5.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_exp5){
      if (effectName == 'Good_Bad_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
      }
      
      else if (effectName == 'Good_Bad_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
      }
      
      else if (effectName == 'Good_Bad_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
      }
      
      else if (effectName == 'Good_Bad_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
        }
      
      M1  <- mean(dataCond1$Value) -> df.ES_5$M1[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_5$SD1[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_5$M2[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_5$SD2[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_5$N[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_5$r[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_5$ES[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] <- tmp2[1,1]
      df.ES_5$ES.var[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_5 <- df.ES_5 %>%
  dplyr::mutate(Domain = ifelse(Effect == "Good_Bad_Mrl" | Effect == "Good_Neut_Mrl" | Effect == "Neut_Bad_Mrl",
                                  "Mrl",
                                ifelse(Effect == "Good_Bad_BP" | Effect == "Good_Neut_BP" | Effect == "Neut_Bad_BP",
                                  "AP",
                                  ifelse(Effect == "Good_Bad_BS" | Effect == "Good_Neut_BS" | Effect == "Neut_Bad_BS",
                                  "AS", 'Emo'))),
                Domain = factor(Domain, levels = c("Mrl", "AP", "AS", "Emo")),
                EffectType = ifelse(Effect == "Good_Bad_Mrl" | Effect == "Good_Bad_BP"  | Effect == "Good_Bad_BS"  | Effect == "Good_Bad_Emo", "Pos_Neg",
                                    ifelse(Effect == "Good_Neut_Mrl" | Effect == "Good_Neut_BP" | Effect == "Good_Neut_BS" | Effect == "Good_Neut_Emo", "Pos_Neut", "Neut_Neg")),
                EffectType = factor(EffectType, levels = c("Pos_Neg", "Pos_Neut", "Neut_Neg")))

p_df_5_RT <- df.ES_5 %>%
  #dplyr::filter(DVtype == "RT") %>%
  ggplot(., aes(x=DVtype, y=ES, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  ggtitle('Valence effect across different domains') +
  coord_cartesian(ylim=c(-1.5, 1.5))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  facet_wrap( ~ Domain, ncol = 4) +
  apatheme 
```
In this part, we analyzed the results from experiment 5, which included positive, neutral, and negative valence from four different domains: morality, emotion, aesthetics of human, and aesthetics of scene. We found interaction between valence and domain for both *d* prime and RT (matched trials). A common pattern appeared in all four domains: each domain showed a binary results instead of gradian on both *d* prime and RT. For morality, aesthetics of human, and aesthetics of scene, the positive conditions had advantages over both neutral and negative conditions (greater *d* prime and faster RT), and neutral and negative conditions didn't differ from each other. But for the emotional stimuli, it was the positive and neutral had advantage over negative conditions, while positive and neutral conditions were not significantly different. See supplementary materials for detailed statistics. Also note that the effect size in moral domain is smaller than the aesthetic domains (beauty of people and beauty of scene).

## Correlation analyses
As the reliability of the quesetionnaire can be found in [@Liu_2020_JOPD]. Then we calculated the correlation between the data from behavioral task and the questionnaire data. 

For the behavioral task part, we derived different indices. First, we used the mean and SD of the RT data from each participants of each condition. We included the RT variation because it has been shown to be meaningful as individual differences [Jensen, 1992; Ouyang et al., 2017]. Second, we used drift diffusion model to estimate four parameters of DDM for each participants. Third, we also calculated the differences between different conditions (valence effect: good-self vs. bad-self, good-self vs. neutral-self, bad-self vs. neutral-self; good-other vs. bad-other, good-other vs. neutral-other, bad-other vs. neutral-other; Self-reference effect: good-self vs. good-other, neutral-self vs. neutral-other, bad-self vs. bad-other), as indexed by Cohen's d and se of Cohen's *d*.

The DDM analyses were finished by HDDM, as reported in Hu et al., (2019: https://psyarxiv.com/9fczh/). That is, we used the reponse code approach, matched response were coded as 1 and mismatched responses were coded as 0. To fully explore all parameters, we allow all four parameters of DDM free to vary. We then extracted the estimation of all the four parameters for each participants for the correlation analyses.

For the questinnaire part, we are most interested in the self-rated distance between different person and self-evaluation related questionnaires: self-esteem, moral-self identity, and moral self-image. Other questionnaires (e.g., personality) were not planned to correlated with behavioral data were not included.


```{r correlation analysis,echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# get data for valence effect ---- 
## mean RT, SD of RT, and d prime for data without reference
tmp1 <- df.meta_rt_1 %>%
  dplyr::filter(Matchness == "Match" & Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = c(RT, RT_SD))
  
tmp2 <- df.meta_d_1 %>%
  dplyr::filter(Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = dprime) %>%
  dplyr::rename(dprime_Good = Good,
                dprime_Neut = Neutral,
                dprime_Bad = Bad)
  
df.meta_1_wide <- merge(tmp1,tmp2); rm(tmp1,tmp2)

## paramters of HDDM for the data without reference
## read all the file name.
params.list <- list.files('.\\HDDM\\', pattern = '*_hddm_params.csv')
params.expname <- data.frame(params.list) %>%
  tidyr::separate(params.list, c('expName','B','C'),sep = '_') %>%
  dplyr::select(expName) %>%
  dplyr::pull()

df_hddm_ls_1 <- params.list[c(1:4, 9:10)]

#rm(df_hddm_param_1)
for (indx in 1:6){
  #expName_tmp <- strsplit(df_hddm_ls_1[indx], '[_]')[[1]][1]
  if (indx == 5 ){
    hddm_params_tmp <- read.csv(paste(".\\HDDM\\", df_hddm_ls_1[indx], sep = ''), header = TRUE, sep = ",",
                   stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
      dplyr::filter(domain == "Morality") %>%
      dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val) %>%
      dplyr::select(Subject, Matchness, Valence, knode_name, mean) %>%
      tidyr::drop_na() %>%
      tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
      dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
      tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')  
    
  } else {
    
    hddm_params_tmp <- read.csv(paste(".\\HDDM\\", df_hddm_ls_1[indx], sep = ''), header = TRUE, sep = ",",
                   stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
      dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val) %>%
      dplyr::select(Subject, Matchness, Valence, knode_name, mean) %>%
      tidyr::drop_na() %>%
      tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
      dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
      tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')   # %>%
      #dplyr::mutate(ExpID = 'Exp1a')
  }
  
  if (exists('df_hddm_param_1')) {
    df_hddm_param_1 <- rbind(df_hddm_param_1, hddm_params_tmp) 
  } else {
    df_hddm_param_1 <- hddm_params_tmp
  }
  
}

df.meta_1_wide <- merge(df.meta_1_wide, df_hddm_param_1) %>%
  dplyr::select(-c(Domain, Identity, Matchness))

## Get data for interaction between ID & Val ----
## mean RT, SD of RT, and d prime for data without reference
tmp1 <- df.meta_rt_2 %>%
  dplyr::filter(Matchness == "Match" & Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = c(RT, RT_SD))
  

tmp2 <- df.meta_d_2 %>%
  dplyr::filter(Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = dprime) %>%
  dplyr::rename(dprime_Good = Good,
                dprime_Neut = Neutral,
                dprime_Bad = Bad)
  
df.meta_2_wide <- merge(tmp1,tmp2); rm(tmp1,tmp2)

## paramters of HDDM 
## read all the file name.
df_hddm_ls_2 <- params.list[c(5, 6, 11:13)]

#rm(df_hddm_param_2)  # in case the variable exist in the env.
for (indx in 1:5){
  #expName_tmp <- strsplit(df_hddm_ls_1[indx], '[_]')[[1]][1]
  hddm_params_tmp <- read.csv(paste(".\\HDDM\\", df_hddm_ls_2[indx], sep = ''), header = TRUE, sep = ",",
                 stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
    dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val, Identity = id) %>%
    dplyr::select(Subject, Matchness, Identity, Valence, knode_name, mean) %>%
    tidyr::drop_na() %>%
    tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
    dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
    tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')   # %>%
    #dplyr::mutate(ExpID = 'Exp1a')
  if (indx == 4 | indx == 5){
    hddm_params_tmp <- hddm_params_tmp %>%
      dplyr::mutate(Neutral_a = NA,
                    Neutral_t = NA,
                    Neutral_v = NA) %>%
      dplyr::select(Subject, Identity, Bad_a, Good_a, Neutral_a, Bad_v, Good_v, Neutral_v, Bad_t, Good_t,
                    Neutral_t)
  }
  
  if (exists('df_hddm_param_2')) {
    df_hddm_param_2 <- rbind(df_hddm_param_2, hddm_params_tmp) 
  } else {
    df_hddm_param_2 <- hddm_params_tmp
  }
}

df.meta_2_wide <- merge(df.meta_2_wide, df_hddm_param_2) %>%
  dplyr::select(-c(Domain, Matchness))

df.meta_1_wide <- df.meta_1_wide %>%
  dplyr::mutate(Identity = NA) %>%
  dplyr::select(colnames(df.meta_2_wide))

df.meta_all_wide <- df.meta_2_wide %>%
  dplyr::filter(Identity == "Self") %>%
  rbind(df.meta_1_wide, .)

library(mosaic) # using this library for its derivedFactor function
# prepare questionnare data
df.scales <- read.csv(".\\Scale_data\\FADGS_dataset4_1_clean.csv",header = TRUE, sep = ",",
                 stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
  dplyr::mutate(expID = derivedFactor("Exp1a" = (expID == "exp1.0"), 
                                      "Exp1b" = (expID == "exp1.1"),
                                      "Exp3a" = (expID == "exp3"),
                                      "Exp3b" = (expID == "exp3.1"),
                                      "Exp4a" = (expID == "exp4.1"),
                                      "Exp4b" = (expID == "exp4.2"),
                                      "Exp5" = (expID == "exp5.2"),
                                      "Exp6b" = (expID == "exp6.2"),
                                      "Exp7a" = (expID == "exp7.1"),
                                      "Exp7b" = (expID == "exp7r"),
                                      "Exp_dpr" = (expID == "exp6"),
                                      .method ="first", .default = NA),
                expID = as.character(expID))

# intersection between participant from behavioral task and scales and get the data
subj.common <- intersect(df.scales$subjID, unique(df.meta_all_wide$Subject))  # 253

df.scales.v <- df.scales %>% dplyr::filter(subjID %in% subj.common) %>%
  dplyr::select_if(~sum(!is.na(.)) > 0) # remove columns that only have NA.

## get the questionnaire names
# Self-esteem
SlfEstNames <- c("SES1","SES2","SES3","SES4","SES5","SES6","SES7","SES8","SES9","SES10")

# moral identity
mrlIdNames <- c("morId_1","morId_2","morId_3","morId_4", "morId_5","morId_6",
                "morId_7","morId_8","morId_9","morId_10","morId_11","morId_12",
                "morId_13","morId_14","morId_15","morId_16")
mrlIdIntNames <- c("morId_1","morId_2","morId_5","morId_8", "morId_10","morId_11",
                   "morId_12","morId_13","morId_14")
mrlIdExtNames <- c("morId_3","morId_4", "morId_6", "morId_7", "morId_9","morId_10",
                   "morId_15", "morId_16")

# moral self images
mrlslfImgNames <- c("morSlfImg_1","morSlfImg_2","morSlfImg_3","morSlfImg_4",
                    "morSlfImg_5","morSlfImg_6","morSlfImg_7","morSlfImg_8","morSlfImg_9")

# personal distance
perDistNames <- c("SelfSelf", 
                  "SelfGood_1", "SelfGood_2", "SelfGood_3", "SelfGood_4",
                  "SelfNeut_1", "SelfNeut_2", "SelfNeut_3", "SelfNeut_4",
                  "SelfBad_1",  "SelfBad_2",  "SelfBad_3",  "SelfBad_4",
                  "SelfStra_1", "SelfStra_2", "SelfStra_3", "SelfStra_4",
                  "GoodNeut_1", "GoodNeut_2", "GoodNeut_3", "GoodNeut_4", 
                  "GoodBad_1",  "GoodBad_2",  "GoodBad_3",  "GoodBad_4",
                  "NeutBad_1",  "NeutBad_2",  "NeutBad_3",  "NeutBad_4")

# calculate the average score of each relevant scale
df.q_scores.v <- df.scales.v %>%
  dplyr::mutate(SlfEst = rowMeans(.[, SlfEstNames],na.rm = F),
                mrlIdInt = rowMeans(.[, mrlIdIntNames], na.rm = F),
                mrlIdExt = rowMeans(.[, mrlIdExtNames], na.rm = F),
                mrlslfImg = rowMeans(.[, mrlslfImgNames], na.rm = F),
                ) %>%
  dplyr::select(subjID, SlfEst, mrlIdInt, mrlIdExt, mrlslfImg)

df.perdist <- df.scales.v %>%
  dplyr::select(c(expID, subjID),perDistNames) %>%
  #dplyr::rowwise() %>%
  dplyr::mutate(sumRaw = rowMeans(.[3:31], na.rm = T),
                SelfSelfraw = SelfSelf,
                SelfGoodraw = rowMeans(.[grep("SelfGood", names(.))], na.rm = T),
                SelfNeutraw = rowMeans(.[grep("SelfNeut", names(.))], na.rm = T),
                SelfBadraw  = rowMeans(.[grep("SelfBad", names(.))], na.rm = T),
                SelfStraraw = rowMeans(.[grep("SelfStra", names(.))], na.rm = T),
                GoodNeutraw = rowMeans(.[grep("GoodNeut", names(.))], na.rm = T),
                GoodBadraw  = rowMeans(.[grep("GoodBad", names(.))], na.rm = T),
                NeutBadraw  = rowMeans(.[grep("NeutBad", names(.))], na.rm = T)) %>%
  dplyr::select(expID, subjID, sumRaw, SelfSelfraw, 
                SelfGoodraw, SelfNeutraw, SelfBadraw,
                SelfStraraw, GoodNeutraw, GoodBadraw, NeutBadraw) %>%
  dplyr::mutate(SelfSelf = SelfSelfraw/sumRaw,
                SelfGood = SelfGoodraw/sumRaw,
                SelfNeut = SelfNeutraw/sumRaw,
                SelfBad = SelfBadraw/sumRaw,
                SelfStra = SelfStraraw/sumRaw, 
                GoodNeut = GoodNeutraw/sumRaw, 
                GoodBad = GoodBadraw/sumRaw, 
                NeutBad = NeutBadraw/sumRaw) %>%
  dplyr::select(subjID, SelfSelf, SelfGood, SelfNeut, SelfBad,
                SelfStra, GoodNeut, GoodBad, NeutBad)

df.q_scores.v <- merge(df.q_scores.v, df.perdist)

## calculate correlation ----
df.corr <- merge(df.q_scores.v, df.meta_all_wide, by.x = 'subjID', by.y = 'Subject') %>%
  dplyr::select(-c(14:18)) %>%
  dplyr::select(-c(SelfSelf, SelfStra)) %>%
  dplyr::select(1:5, 8,6,7,10,9,11, 18:20, 24:26, 27:29, 12:17, 21:23) %>%
  dplyr::na_if("NaN")

#library(corrr)
res.cor <- df.corr %>%
  dplyr::select(-c(subjID)) %>%
  as.matrix(.) %>%
  #dplyr::select_if(~sum(!is.na(.)) > 0) %>%
  Hmisc::rcorr(.)

corrplot::corrplot(res.cor$r, method="color", sig.level = .2, order = "FPC")

```
We found that data from behavioral task are closely related, but not with self-reported questionnaire data.

# Discussion

# References
```{r create_r-references, echo=FALSE,results='hide'}
#r_refs(file = "r-references.bib"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
