---
title             : "Positivity bias in perceptual matching may reflect a spontaneous self-referential processing"
shorttitle        : "Positivity as spontaneous self-referential processing"

author: 
  - name          : "Hu Chuan-Peng"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Langenbeckstr. 1, Neuroimaging Center, University Medical Center Mainz, 55131 Mainz, Germany"
    email         : "hcp4715@gmail.com"
  - name          : "Kaiping Peng"
    affiliation   : "3"
  - name          : "Jie Sui"
    affiliation   : "3,4"

affiliation:
  - id            : "1"
    institution   : "TBA"
  - id            : "2"
    institution   : "Leibniz Institute for Resilience Research, 55131 Mainz, Germany"
  - id            : "3"
    institution   : "Tsinghua University, 100084 Beijing, China"
  - id            : "4"
    institution   : "University of Aberdeen, Aberdeen, Scotland"

authornote: |
  Hu Chuan-Peng, Leibniz Institute for Resilience Research (LIR).
  Kaiping Peng, Department of Psychology, Tsinghua University, 100084 Beijing, China.
  Jie Sui, School of Psychology, University of Aberdeen, Aberdeen, Scotland.

  Authors contriubtion: HCP, JS, & KP design the study, HCP collected the data, HCP analyzed the data and drafted the manuscript. KP & JS supported this project.

abstract: |
  To navigate in a complex social world, individual has learnt to prioritize valuable information. Previous studies suggested the moral related stimuli was prioritized [@gantman_moral_2014;@anderson_visual_2011]. Using social associative learning paradigm (self-tagging paradigm), we found that when geometric shapes, without soical meaning, were associated with different moral valence (morally good, neutral, or bad), the shapes that associated with positive moral valence were prioritized in a perceptual matching task. This patterns of results were robust across different procedures. Further, we tested whether this positive effect was modulated by self-relevance by manipulating the self-referential explicitly and found that this moral positivity effect only occured when the moral valence are self-relevant but evidence to support such effect when the moral valence are other-relevant is weak. We further found that this effect exist even when the self-relevance or the moral valence were presented as a task-irrelevant information, though the effect size become much smaller. We also tested whether the positivity effect only exist in moral domain and found that this effect was not limited to moral domain. Exploratory analyses on task-questionnaire relationship found that moral self-image score (how closely one feel they are to the ideal moral image of themselves) is positively correlated to the *d'* of morally positive condition in singal detection and the drift rate using DDM, while the self-esteem is negatively correlated with *d'* of neutral and morally negative conditions. These results suggest that the positive self prioritzation in perceptual decision-making may reflect ...
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Perceptual decision-making, Self, positive bias, morality"
wordcount         : "X"

bibliography      : 
  - r-references.bib
  - endnote.bib

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
figsintext        : no

documentclass     : "apa6"
classoption       : "man"
output            : 
  papaja::apa6_pdf:
    latex_engine  : xelatex

---
 <!-- This documents used Frequentists' approach -->
 
```{r setup, include = FALSE}
#rm(list = ls())
source('Initial.r')

curDir = here::here() # dirname(rstudioapi::getActiveDocumentContext()$path)
figDir = here::here('figures')

# Seed for random number generation
set.seed(42)
options(tinytex.verbose = T) # debug the tex
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction
XXXX
In perceptual matching, same is faster than different [@Krueger_1978; @Farell_1985].
Automatic processing [@Spruyt_de_Houwer_2017]

@van_zandt_comparison_2000: A comparison of two response time models applied to perceptual matching

Yakushijin, ReikoJacobs, Robert A (2020), Are People Successful at Learning Sequential Decisions on a Perceptual Matching Task?

Schooler, L. J., Shiffrin, R. M., & Raaijmakers, J. G. W. (2001). A Bayesian model for implicit effects in perceptual identification. Psychological Review, 108(1), 257–272. https://doi.org/10.1037/0033-295X.108.1.257

We reported results from eleven experiments. In first set of experiments, we found that shapes associated with morally positive person label were responded faster and more accurately. In the second set of experiment, we explore the potential role of good self in perceptual matching task and added one more independent variable, we found that the effect was mainly on good self. In the third part we tested whether the morality will automatically binds with person-relevance. Finally, we explore the correlation between behavioral task and questionnaire scores.

# Disclosures
We reported all the measurements, analyses, and results in all the experiments in the current study. Participants whose overall accuracy lower than 60% were excluded from analysis. Also, the accurate responses with less than 200ms reaction times were excluded from the analysis. To have a better overview of the effect reported in this series experiment, we reported the synthesized results in the main text and individual experiment in supplementary materials. 

All the experiments reported were not pre-registered. Most experiments (1a ~ 6b, except experiment 3b) reported in the current study were first finished between 2014 to 2016 in Tsinghua University, Beijing, China. Participants in these experiments were recruited in the local community. To increase the sample size of experiments to 50 or more [@Simmons_2013_life], we recruited additional participants in Wenzhou University, Wenzhou, China in 2017 for experiment 1a, 1b, 4a, and 4b. Experiment 3b was finished in Wenzhou University in 2017. To have a better estimation of the effect size, we included the data from two experiments (experiment 7a, 7b) that were reported in @Hu_2020_GoodSelf (See Table S1 for overview of these experiments). 

All participant received informed consent and compensated for their time. These experiments were approved by the ethic board in the Department of Tsinghua University. 

 <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->


```{r loadingData,echo=FALSE,results='hide'}
load("AllData.RData")
```

```{r define_funs,echo=FALSE,results='hide'}
# define a function to run the sdt GLMM for all exp with Matchness * Valence design
# for 1a, 1b, 1c, 2, 6a
fun_sdt_val <- function(exp_name) {
  df_name <- paste('df', exp_name, '.v', sep = '')
  m_name <- paste("glmmModels/exp", exp_name, "_sdt_m1_DummyCode", sep = '')
  df <- get(df_name)  # get the data by string
  
  m <- df %>%
  dplyr::filter(!is.na(RESP)) %>% # filter trials without response
  dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                saymatch = ifelse((Matchness == 'Match' & ACC == 1) | 
                                    (Matchness == 'Mismatch' & ACC == 0), 1, 0),
                Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
  brms::brm(saymatch ~ 0 + Valence + Valence:ismatch + 
              (0 + Valence + Valence:ismatch | Subject),
            family = bernoulli(link="probit"),
            data = .,
            control = list(adapt_delta = .99),
            iter = 4000,
            thin = 2,
            cores = parallel::detectCores(),
            file = here::here(m_name))
  return(m)
}

fun_plot_sdt_val <- function(m_sdt) {
    # extract c
    tmp_c <- m_sdt %>% 
      tidybayes::gather_draws(b_ValenceBad, b_ValenceNeutral, b_ValenceGood) %>%
      dplyr::rename(Valence = .variable, sdt_c = .value) %>% dplyr::ungroup() %>%
      dplyr::mutate(Valence = gsub("b_", "", Valence)) %>%
      dplyr::mutate(Valence = ifelse(stringr::str_detect(Valence, 'Bad'), 'Bad',
                                     ifelse(stringr::str_detect(Valence, 'Good'), 'Good', 'Neutral')))
    
    # dprime
    tmp_d <- m_sdt %>% 
      tidybayes::gather_draws(`b_ValenceBad:ismatch`, `b_ValenceNeutral:ismatch`, 
                              `b_ValenceGood:ismatch`) %>%
      dplyr::rename(Valence = .variable, sdt_d = .value) %>% dplyr::ungroup() %>%
      dplyr::mutate(Valence = gsub("b_", "", Valence)) %>%
      dplyr::mutate(Valence = ifelse(stringr::str_detect(Valence, 'Bad'), 'Bad',
                                     ifelse(stringr::str_detect(Valence, 'Good'), 'Good', 'Neutral')))
    
    # plot summaries with densities
    p_sdt_d_sum <- tmp_d %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      ggplot2::ggplot(aes(x = sdt_d, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(x = "sensitivity (d')", y = 'Posterior') +
      theme_classic()
    
    p_sdt_c_sum <- tmp_c %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      ggplot2::ggplot(aes(x = sdt_c, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(x = "criteria (c)", y = 'Posterior') +
      theme_classic()
    
    # plot comparison
    p_sdt_d <- tmp_d %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
      tidybayes::compare_levels(sdt_d, by = Valence) %>%
      ggplot2::ggplot(aes(x = sdt_d, y = Valence, fill = stat(x > 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(x = "sensitivity (d')", y = 'Comparison') +
      theme_classic()
    
    p_sdt_c <- tmp_c %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
      tidybayes::compare_levels(sdt_c, by = Valence) %>%
      ggplot2::ggplot(aes(x = sdt_c, y = Valence, fill = stat(x > 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(x = "criteria (c)", y = 'Comparison') +
      theme_classic()
    
    return(list(p_sdt_d_sum, p_sdt_c_sum, p_sdt_d, p_sdt_c))
}

# define a function to run the RT GLMM for all exp with Matchness * Valence design
fun_rt_val <- function(exp_name) {
  df_name <- paste('df', exp_name, '.v', sep = '')
  m_name <- paste("glmmModels/exp", exp_name, "_rt_m1_DummyCode", sep = '')
  df <- get(df_name)  # get the data by string
  m <- df %>%
    dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
    dplyr::filter(ACC == 1) %>%
    dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                  Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
    brms::brm(RT_sec ~ Valence*ismatch + (Valence*ismatch | Subject),
              family = shifted_lognormal(),
              data = ., control = list(adapt_delta = .99),
              iter = 4000,
              thin = 2,
              cores = parallel::detectCores(),
              file = here::here(m_name))
  return(m)
}

fun_plot_rt_val <- function(m_rt) {
    tmp_rt <- m_rt %>% 
      tidybayes::spread_draws(b_Intercept, b_ValenceBad, b_ValenceGood, 
                              b_ismatch,   `b_ValenceBad:ismatch`, `b_ValenceGood:ismatch`) %>%
      dplyr::mutate(Neut_MM = b_Intercept,
                    Bad_MM = Neut_MM + b_ValenceBad,
                    Good_MM = Neut_MM + b_ValenceGood,
                    Neut_M = Neut_MM + b_ismatch,
                    Bad_M = Neut_MM + b_ismatch + `b_ValenceBad:ismatch`,
                    Good_M = Neut_MM + b_ismatch + `b_ValenceGood:ismatch`) %>%
      dplyr::select(-contains('b_')) %>%
      tidyr::pivot_longer(cols = Neut_MM:Good_M,
                          names_to = 'cond',
                          values_to = 'logRT') %>%
      dplyr::mutate(RT = exp(logRT)*1000,
                    Matchness = dplyr::case_when(cond == 'Neut_MM' | cond == 'Bad_MM' | cond == 'Good_MM' ~ 'Mismatch',
                                                 cond == 'Neut_M'  | cond == 'Bad_M'  | cond == 'Good_M' ~ 'Match'),
                    Valence = dplyr::case_when(cond == 'Neut_MM' | cond == 'Neut_M' ~ 'Neutral',
                                               cond == 'Bad_MM'  | cond == 'Bad_M'  ~ 'Bad', 
                                               cond == 'Good_MM' | cond == 'Good_M' ~ 'Good'))
    p_exp1b_rt_m_sum <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      dplyr::filter(Matchness == 'Match') %>%
      ggplot2::ggplot(aes(x = RT, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(x = "RTs (Matching, ms)", y = 'Posterior') +
      theme_classic()
    p_exp1b_rt_mm_sum <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      dplyr::filter(Matchness == 'Mismatch') %>%
      ggplot2::ggplot(aes(x = RT, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(tag = 'D', x = "RTs (Mismatching, ms)", y = 'Posterior') +
      theme_classic()
    
    # plot comparison
    p_exp1b_rt_m <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
      dplyr::filter(Matchness == 'Match') %>%
      tidybayes::compare_levels(RT, by = Valence) %>%
      ggplot2::ggplot(aes(x = RT, y = Valence, fill = stat(x < 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(tag = 'C', x = "RTs (Matching, ms)", y = 'Comparison') +
      theme_classic()
    p_exp1b_rt_mm <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
      dplyr::filter(Matchness == 'Mismatch') %>%
      tidybayes::compare_levels(RT, by = Valence) %>%
      ggplot2::ggplot(aes(x = RT, y = Valence, fill = stat(x < 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(tag = 'D', x = "RTs (Mismatching, ms)", y = 'Comparison') +
      theme_classic()
    return(list(p_exp1b_rt_m_sum, p_exp1b_rt_mm_sum, p_exp1b_rt_m, p_exp1b_rt_mm))
}

```

# Part 1: Moral valence effect
In this part, we report five experiments that aimed at testing whether the instantly acquired association between shapes and good person would be prioritized in perceptual decision-making.

## Experiment 1a 
### Methods
#### Participants
`r df1a.T.basic$N` college students (`r df1a.T.basic$Nf` female, age = `r df1a.T.basic$Age_mean` $\pm$ `r df1a.T.basic$Age_sd` years) participated. `r df1a.T.basic$N_thu` of them were recruited from Tsinghua University community in 2014; `r df1a.T.basic$N_wzu` were recruited from Wenzhou University in 2017. All participants were right-handed except one, and all had normal or corrected-to-normal vision. Informed consent was obtained from all participants prior to the experiment according to procedures approved by the local ethics committees. `r nrow(df1a.excld.sub)` participant’s data were excluded from analysis because nearly random level of accuracy, leaving `r df1a.v.basic$N` participants (`r df1a.v.basic$Nf` female, age = `r df1a.v.basic$Age_mean` $\pm$ `r df1a.v.basic$Age_sd` years).

#### Stimuli and Tasks
Three geometric shapes were used in this experiment: triangle, square, and circle. These shapes were paired with three labels (bad person, good person or neutral person). The pairs were counterbalanced across participants. 

#### Procedure
This experiment had two phases. First, there was a brief learning stage. Participants were asked to learn the relationship between geometric shapes (triangle, square, and circle) and different person (bad person, a good person, or a neutral person). For example, a participant was told, “bad person is a circle; good person is a triangle; and a neutral person is represented by a square.” After participant remember the associations (usually in a few minutes), participants started a practicing phase of matching task which has the exact task as in the experimental task. 
In the experimental task, participants judged whether shape–label pairs, which were subsequently presented, were correct. Each trial started with the presentation of a central fixation cross for 500 ms. Subsequently, a pairing of a shape and label (good person, bad person, and neutral person) was presented for 100 ms. The pair presented could confirm to the verbal instruction for each pairing given in the training stage, or it could be a recombination of a shape with a different label, with the shape–label pairings being generated at random. The next frame showed a blank for 1100ms. Participants were expected to judge whether the shape was correctly assigned to the person by pressing one of the two response buttons as quickly and accurately as possible within this timeframe (to encourage immediate responding). Feedback (correct or incorrect) was given on the screen for 500 ms at the end of each trial, if no response detected, “too slow” was presented to remind participants to accelerate. Participants were informed of their overall accuracy at the end of each block. The practice phase finished and the experimental task began after the overall performance of accuracy during practice phase achieved 60%. 
For participants from the Tsinghua community, they completed 6 experimental blocks of 60 trials. Thus, there were 60 trials in each condition (bad-person match, bad-person nonmatch, good-person match, good-person nonmatch, neutral-person match, and neutral-person nonmatch). For the participants from Wenzhou University, they finished 6 blocks of 120 trials, therefore, 120 trials for each condition.

#### Data analysis
We analyzed accuracy performance using a signal detection theory approach. The performance in each match condition was combined with that in the nonmatch condition with the same shape to form a measure of $d'$, the match trials were regarded as signal while the nonmatch trials were regarded as noise [@Sui_2012_JEPHPP]. Given that the match and nonmatch trials are presented in the same way and had same number of trials across all studies, we assume that participants' inner distribution of these two types of trials had equal variance but may had different means. That is, we used the equal variance Gaussian SDT model (EVSDT) here [@Rouder_2005_BHM_SDT]. **Trials without response were excluded from the analysis.** 

We analyzed the *d* prime and reaction times using the generalized linear model (GLM). The GLM approach didn't assume a particular generative model of the data but using linear model to fit the data. We applied the traditional repeated measures ANOVA, which is a special case of GLM. We also reported results from Bayesian generalized linear model approach.

We also analyzed the accuracy and reaction times data using the drift-diffusion model (DDM), which combine the accuracy and reaction times together.


##### Classic NHST
In the classic NHST approach, we've used the maximum likelihood estimate of the EVSDT parameters (mainly, the sensitivity $d'$) separately for each participant in the each experiment condition [@Hu_2020_GoodSelf; @Sui_2012_JEPHPP], and then used the parameter of each condition from each participant as dependent variable for repeated measures ANOVA. The $d'$ is calculated as the difference of the standardized hit and false alarm rats [@Stanislaw_Todorov_1999]: 
$$ d' = zHR - zFAR = \Phi^{-1}(HR) - \Phi^{-1}(FAR) $$
where the $HR$ means hit rate and the $FAR$ mean false alarm rate. $zHR$ and $zFAR$ are the standardized hit rate and false alarm rates, respectively. These two $z$-scores were converted from proportion (i.e., hit rate or false alarm rate) by inverse cumulative normal density function, $\Phi^{-1}$ ($\Phi$ is the cumulative normal density function, and is used convert $z$ score into probabilities). Another parameter of signal detection theory, response criterion $c$, is defined by the negative standardized false alarm rate [@DeCarlo_1998]: $-zFAR$.

For the RTs, we used mean RTs of the accurate trials of each condition and subject for repeated measures ANOVA. 

The repeated measure ANOVA was done by `afex` package.

##### Bayesian hierarchical generalized linear model (GLM)
The maximum likelihood estimate of parameters of EVSDT may ignored the uncertainty in estimates of the parameters [@Rouder_2005_BHM_SDT]. Bayesian generalized linear model (GLM) can help to estimate the uncertainty of parameters. We used BRMs [@Bürkner_2017; @Carpenter_2017_stan] to model the data. 

In the GLM model, we assume that the outcome of each trial is Bernoulli distributed (binomial with 1 trial), with probability $p_{i}$ that $y_{i} = 1$. 

$$ y_{i} \sim Bernoulli(p_{i})$$
In the perceptual matching task, the probability $p_{i}$ can then be modeled as a function of the trial type:

$$ \Phi(p_{i}) =  \beta_{0} + \beta_{1}IsMatch_{i}$$
The outcomes $y_{i}$ are 0 if the participant responded "nonmatch" on trial $i$, 1 if they responded "match". The probability of the "match" response for trial $i$ for a participant is $p_{i}$. We then write the generalized linear model on the probits (z-scores; $\Phi$, "Phi") of $p$s. $\Phi$ is the cumulative normal density function and maps $z$ scores to probabilities. Given this parameterization, the intercept of the model ($\beta_0$) is the standardized false alarm rate (probability of saying 1 when predictor is 0), which we take as our criterion $c$. The slope of the model ($\beta_1$) is the increase of saying 1 when predictor is 1, in $z$-scores,  which is another expression of $d'$. Therefore, $c$ = -$z$HR = $-\beta_0$, and $d' = \beta_1$.

In each experiment, we had multiple participants, then we need also consider the variations between subjects, i.e., a hierarchical mode in which individual's parameter and the the population level parameter are estimated simultaneously. We assume that the outcome of each trial is Bernoulli distributed (binomial with 1 trial), with probability $p_{ij}$ that $y_{ij} = 1$. 

$$ y_{ij} \sim Bernoulli(p_{ij})$$
Similarly, the generalized linear model was extended to two levels:
$$ \Phi(p_{ij}) =  \beta_{0j} + \beta_{1j}IsMatch_{ij}$$
The outcomes $y_{ij}$ are 0 if participant $j$ responded "nonmatch" on trial $i$, 1 if they responded "match". The probability of the "match" response for trial $i$ for subject $j$ is $p_{ij}$. We again can write the generalized linear model on the probits (z-scores; $\Phi$, "Phi") of $p$s. 

The subjective-specific intercepts ($\beta_{0} = -zFAR$) and slopes ($\beta_{1} = d'$) are describe by multivariate normal with means and a covariance matrix for the parameters.
$$ \begin{bmatrix}\beta_{0j}\\
\beta_{1j}\\
\end{bmatrix} \sim N(\begin{bmatrix}\theta_{0}\\
\theta_{1}\\
\end{bmatrix}, \sum) $$

In the same vein, when trying to estimate the parameter across different experiments, we can further consider the participant is nested in different experiments, and each experiment, because of its variation in stimuli or stimuli presentation, may have different intercepts and slopes. In this case, we can use a nested hierarchical model to model all the experiment with similar design:
$$y_{ijk} \sim Bernoulli(p_{ijk})$$
where 
$$ \Phi(p_{ijk}) =  \beta_{0jk} + \beta_{1jk}IsMatch_{ijk}$$
The outcomes $y_{ijk}$ are 0 if participant $j$ in experiment k responded "nonmatch" on trial $i$, 1 if they responded "match". 

$$\begin{bmatrix}\beta_{0jk}\\
\beta_{1jk}\\
\end{bmatrix} \sim N(\begin{bmatrix}\theta_{0k}\\
\theta_{1k}\\
\end{bmatrix}, \sum)$$

and the experiment level parameter $mu_{0k}$ and $mu_{1k}$ is from a higher order distribution:

$$\begin{bmatrix}\theta_{0k}\\
\theta_{1k}\\
\end{bmatrix} \sim N(\begin{bmatrix}\mu_{0}\\
\mu_{1}\\
\end{bmatrix}, \sum)$$
in which $\mu_{0}$ and $\mu_{1}$ means the population level parameter.

Using the Bayesian hierarchical model, we can directly estimate the over-all effect of valence on $d'$ across all experiments with similar experimental design, instead of using a two-step approach where we first estimate the $d'$ for each participant and then use a random effect model meta-analysis [@Goh_2016_mini]. 

For the reaction time, there are many criticism about using the mean RTs as the representation of each participant [@Rousselet_2019], to better capture a representative parameter for RTs, we used the log normal distribution (https://lindeloev.github.io/shiny-rt/#34_(shifted)_log-normal). This distribution has two parameters: $\mu$, $\sigma$. $\mu$ is the mean of the logNormal distribution, and $\sigma$ is the disperse of the distribution. The log normal distribution can be extended to shifted log normal distribution, with one more parameter: shift, which is the earliest possible response.

$$y_{i} = \beta_{0} + \beta_{1}*IsMatch_{i} * Valence_{i}$$

Shifted log-normal distribution:
$$ log(y_{ij}) \sim N(\mu_{j}, \sigma_{j})$$ 
$y_{ij}$ is the RT of the $i$th trial of the $j$th participants.

$$\mu_{j} \sim N(\mu, \sigma)$$

$$\sigma_{j} \sim Cauchy()$$

This model can be easily expand to three-level model in which participants and experiments are two group level variable and participants were nested in the experiments.
$$ log(y_{ijk}) \sim N(\mu_{jk}, \sigma_{jk})$$ 

$y_{ijk}$ is the RT of the $i$th trial of the $j$th participants in the $k$th experiment.

$$\mu_{jk} \sim N(\mu_{k}, \sigma_{k})$$

$$\sigma_{jk} \sim Cauchy()$$

$$\theta_{jk} \sim Cauchy()$$

$$\mu_{k} \sim N(\mu, \sigma)$$

##### Hierarchical drift diffusion model (HDDM)
To further explore the psychological mechanism under perceptual decision-making, we used HDDM [@wiecki_hddm_2013] to model our RTs and accuracy data. We used the prior implemented in HDDM, that is, informative priors that constrains parameter estimates to be in the range of plausible values based on past literature [@matzke_psychological_2009] 
 <!-- Shall I write more about HDDM here, or just refer the interested reader to the paper? -->

### Results

```{r 'ex1a-dprime-rt', fig.cap="RT and *d* prime of Experiment 1a.", fig.height=4.5, fig.width=9, warning=FALSE}
Val_plot_NHST(df.rt = df1a.v.rt_m, df.d = df1a.v.dprime_l)
```

#### Classic analytical approach
##### *d* prime
Figure \@ref(fig:ex1a-dprime-rt) shows *d* prime and reaction times during the perceptual matching task. We conducted a single factor (valence: good, neutral, bad) repeated measure ANOVA. 

```{r 1a_dprime_NHST, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# anova for d prime with 2*2 design
df1a_dprime_anova <- afex::aov_ez('Subject','dprime',df1a.v.dprime_l, within = c('Valence'))
df1a_dprime_anova_apa <- df1a_dprime_anova %>% papaja::apa_print.afex_aov()

posthoc_1a_d <- emmeans::emmeans(df1a_dprime_anova, "Valence") # compare each valence for both self and other condition
#pairs(posthoc_1a_d)
```

```{r results='asis', echo = F, eval=F}
# knitr::kable(nice(df4b_dprime_anova), caption = "ANOVA of d prime")
papaja::apa_table(df1a_dprime_anova_apa$table
  , caption = "Repeated measure ANOVA of *d* prime from Experiment 1a"
  , note = "*d* prime was calculated in a signal detection way.")

```

We found the effect of Valence (`r df1a_dprime_anova_apa$full$Valence`). The post-hoc comparison with multiple comparison correction revealed that the shapes associated with Good-person (2.11, SE = 0.14) has greater *d* prime than shapes associated with Bad-person (1.75, SE = 0.14), *t*(50) = 3.304, *p* = 0.0049. The Good-person condition was also greater than the Neutral-person condition (1.95, SE = 0.16), but didn't reach statistical significant, *t*(50) = 1.54, *p* = 0.28. Neither the Neutral-person condition is significantly greater than the Bad-person condition, *t*(50) = 2.109, *p* = .098.

##### Reaction times
```{r 1a_RT_NHST, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
df1a_RT_anova <- afex::aov_ez('Subject','RT_m',df1a.v.rt_m, within = c('Matchness','Valence')) # using afex's function

df1a_RT_anova_apa <- df1a_RT_anova %>% papaja::apa_print.afex_aov()

df1a.v.rt_m1 <- df1a.v %>%
  dplyr::filter(ACC == 1 & Matchness == "Match") %>%
  dplyr::group_by(Site,Subject,Matchness, Valence) %>%
  dplyr::summarise(RT_m = mean(RT),
                   Ntrial = length(RT)) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(Valence = factor(Valence, levels = c("Good","Neutral","Bad")))

df1a_RT_anova_m <- afex::aov_ez('Subject','RT_m',df1a.v.rt_m1, within = c('Valence'))
df1a_RT_anova_m_apa <- df1a_RT_anova_m %>% papaja::apa_print.afex_aov()

df1a.v.rt_m2 <- df1a.v %>%
  dplyr::filter(ACC == 1 & Matchness == "Mismatch") %>%
  dplyr::group_by(Site,Subject,Matchness, Valence) %>%
  dplyr::summarise(RT_m = mean(RT),
                   Ntrial = length(RT)) %>%
  dplyr::ungroup()

df1a_RT_anova_nm <- afex::aov_ez('Subject','RT_m',df1a.v.rt_m2, within = c('Valence'))
df1a_RT_anova_nm_apa <- df1a_RT_anova_nm %>% papaja::apa_print.afex_aov()

posthoc_1a_rt <- emmeans::emmeans(df1a_RT_anova_m, "Valence") # compare each valence for both self and other condition
# pairs(posthoc_1a_rt)
```
We conducted 2 (Matchness: match v. nonmatch) by 3 (Valence: good, neutral, bad) repeated measure ANOVA. We found the main effect of Matchness (`r df1a_RT_anova_apa$full$Matchness`), main effect of valence (`r df1a_RT_anova_apa$full$Valence`), and interaction between Matchness and Valence (`r df1a_RT_anova_apa$full$Matchness_Valence`).

We then carried out two separate ANOVA for Match and Mismatched trials. For matched trials, we found the effect of valence `r df1a_RT_anova_m$full$Valence`. We further examined the effect of valence for both self and other for matched trials. We found that shapes associated with Good Person (684 ms, SE = 11.5) responded faster than Neutral (709 ms, SE = 11.5), *t*(50) = -2.265, *p* = 0.0702) and Bad Person (728 ms, SE = 11.7), *t*(50) = -4.41, *p* = 0.0002), and the Neutral condition was faster than the Bad condition, *t*(50) = -2.495, *p* = 0.0415). For non-matched trials, there was no significant effect of Valence (`r df1a_RT_anova_nm$full$Valence`).

#### Bayesian hierarchical GLM
##### d prime
```{r 1a_dprime_BGLM, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp1a_sdt_m1 <- fun_sdt_val('1a')

summary(exp1a_sdt_m1)    # check summary
# pp_check(exp1a_sdt_m1)  # posterior predictive check

hypothesis(exp1a_sdt_m1, "ValenceGood:ismatch > ValenceNeutral:ismatch")  # 0.97
hypothesis(exp1a_sdt_m1, "ValenceGood:ismatch > ValenceBad:ismatch")      # 1
hypothesis(exp1a_sdt_m1, "ValenceNeutral:ismatch > ValenceBad:ismatch")   # 0.91
hypothesis(exp1a_sdt_m1, "ValenceGood > ValenceNeutral")  # 0.82
hypothesis(exp1a_sdt_m1, "ValenceGood > ValenceBad")      # .95
hypothesis(exp1a_sdt_m1, "ValenceNeutral > ValenceBad")   # 0.81

exp1a_sdt_p <- fun_plot_sdt_val(exp1a_sdt_m1)
```
We fitted a Bayesian hierarchical GLM for signal detection theory approach. The results showed that when the shapes were tagged with labels with different moral valence, the sensitivity ($d'$) and criteria ($c$) were both influence. For the $d'$, we found that the shapes tagged with morally good person (2.46, 95% CI[2.21 2.72]) is greater than shapes tagged with moral bad (2.07, 95% CI[1.83 2.32]), $P_{PosteriorComparison} = 1$. Shape tagged with morally good person is also greater than shapes tagged with neutral person (2.23, 95% CI[1.95 2.49]), $P_{PosteriorComparison} = 0.97$. Also, the shapes tagged with neutral person is greater than shapes tagged with morally bad person, $P_{PosteriorComparison} = 0.92$. 

Interesting, we also found the criteria for three conditions also differ, the shapes tagged with good person has the highest criteria (-1.01, [-1.14 -0.88]), followed by shapes tagged with neutral person(-1.06, [-1.21 -0.92]), and then the shapes tagged with bad person(-1.11, [-1.25 -0.97]). However, pair-wise comparison showed that only showed strong evidence for the difference between good and bad conditions.

##### Reaction times
```{r 1a_rt_BGLM, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# fit a three-level hierarchical model for RT, didn't specify the prior
exp1a_rt_m1 <- fun_rt_val('1a')
#plot(exp1a_rt_m1, "b_")
summary(exp1a_rt_m1)  # n
# pp_check(exp1a_rt_m1)

# Population-Level Effects: 
#                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept              -0.30      0.02    -0.33    -0.26 1.02      288      642  # baseline: mismatch:neutral = -.3
# ValenceBad              0.00      0.01    -0.01     0.01 1.00     3611     3116  # mismatch:neutral + mismatch:bad  = -.3
# ValenceGood             0.00      0.01    -0.01     0.01 1.00     2749     2691  # mismatch:neutral + mismatch:Good = -.3
# ismatch                -0.07      0.01    -0.09    -0.06 1.00     2203     2371  # mismatch:neutral + ismatch = (-0.30) + (-0.07) = -0.37
# ValenceBad:ismatch      0.02      0.01     0.00     0.05 1.00     2451     2495  # mismatch:neutral + ismatch + match:bad  = (-0.30) + (-0.07) + 0.02 = -0.35
# ValenceGood:ismatch    -0.04      0.02    -0.07    -0.01 1.00     1911     2326  # mismatch:neutral + ismatch + match:good = (-0.30) + (-0.07) +-0.04 = -0.41

hypothesis(exp1a_rt_m1, "ismatch < 0")  # Effect of matchness: Match < mis-match, p = 1
hypothesis(exp1a_rt_m1, "ValenceGood:ismatch < 0")  # Match good < Match Neutral, p = 0.99
hypothesis(exp1a_rt_m1, "ValenceBad:ismatch > 0")   # Match Bad > Match Neutral, p = 0.99
hypothesis(exp1a_rt_m1, "(ValenceGood:ismatch - ValenceBad:ismatch) < 0")   # Match Good < Match Bad, p = 1

exp1a_rt_p <- fun_plot_rt_val(exp1a_rt_m1)
```

```{r plot-exp1a-BGLM, fig.cap="Exp1a: Results of Bayesian GLM analysis.", fig.height=4.5, fig.width=9, warning=FALSE}
library(patchwork)
exp1a_sdt_p[[1]] + exp1a_sdt_p[[2]] + exp1a_sdt_p[[3]] + exp1a_sdt_p[[4]] + exp1a_rt_p[[1]] + exp1a_rt_p[[2]] + exp1a_rt_p[[3]] + exp1a_rt_p[[4]] + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 4, byrow = FALSE)
```

We fitted a Bayesian hierarchical GLM for RTs, with a log-normal distribution as the link function. We used the posterior distribution of the regression coefficient to make statistical inferences. As in previous studies, the matched conditions are much faster than the mismatched trials ($P_{PosteriorComparison} = 1$). We focused on matched trials only, and compared different conditions: Good is faster than the neutral, $P_{PosteriorComparison} = .99$, it was also faster than the Bad condition, $P_{PosteriorComparison} = 1$. And the neutral condition is faster than the bad condition, $P_{PosteriorComparison} = .99$. However, the mismatched trials are largely overlapped. See Figure \@ref(fig:plot-exp1a-BGLM).

#### HDDM

```{r plot-exp1a-HDDM, fig.cap="Exp1a: Results of HDDM.",  fig.height=4.5, fig.width=9, warning=FALSE}
df1a.hddm.group.trace <- read_csv(here::here('HDDM','df1a_group_traces.csv'))

params_p <- df1a.hddm.group.trace %>%
  dplyr::mutate(sample = 1:nrow(.)) %>%
  dplyr::select(chain, sample, contains('Match') | contains('Mismatch')) %>%
  tidyr::pivot_longer(.,`a(Match.Bad)`:`t(Mismatch.Neutral)`, names_to = 'conditions', values_to = 'value') %>%
  tidyr::separate(., conditions, into = c('v1', 'valence'), sep= '[.]') %>%       # split into two part
  tidyr::separate(., v1, into = c('param', 'matchness'), sep = '[(]') %>%         # further split the first half into two part
  dplyr::mutate(valence = stringr::str_sub(.$valence, start = 1, end = -2)) %>%   # remove the last two elements ') ' from the strings
  dplyr::arrange(., param) %>%
  tidyr::pivot_wider(., id_cols = c('chain', 'sample', 'matchness', 'valence'), names_from = 'param', values_from = 'value')

params_p %>% 
  dplyr::mutate(valence = factor(valence, levels = c("Good", "Neutral", "Bad")),
                matchness = ifelse(matchness == 'Mismatch', 'Nonmatch', matchness)) %>%
  ggplot2::ggplot(., aes(x = v, y = a, group = valence, color = valence)) +
  geom_point() + 
  scale_colour_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  facet_grid(~ matchness) +
  ylab(expression(paste("Boundary separation ",italic("a"), sep = ' '))) +
  xlab(expression(paste("Drift rate ",italic("v"), sep = ' '))) +
  theme_bw()+
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          panel.border = element_blank(),
          text=element_text(family='Times'),
          legend.title=element_blank(),
          legend.text = element_text(size =16),
          plot.title = element_text(lineheight=.8, face="bold", size = 18, margin=margin(0,0,20,0)),
          axis.text = element_text (size = 16, color = 'black'),
          axis.title = element_text (size = 16),
          #axis.title.x = element_blank(),
          #axis.title.y = element_blank(),
          axis.line.x = element_line(color='black', size = 1),    # increase the size of font
          axis.line.y = element_line(color='black', size = 1),    # increase the size of font
          strip.text = element_text (size = 16, color = 'black'), # size of text in strips, face = "bold"
          panel.spacing = unit(3, "lines")
    ) 
```

We fitted our data with HDDM, using the response-coding [See also, @Hu_2020_GoodSelf]. We estimated separate drift rate ($v$), non-decision time ($T_{0}$), and boundary separation ($a$) for each condition. We found that the shapes tagged with good person has higher drift rate and higher boundary separation than shapes tagged with both neutral and bad person. Also, the shapes tagged with neutral person has a higher drift rate than shapes tagged with bad person, but not for the boundary separation. Finally, we found that shapes tagged with bad person had longer non-decision time (see Figure \@ref(fig:plot-exp1a-HDDM)).


## Design and Procedure
This series of experiments started to test the effect of instantly acquired moral valence on perceptual decision-making. For this purpose, we used the social associative learning paradigm (or tagging paradigm)[@Sui_2012_JEPHPP], in which participants first learned the associations between geometric shapes and labels of person with different moral valence (e.g., in first three studies, the triangle, square, and circle and good person, neutral person, and bad person, respectively). The associations of the shapes and label were counterbalanced across participants. After remembered the associations, participants finished a practice phase to familiar with the task, in which they viewed one of the shapes upon the fixation while one of the labels below the fixation and judged whether the shape and the label matched the association they learned. When participants reached 60% or higher accuracy at the end of the practicing session, they started the experimental task which was the same as in the practice phase. 

The experiment 1a, 1b, 1c, 2, and 6a shared a 2 (matching: match vs. nonmatch) by 3 (moral valence: good vs. neutral vs. bad) within-subject design. Experiment 1a was the first one of the whole series studies and 1b, 1c, and 2 were conducted to exclude the potential confounding factors. More specifically, experiment 1b used different Chinese words as label to test whether the effect only occurred with certain familiar words. Experiment 1c manipulated the moral valence indirectly: participants first learned to associate different moral behaviors with different neutral names, after remembered the association, they then performed the perceptual matching task by associating names with different shapes. Experiment 2 further tested whether the way we presented the stimuli influence the effect of valence, by sequentially presenting labels and shapes. Note that part of participants of experiment 2 were from experiment 1a because we originally planned a cross task comparison. Experiment 6a, which shared the same design as experiment 2, was an EEG experiment which aimed at exploring the neural correlates of the effect. But we will focus on the behavioral results of experiment 6a in the current manuscript.

For experiment 3a, 3b, 4a, 4b, 6b, 7a, and 7b, we included self-reference as another within-subject variable in the experimental design. For example, the experiment 3a directly extend the design of experiment 1a into a 2 (matchness: match vs. nonmatch) by 2 (reference: self vs. other) by 3 (moral valence: good vs. neutral vs. bad) within-subject design. Thus in experiment 3a, there were six conditions (good-self, neutral-self, bad-self, good-other, neutral-other, and bad-other) and six shapes (triangle, square, circle, diamond, pentagon, and trapezoids). The experiment 6b was an EEG experiment extended from experiment 3a but presented the label and shape sequentially. Because of the relatively high working memory load (six label-shape pairs), experiment 6b were conducted in two days: the first day participants finished perceptual matching task as a practice, and the second day, they finished the task again while the EEG signals were recorded. Experiment 3b was designed to separate the self-referential trials and other-referential trials. That is, participants finished two different blocks: in the self-referential blocks, they only responded to good-self, neutral-self, and bad-self, with half match trials and half non-match trials; for the other-reference blocks, they only responded to good-other, neutral-other, and bad-other. Experiment 7a and 7b were designed to test the cross task robustness of the effect we observed in the aforementioned experiments [see, @Hu_2020_GoodSelf]. The matching task in these two experiments shared the same design with experiment 3a, but only with two moral valence, i.e., good vs. bad. We didn't include the neutral condition in experiment 7a and 7b because we found that the neutral and bad conditions constantly showed non-significant results in experiment 1 ~ 6.

Experiment 4a and 4b were design to test the automaticity of the binding between self/other and moral valence. In 4a, we used only two labels (self vs. other) and two shapes (circle, square). To manipulate the moral valence, we added the moral-related words within the shape and instructed participants to ignore the words in the shape during the task. In 4b, we reversed the role of self-reference and valence in the task: participant learnt three labels (good-person, neutral-person, and bad-person) and three shapes (circle, square, and triangle), and the words related to identity, "self" or "other", were presented in the shapes. As in 4a, participants were told to ignore the words inside the shape during the task. 

Finally, experiment 5 was design to test the specificity of the moral valence. We extended experiment 1a with an additional independent variable: domains of the valence words. More specifically, besides the moral valence, we also added valence from other domains: appearance of person (beautiful, neutral, ugly), appearance of a scene (beautiful, neutral, ugly), and emotion (happy, neutral, and sad). Label-shape pairs from different domains were separated into different blocks. 

E-prime 2.0 was used for presenting stimuli and collecting behavioral responses, except that experiment 7a and 7b used Matlab Psychtoolbox [@Brainard_1997;@Pelli_1997]. For participants recruited in Tsinghua University, they finished the experiment individually in a dim-lighted chamber, stimuli were presented on 22-inch CRT monitors and their head were fixed by a chin-rest brace. The distance between participants' eyes and the screen was about 60 cm. The visual angle of geometric shapes was about $3.7^\circ × 3.7^\circ$, the fixation cross is of ($0.8^\circ × 0.8^\circ$ of visual angle) at the center of the screen. The words were of $3.6^\circ$ × $1.6^\circ$ visual angle. The distance between the center of the shape or the word and the fixation cross was $3.5^\circ$ of visual angle. For participants recruited in Wenzhou University, they finished the experiment in a group consisted of 3 ~ 12 participants in a dim-lighted testing room. Participants were required to finished the whole experiment independently. Also, they were instructed to start the experiment at the same time, so that the distraction between participants were minimized. The stimuli were presented on 19-inch CRT monitor. The visual angles are could not be exactly controlled because participants’s chin were not fixed.

In most of these experiments, participant were also asked to fill a battery of questionnaire after they finish the behavioral tasks. All the questionnaire data are open [see, dataset 4 in @Liu_2020_JOPD]. See Table S1 for a summary information about all the experiments. 

## Data analysis

### Analysis of individual study
The individual experiment's results were reported in supplementary materials. We used the `tidyverse` of r (see script `Load_save_data.r`) to exclude the practicing trials, invalid trials of each participants, and invalid participants, if there were any, in the raw data. 

Results of each experiment were analyzed as in @Sui_2012_JEPHPP. That is, the accuracy performance using a signal detection approach, in which the performance in each match condition was combined with that in the nonmatch condition with the same shape to form a measure of $d'$ . Trials without response were coded either as “miss” (match trials) or “false alarm” (nonmatch trials). For the reaction times (RTs), only RTs of accurate trials were  analyzed. 

Both signal detection theory analysis of accuracy and RTs were analyzed in Frequentists' approach and Bayesian approach. In the Frequentists' approach, we calculated the $d'$ using Maximum Likelihood approach and then subjected the $d'$ estimated from each participant to repeated measures analyses of variance (repeated measures ANOVA), via `afex` []; we used the mean RTs of each participant in each condition and subject these mean value to repeated measures ANOVA too. We reported the results from significance test and effect sizes (including 95% confidence intervals). To control the false positive rate when conducting the post-hoc comparisons, we used Bonferroni correction. 

In the Bayesian approach, we used `brms` [@Bürkner_2017; @Carpenter_2017_stan] to implement the Bayesian hierarchical generalized linear model to estimate the effect of valence and self-referential. See supplementary materials for the results of each experiment's method and results. 

Finally, we also explored the psychological processes during the perceptual decision-making using the drift-diffusion model (DDM). We used HDDM [@wiecki_hddm_2013] for this purpose.

### Synthesized results
We reported the synthesized results from the experiments, because many of them shared the similar experimental design. We reported the results in five parts: valence effect, explicit interaction between valence and self-relevance, implicit interaction between valence and self-relevance, specificity of valence effect, and behavior-questionnaire correlation. 

For the first two parts, we reported the synthesized results from Frequestist's approach[mini-meta-analysis, @Goh_2016_mini]. The mini meta-analyses were carried out by using `metafor` package (Viechtbauer, 2010). We first calculated the mean of  $d'$  and RT of each condition for each participant, then calculate the effect size (Cohen's  $d$ ) and variance of the effect size for all contrast we interested: Good v. Bad, Good v. Neutral, and Bad v. Neutral for the effect of valence, and self vs. other for the effect of self-relevance. Cohen's $d$ and its variance were estimated using the following formula [@Cooper_2009_handbook]:

$$d = \frac {(M_{1} - M_{2})}{\sqrt {(sd_{1}^2 + sd_{2}^2) - 2rsd_{1}sd_{2}}}  \sqrt {2(1-r)}$$

$$var.d = 2 (1-r)  (\frac{1}{n} + \frac{d^2}{2n})$$

$M_1$ is the mean of the first condition, $sd_1$ is the standard deviation of the first condition, while $M_2$ is the mean of the second condition, $sd_2$ is the standard deviation of the second condition. $r$ is the correlation coefficient between data from first and second condition. $n$ is the number of data point (in our case the number of participants included in our research).

The effect size from each experiment were then synthesized by random effect model using `metafor` (Viechtbauer, 2010). Note that to avoid the cases that some participants participated more than one experiments, we inspected the all available information of participants and only included participants' results from their first participation. As mentioned above, 24 participants were intentionally recruited to participate both exp 1a and exp 2, we only included their results from experiment 1a in the meta-analysis.

### Valence effect
We synthesized effect size of $d'$ and RT from experiment 1a, 1b, 1c, 2, 5 and 6a for the valence effect. We reported the synthesized the effect across all experiments that tested the valence effect, using the mini meta-analysis approach [@Goh_2016_mini]. 

### Explicit interaction between Valence and self-relevance
The results from experiment 3a, 3b, 6b, 7a, and 7b. These experiments explicitly included both moral valence and self-reference. 

### Implicit interaction between valence and self-relevance
In the third part, we focused on experiment 4a and 4b, which were designed to examine the implicit effect of the interaction between moral valence and self-referential processing. We are interested in one particular question: will self-referential and morally positive valence had a mutual facilitation effect. That is, when moral valence (experiment 4a) or self-referential (experiment 4a) was presented as task-irrelevant stimuli, whether they would facilitate self-referential or valence effect on perceptual decision-making. For experiment 4a, we reported the comparisons between different valence conditions under the self-referential task and other-referential task. For experiment 4b, we first calculated the effect of valence for both self- and other-referential conditions and then compared the effect size of these three contrast from self-referential condition and from other-referential condition. Note that the results were also analyzed in a standard repeated measure ANOVA (see supplementary materials).

### Specificity of the valence effect
In this part, we reported the data from experiment 5, which included positive, neutral, and negative valence from four different domains: morality, aesthetic of person, aesthetic of scene, and emotion. This experiment was design to test whether the positive bias is specific to morality.

### Behavior-Questionnaire correlation

Finally, we explored correlation between results from behavioral results and self-reported measures. 

For the questionnaire part, we are most interested in the self-rated distance between different person and self-evaluation related questionnaires: self-esteem, moral-self identity, and moral self-image. Other questionnaires (e.g., personality) were not planned to correlated with behavioral data were not included. Note that all data were reported in [@Liu_2020_JOPD].

For the behavioral task part, we derived different indices. First, we used the mean of the RT and *d'* from each participants of each condition. Second, we used three parameters from drift diffusion model: drift rate (*v*), boundary separation (*a*), and non decision-making time (*t*). Third, we calculated the differences between different conditions (valence effect: good-self vs. bad-self, good-self vs. neutral-self, bad-self vs. neutral-self; good-other vs. bad-other, good-other vs. neutral-other, bad-other vs. neutral-other; Self-reference effect: good-self vs. good-other, neutral-self vs. neutral-other, bad-self vs. bad-other), as indexed by Cohen's $d$ and standard error (SE) of Cohen's $d$. 
$$ Cohen's \space d_{z} = \frac {(M_{1} - M_{2})} {\sqrt{(SD_{1}^2 + SD_{2}^2)/2}}$$
Given that the task difficulty were different across experiments, we z-transformed all these indices so that they become unit-free.

The DDM analyses were finished by HDDM, as reported in @Hu_2020_GoodSelf. That is, we used the response code approach, match response were coded as 1 and nonmatch responses were coded as 0. To fully explore all parameters, we allow all four parameters of DDM free to vary. We then extracted the estimation of all the four parameters for each participants for the correlation analyses. However, because the starting point is only related to response (match vs. non-match) but not the valence of the stimuli, we didn't included it in correlation analysis.

We used Pearson correlation to quantify the correlation. For those correlation that is significant ($p < 0.05$), we further tested the robustness of the correlation using bootstrap by `BootES` package [@kirby_bootes_2013]. To avoid false positive, we further determined the threshold for signifcant by permutation. More specifically, for each pairs that initially with  $p < .05$, we randomly shuffle the participants data of each score and calculated the correlation between the shuffled vectors. After repeating this procedure for 5000 times, we choose arrange these 5000 correlation coefficients and use the 95% percentile number as our threshold.

```{r remove repeated subj Data,echo=FALSE,results='hide'}
## exclude the repeating subjects
df1c.meta.d <- df1c.meta.d %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210))
df1c.meta.rt <- df1c.meta.rt %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210)) # exclude participants who participated exp1a or 1b

df2.meta.d <- df2.meta.d %>% dplyr::filter(Subject > 2000)    # exclude participant from exp 1a
df2.meta.rt <- df2.meta.rt %>% dplyr::filter(Subject > 2000)

df3a.meta.d <- df3a.meta.d %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) # exclude participants from ex1b, 1c, and 2
df3a.meta.rt <- df3a.meta.rt %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) # exclude participants from ex1b, 1c, and 2

df4b.meta.d <- df4b.meta.d %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201))   # exclude participants from ex1b, 1c, and 2
df4b.meta.rt <- df4b.meta.rt %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201)) # exclude participants from ex1b, 1c, and 2

df5.meta.d <- df5.meta.d %>% dplyr::filter(!Subject %in% c(5201))   # exclude participants from ex1b, 1c, and 2
df5.meta.rt <- df5.meta.rt %>% dplyr::filter(!Subject %in% c(5201)) # exclude participants from ex1b, 1c, and 2

df6a.meta.d <- df6a.meta.d %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131))   # exclude participants from ex1b, 1c, and 2
df6a.meta.rt <- df6a.meta.rt %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131)) # exclude participants from ex1b, 1c, and 2

df6b.meta.d <- df6b.meta.d %>% dplyr::filter(!Subject %in% c(6217))   # exclude participants from ex1b, 1c, and 2
df6b.meta.rt <- df6b.meta.rt %>% dplyr::filter(!Subject %in% c(6217)) # exclude participants from ex1b, 1c, and 2

df7a_m.meta.d <- df7a_m.meta.d %>% dplyr::filter(!Subject %in% c(7020))   # exclude participants from ex1b, 1c, and 2
df7a_m.meta.rt <- df7a_m.meta.rt %>% dplyr::filter(!Subject %in% c(7020)) # exclude participants from ex1b, 1c, and 2

```

# Results
```{r first meta,echo=FALSE,results='hide'}
# Combine the data -----
df.meta_d_1 <- rbind(df1a.meta.d, df1b.meta.d, df1c.meta.d, df2.meta.d, df4b.meta.d , df5.meta.d, df6a.meta.d) 
df.meta_rt_1 <- rbind(df1a.meta.rt, df1b.meta.rt, df1c.meta.rt, df2.meta.rt, df4b.meta.rt,df5.meta.rt, df6a.meta.rt)

# Prepare the data for meta ----
# calculate the mean, sd, n, and r for estimating the effect size and SE of effect size.
effectList_1 <- c('Good_Bad','Good_Neut','Bad_Neut')

df.ES_1 <- data.frame(matrix(nrow=length(unique(df.meta_d_1$ExpID))*length(effectList_1)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df.meta_d_1$ExpID))*length(effectList_1)),
                ExpID  = rep(rep(unique(df.meta_d_1$ExpID), each = length(effectList_1)), 2),
                Effect = rep(effectList_1, length(unique(df.meta_d_1$ExpID))*2),
                #Group  = rep(groupList, length(unique(df.meta_d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df.meta_rt_1 %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df.meta_d_1 %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_1){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Good_Bad'){
        #print(paste('processing Good_Bad of ', expName, sep = ''))
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
        }
      else if (effectName == 'Good_Neut'){
        #print(paste('processing Good_Neut of ', expName, sep = ''))
        #if ('Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        #  }
        #else{
          #print(paste('There is no Neutral condition in', expName, sepe=''))
        #  next
        #  }
        }
      else if (effectName == 'Bad_Neut'){
        #if ('Neutral' %in% tmpdata$Valence)
        #  {
            dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad")
            dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        #  }
        #else{
            #print(paste('There is no Neutral condition in', expName, sepe=''))
        #    next
        #  }
        }
      #}
      M1  <- mean(dataCond1$Value) -> df.ES_1$M1[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_1$SD1[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_1$M2[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_1$SD2[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_1$N[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_1$r[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_1$ES[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName] <- tmp2[1,1]
      df.ES_1$ES.var[df.ES_1$DVtype == DVName & df.ES_1$ExpID == expName & df.ES_1$Effect == effectName] <- tmp2[1,2]
    }
  }
}

# Do the meta-analysis in a for loop ----
df.ES_1_sum <- df.ES_1 %>% 
  dplyr::group_by(DVtype, Effect) %>% 
  tidyr::drop_na() %>% 
  dplyr::summarise(Nexp = length(unique(ExpID)), Nsubj = sum(N, na.rm = T))

df.res.meta_1 <- data.frame(matrix(nrow= 3*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = 3),
                #Group  = rep(groupList_1, 2),
                Effect = rep(effectList_1, 2),
                #Group  = rep(groupList, length(unique(df.meta_d$ExpID))*2),
                N_exp = NA, Cohen_d = NA, se = NA, CI_low = NA, CI_upp = NA, pval = NA)

for (DVName in c('RT','dprime')){
  for (effectName in effectList_1){
    df.res.meta <- df.ES_1 %>%
      dplyr::filter(DVtype == DVName & Effect == effectName) %>%
      tidyr::drop_na()
    
    tmp.meta.res <- metafor::rma(yi = df.res.meta$ES,
                           vi = df.res.meta$ES.var,
                           slab = df.res.meta$ExpID)
    df.res.meta_1$N_exp[df.res.meta_1$DVtype == DVName & df.res.meta_1$Effect == effectName] <- tmp.meta.res$k
    df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == DVName & df.res.meta_1$Effect == effectName] <- tmp.meta.res$beta
    df.res.meta_1$se[df.res.meta_1$DVtype == DVName & df.res.meta_1$Effect == effectName] <- tmp.meta.res$se
    df.res.meta_1$CI_low[df.res.meta_1$DVtype == DVName & df.res.meta_1$Effect == effectName] <- tmp.meta.res$ci.lb
    df.res.meta_1$CI_upp[df.res.meta_1$DVtype == DVName & df.res.meta_1$Effect == effectName] <- tmp.meta.res$ci.ub
    df.res.meta_1$pval[df.res.meta_1$DVtype == DVName & df.res.meta_1$Effect == effectName] <- tmp.meta.res$pval
  }
}

# Prepare data for plotting the effect size ----
df.res.meta_1 <- df.res.meta_1 %>%
  dplyr::mutate(Identity = "No-Ref.",
                EffectType = Effect) 
```

```{r second meta,echo=FALSE,results='hide'}
# Results part 2: with self-referential, included experiments: 3a, 3b, 6b, 7a, 7b

# Combine the data  ----
df.meta_d_2 <- rbind(df3a.meta.d, df3b.meta.d, df6b.meta.d, df7a_m.meta.d, df7b_m.meta.d) 
df.meta_rt_2 <- rbind(df3a.meta.rt, df3b.meta.rt, df6b.meta.rt, df7a_m.meta.rt, df7b_m.meta.rt)

# Calculate the mean, sd, n, and r ----
# for estimating the effect size and SE of effect size.
effectList_2 <- c('Good_Bad_S','Good_Neut_S','Bad_Neut_S',
                'Good_Bad_O','Good_Neut_O','Bad_Neut_O')

df.ES_2 <- data.frame(matrix(nrow=length(unique(df.meta_d_2$ExpID))*length(effectList_2)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df.meta_d_2$ExpID))*length(effectList_2)),
                ExpID  = rep(rep(unique(df.meta_d_2$ExpID), each = length(effectList_2)), 2),
                Effect = rep(effectList_2, length(unique(df.meta_d_2$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df.meta_rt_2 %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df.meta_d_2 %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_2){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Good_Bad_S'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Good_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Bad_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Bad_O'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Bad_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
      }
      
      M1  <- mean(dataCond1$Value) -> df.ES_2$M1[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_2$SD1[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_2$M2[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_2$SD2[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_2$N[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_2$r[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_2$ES[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] <- tmp2[1,1]
      df.ES_2$ES.var[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] <- tmp2[1,2]
    }
  }
}

# Do the meta ----
# info about participants
df.ES_2_sum <- df.ES_2 %>% 
  dplyr::group_by(DVtype, Effect) %>% 
  tidyr::drop_na() %>% 
  dplyr::summarise(Nexp = length(unique(ExpID)), Nsubj = sum(N, na.rm = T))

df.res.meta_2 <- data.frame(matrix(nrow= (2*3)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = (2*3)),
                Effect = rep(effectList_2, 2),
                N_exp = NA, Cohen_d = NA, se = NA, CI_low = NA, CI_upp = NA, pval = NA)

# meta -analysis
for (DVName in c('RT','dprime')){
  for (effectName in effectList_2){
    df.res.meta <- df.ES_2 %>%
      dplyr::filter(DVtype == DVName & Effect == effectName) %>%
      tidyr::drop_na()
  
    tmp.meta.res <- metafor::rma(yi = df.res.meta$ES,
                           vi = df.res.meta$ES.var,
                           slab = df.res.meta$ExpID)
    df.res.meta_2$N_exp[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$k
    df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$beta
    df.res.meta_2$se[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$se
    df.res.meta_2$CI_low[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$ci.lb
    df.res.meta_2$CI_upp[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$ci.ub
    df.res.meta_2$pval[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$pval
  }
}

# plot the effect size  ----
df.res.meta_2 <- df.res.meta_2 %>%
  dplyr::mutate(Identity = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Neut_S" | Effect == "Bad_Neut_S",
                                  "Self-Ref.", "Other-Ref."),
                EffectType = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Bad_O", "Good_Bad",
                                    ifelse(Effect == "Good_Neut_S" | Effect == "Good_Neut_O", "Good_Neut", "Bad_Neut")))

df.res_meta_pdata <- rbind(df.res.meta_1, df.res.meta_2) %>%
  dplyr::mutate(Identity = factor(Identity, levels = c("No-Ref.", "Self-Ref.", "Other-Ref.")),
                EffectType = factor(EffectType, levels = c("Good_Bad", "Good_Neut", "Bad_Neut" )))

```

## Effect of moral valence

```{r plot-all-effect, fig.cap="Effect size (Cohen's *d*) of Valence.", warning=FALSE}
# fig.width=8, 
df.res_meta_pdata %>%
  dplyr::filter(EffectType != 'Good_Bad') %>%
  ggplot2::ggplot(., aes(x = EffectType, y=Cohen_d, color=EffectType, fill=EffectType )) + # 
  geom_pointrange(aes(ymin=Cohen_d - 1.96*se, ymax = Cohen_d + 1.96*se), 
                  position = position_dodge(width = 0.5),
                  shape=18, size=0.8) +
  geom_hline(yintercept=0, size=1, color='grey', linetype = 'dashed') +
  coord_cartesian(ylim=c(-1.5, 1.5))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Effect size: Cohen's ",italic("d"), sep = ' '))) +
  xlab('Contrasts between different valence') +
  apatheme_x +
  facet_grid(  DVtype ~ Identity)
```

In this part, we synthesized results from experiment 1a, 1b, 1c, 2, 5 and 6a. Data from 192 participants were included in these analyses. We found differences between positive and negative conditions on RT was Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']`]; on *d'* was Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']`]; *d'*: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Bad_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Bad_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Bad_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Bad_Neut']`]; *d'*: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Bad_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Bad_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Bad_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Bad_Neut']`]. See Figure \@ref(fig:plot-all-effect) left panel.

## Interaction between valence and self-reference
In this part, we combined the experiments that explicitly manipulated the self-reference and valence, which includes 3a, 3b, 6b, 7a, and 7b. For the positive versus negative contrast, data were from five experiments with 178 participants; for positive versus neutral and neutral versus negative contrasts, data were from three experiments ( 3a, 3b, and 6b) with 108 participants.

In most of these experiments, the interaction between self-reference and valence was significant (see results of each experiment in supplementary materials). In the mini-meta-analysis, we analyzed the valence effect for self-referential condition and other-referential condition separately.

For the self-referential condition, we found the same pattern as in the first part of results. That is we found significant differences between positive and neutral as well as positive and negative, but not neutral and negative. The effect size of RT between positive and negative is Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']`]; on *d'* was Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_S']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_S']`]. See Figure \@ref(fig:plot-all-effect) the middle panel.

For the other-referential condition, we found that only the difference between positive and negative on RT was significant, all the other conditions were not. The effect size of RT between positive and negative is Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']`]; on *d'* was Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']`]. The effect was not observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Bad_Neut_O']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Bad_Neut_O']`]. See Figure \@ref(fig:plot-all-effect) right panel.

## Generalizibility of the valence effect
In this part, we reported the results from experiment 4 in which either moral valence or self-reference were manipulated as task-irrelevant stimuli. 

```{r analyzing exp4a, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
df.ES_4a <- data.frame(matrix(nrow=length(unique(df4a.meta.d$ExpID))*length(effectList_2)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df4a.meta.d$ExpID))*length(effectList_2)),
                ExpID  = rep(rep(unique(df4a.meta.d$ExpID), each = length(effectList_2)), 2),
                Effect = rep(effectList_2, length(unique(df4a.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df4a.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df4a.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_2){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Good_Bad_S'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Good_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Bad_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Bad_O'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Bad_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
      }
      
      M1  <- mean(dataCond1$Value) -> df.ES_4a$M1[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_4a$SD1[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_4a$M2[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_4a$SD2[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_4a$N[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_4a$r[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_4a$ES[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] <- tmp2[1,1]
      df.ES_4a$ES.var[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_4a <- df.ES_4a %>%
  dplyr::mutate(Identity = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Neut_S" | Effect == "Bad_Neut_S",
                                  "Self-ref.", "Other-ref."),
                EffectType = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Bad_O", "Good_Bad",
                                    ifelse(Effect == "Good_Neut_S" | Effect == "Good_Neut_O", "Good_Neut", "Bad_Neut")),
                Identity = factor(Identity, levels = c("Self-ref.", "Other-ref.")),
                EffectType = factor(EffectType, levels = c("Good_Bad", "Good_Neut", "Bad_Neut")))
```

```{r 'plot-exp4a-effect', fig.cap="Effect size (Cohen's *d*) of Valence in Exp4a.", warning=FALSE}
df.ES_4a %>%
  dplyr::filter(EffectType != "Good_Bad") %>%
  ggplot(., aes(x=EffectType, y=ES, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.5),
                  shape=18, size=0.8) +
  geom_hline(yintercept=0, size=1, color='grey', linetype = 'dashed') +
  # ggtitle('A: Valence effect') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Effect size: Cohen's ",italic("d"), sep = ' ')))+
  xlab('Contrasts between different valence')+
  apatheme_x +
  facet_grid(DVtype ~ Identity)
```

For experiment 4a, when self-reference was the target and moral valence was task-irrelevant, we found that only under the implicit self-referential condition, i.e., when the moral words were presented as task irrelevant stimuli, there was the main effect of valence and interaction between valence and reference for both *d* prime and RT (See supplementary results for the detailed statistics). For *d* prime, we found good-self condition (`r df.ES_4a$M1[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD1[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']`) had higher *d* prime than bad-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']`); good self condition was also higher than neutral self (`r df.ES_4a$M2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Neut_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Neut_S']`) but there was not statistically significant, while the neutral-self condition was higher than bad self condition and not significant neither. For reaction times, good-self condition (`r df.ES_4a$M1[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD1[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']`) were faster relative to bad-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']`), and over neutral-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Neut_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Neut_S']`). The difference between neutral-self and bad-self conditions were not significant. However, for the other-referential condition, there was no significant differences between different valence conditions. See Figure \@ref(fig:plot-exp4a-effect).

```{r analyzing exp4b, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
#### Approach 1: compared between self and other
effectList_3 <- c('Self_Other_G','Self_Other_N', 'Self_Other_B')

df.ES_4b <- data.frame(matrix(nrow=length(unique(df4b.meta.d$ExpID))*length(effectList_3)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df4b.meta.d$ExpID))*length(effectList_3)),
                ExpID  = rep(rep(unique(df4b.meta.d$ExpID), each = length(effectList_3)), 2),
                Effect = rep(effectList_3, length(unique(df4b.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df4b.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df4b.meta.d %>% dplyr::rename(Value = dprime)
  }
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_3){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')

      if (effectName == 'Self_Other_G'){

        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Self_Other_N'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          }
        else{
          next
          }
        }
      else if (effectName == 'Self_Other_B'){
        if (!all(is.na(tmpdata$Identity)) & 'Bad' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')
          }
        else{
          next
          }
        }

      M1  <- mean(dataCond1$Value) -> df.ES_4b$M1[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_4b$SD1[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      M2  <- mean(dataCond2$Value) -> df.ES_4b$M2[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_4b$SD2[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_4b$N[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_4b$r[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_4b$ES[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] <- tmp2[1,1]
      df.ES_4b$ES.var[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_4b <- df.ES_4b %>%
  dplyr::mutate(Val = ifelse(Effect == "Self_Other_G", "Good",
                                  ifelse(Effect == "Self_Other_N", 'Neutral', 'Bad')),
                EffectType = 'Self_Other',
                Val = factor(Val, levels = c("Good", "Neutral", "Bad")))

#### Approach 2: compared between self and other
# Added the interaction in the effect size calculation directly
df.ES_4b_2 <- data.frame(matrix(nrow= (length(unique(df4b.meta.d$ExpID))*length(effectList_2) +3)*2 , ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df4b.meta.d$ExpID))*length(effectList_2) + 3),
                ExpID  = rep(rep(unique(df4b.meta.d$ExpID), each = length(effectList_2) + 3), 2),
                Effect = rep(c(effectList_2, c('Good_Bad_SO', 'Good_Neut_SO', 'Bad_Neut_SO')), length(unique(df4b.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df4b.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df4b.meta.d %>% dplyr::rename(Value = dprime)
  }

  for (expName in unique(metaData$ExpID)){
    for (effectName in c(effectList_2, c('Good_Bad_SO', 'Good_Neut_SO', 'Bad_Neut_SO'))){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')

      if (effectName == 'Good_Bad_S'){

        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Good_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          }
        else{
          next
          }
        }
      else if (effectName == 'Bad_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Bad_O'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          }
        else{
          next
          }
        }
      else if (effectName == 'Bad_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          }
        else{
          next
          }
      }
      else if (effectName == 'Good_Bad_SO'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond01 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond02 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond03 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond04 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other') 
          
          dataCond1 <- dataCond01 %>% dplyr::mutate(Value = Value - dataCond02$Value)  # good-self - bad-self 
          dataCond2 <- dataCond03 %>% dplyr::mutate(Value = Value - dataCond04$Value)   # good-other - bad-other
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_SO'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond01 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self') 
          dataCond02 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond03 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond04 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          
          dataCond1 <- dataCond01 %>% dplyr::mutate(Value = Value - dataCond02$Value)   # good-self - neutral-self 
          dataCond2 <- dataCond03 %>% dplyr::mutate(Value = Value - dataCond04$Value)   # good-other - neutral-other
          }
        else{
          next
          }
        }
      else if (effectName == 'Bad_Neut_SO'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond01 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond02 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond03 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')
          dataCond04 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          
          dataCond1 <- dataCond01 %>% dplyr::mutate(Value = Value - dataCond02$Value) # bad-self - neutral-self
          dataCond2 <- dataCond03 %>% dplyr::mutate(Value = Value - dataCond04$Value) # bad-other - neutral-other
          }
        else{
          next
          }
      }
      M1  <- mean(dataCond1$Value) -> df.ES_4b_2$M1[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_4b_2$SD1[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName]
      M2  <- mean(dataCond2$Value) -> df.ES_4b_2$M2[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_4b_2$SD2[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_4b_2$N[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_4b_2$r[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName]
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_4b_2$ES[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName] <- tmp2[1,1]
      df.ES_4b_2$ES.var[df.ES_4b_2$DVtype == DVName & df.ES_4b_2$ExpID == expName & df.ES_4b_2$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_4b_2 <- df.ES_4b_2 %>%
  dplyr::mutate(Identity = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Neut_S" | Effect == "Bad_Neut_S",
                                  "Self-ref.", 
                                  ifelse(Effect == "Good_Bad_O" | Effect == "Good_Neut_O" | Effect == "Bad_Neut_O", 
                                         "Other-ref.", 'Interaction')),
                EffectType = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Bad_O"  | Effect == "Good_Bad_SO", "Good_Bad",
                                    ifelse(Effect == "Good_Neut_S" | Effect == "Good_Neut_O"  | Effect == "Good_Neut_SO", "Good_Neut", "Bad_Neut")),
                Identity = factor(Identity, levels = c("Self-ref.", "Other-ref.", 'Interaction')),
                EffectType = factor(EffectType, levels = c("Good_Bad", "Good_Neut", "Bad_Neut")))
```

```{r 'plot-exp4b-effect-1', fig.cap="Effect size (Cohen's *d*) of Valence in Exp4b.", warning=FALSE}
df.ES_4b %>%
  #dplyr::filter(Identity == "Self") %>%
  ggplot(., aes(x=EffectType, y=ES, color=Val, fill=Val)) + # , color=Val, fill=Val
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)),
                  position = position_dodge(width = 0.5),
                  shape=18, size=0.8) +
  geom_hline(yintercept=0, size=0.5, color='grey', linetype = 'dashed') +
  # ggtitle('Self-reference effect') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' '))) +
  xlab('Self-reference effect') + 
  apatheme_x +
  facet_grid(DVtype ~ .)
```

```{r 'plot-exp4b-effect-2', fig.cap="Effect size (Cohen's *d*) of Valence in Exp4b.", warning=FALSE}
df.ES_4b_2 %>%
  dplyr::filter(Effect %in% effectList_2) %>%
  dplyr::filter(EffectType != "Good_Bad") %>%
  ggplot(., aes(x=EffectType, y=ES, color=EffectType, fill=EffectType)) +
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)),
                  position = position_dodge(width = 0.5),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='grey', linetype = 'dashed') +
  # ggtitle('A: Valence effect') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Effect size: Cohen's ",italic("d"), sep = ' ')))+
  xlab('Valence effect')+
  apatheme_x +
  facet_grid(DVtype ~ Identity)
```

```{r 'plot-exp4b-diff-diff', fig.cap="Effect size (Cohen's *d*) of Valence in Exp4b.", warning=FALSE}
df.ES_4b_2 %>%
  #dplyr::filter(Effect %in% c('Good_Bad_SO', 'Good_Neut_SO', 'Bad_Neut_SO')) %>%
  dplyr::filter(Effect %in% c('Good_Neut_SO', 'Bad_Neut_SO')) %>%
  ggplot(., aes(x=EffectType, y=ES, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.5),
                  shape=18, size=0.8) +
  geom_hline(yintercept=0, size=0.5, color='grey', linetype = 'dashed') +
  # ggtitle('Differences in valence effect (self-other)') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Effect size: Cohen's ",italic("d"), sep = ' ')))+
  xlab('Diff of valence effect between self vs. other condition')+
  apatheme_x +
  facet_grid(DVtype ~ .)
  # facet_wrap(~ DVtype, strip.position="right")
```

For experiment 4b, when valence was the target and the identity was task-irrelevant, we found a strong valence effect (see supplementary results and Figure \@ref(fig:plot-exp4b-effect-1), Figure \@ref(fig:plot-exp4b-effect-2)). 

In this experiment, the advantage of good-self condition can only be disentangled by comparing the self-referential and other-referential conditions. Therefore, we calculated the differences between the valence effect under self-referential and other referential conditions and used the weighted variance as the variance of this differences. We found this modulation effect on RT. The valence effect of RT was stronger in self-referential than other-referential for the Good vs. Neutral condition (`r df.ES_4b_2$ES[df.ES_4b_2$DVtype == 'RT' & df.ES_4b_2$Effect == 'Good_Neut_SO']` $\pm$ `r df.ES_4b_2$ES.var[df.ES_4b_2$DVtype == 'RT' & df.ES_4b_2$Effect == 'Good_Neut_SO']`), and to a less extent the Good vs. Bad condition (`r df.ES_4b_2$ES[df.ES_4b_2$DVtype == 'RT' & df.ES_4b_2$Effect == 'Good_Bad_SO']` $\pm$ `r df.ES_4b_2$ES.var[df.ES_4b_2$DVtype == 'RT' & df.ES_4b_2$Effect == 'Good_Bad_SO']`). While the size of the other effect's CI included zero, suggestion those effects didn't differ from zero. See Figure \@ref(fig:plot-exp4b-diff-diff).

## Specificity of valence effect

```{r analyzing exp5, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
effectList_exp5 <- c('Good_Bad_Mrl','Good_Neut_Mrl','Bad_Neut_Mrl',
                     'Good_Bad_BP','Good_Neut_BP','Bad_Neut_BP',
                     'Good_Bad_BS','Good_Neut_BS','Bad_Neut_BS',
                     'Good_Bad_Emo','Good_Neut_Emo','Bad_Neut_Emo')

df.ES_5 <- data.frame(matrix(nrow=length(unique(df5.meta.d$ExpID))*length(effectList_exp5)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df5.meta.d$ExpID))*length(effectList_exp5)),
                ExpID  = rep(rep(unique(df5.meta.d$ExpID), each = length(effectList_exp5)), 2),
                Effect = rep(effectList_exp5, length(unique(df5.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df5.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df5.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_exp5){
      if (effectName == 'Good_Bad_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Bad_Neut_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
      }
      
      else if (effectName == 'Good_Bad_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Bad_Neut_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
      }
      
      else if (effectName == 'Good_Bad_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Bad_Neut_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
      }
      
      else if (effectName == 'Good_Bad_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Bad_Neut_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        }
      
      M1  <- mean(dataCond1$Value) -> df.ES_5$M1[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_5$SD1[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_5$M2[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_5$SD2[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_5$N[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_5$r[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_5$ES[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] <- tmp2[1,1]
      df.ES_5$ES.var[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_5 <- df.ES_5 %>%
  dplyr::mutate(Domain = ifelse(Effect == "Good_Bad_Mrl" | Effect == "Good_Neut_Mrl" | Effect == "Bad_Neut_Mrl",
                                  "Mrl",
                                ifelse(Effect == "Good_Bad_BP" | Effect == "Good_Neut_BP" | Effect == "Bad_Neut_BP",
                                  "AP",
                                  ifelse(Effect == "Good_Bad_BS" | Effect == "Good_Neut_BS" | Effect == "Bad_Neut_BS",
                                  "AS", 'Emo'))),
                Domain = factor(Domain, levels = c("Mrl", "AP", "AS", "Emo")),
                EffectType = ifelse(Effect == "Good_Bad_Mrl" | Effect == "Good_Bad_BP"  | Effect == "Good_Bad_BS"  | Effect == "Good_Bad_Emo", "Pos_Neg",
                                    ifelse(Effect == "Good_Neut_Mrl" | Effect == "Good_Neut_BP" | Effect == "Good_Neut_BS" | Effect == "Good_Neut_Emo", "Pos_Neut", "Neg_Neut")),
                EffectType = factor(EffectType, levels = c("Pos_Neg", "Pos_Neut", "Neg_Neut")))
```

```{r 'plot-exp5-effect', fig.cap="Effect size (Cohen's *d*) of Valence in Exp5.", warning=FALSE}
df.ES_5 %>%
  dplyr::filter(EffectType != "Pos_Neg") %>%
  ggplot(., aes(x=EffectType, y=ES, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  # ggtitle('Valence effect across different domains') +
  coord_cartesian(ylim=c(-1.5, 1.5))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Effect size: Cohen's ",italic("d"), sep = ' '))) +
  xlab('Contrast between different valences') +
  facet_grid(DVtype ~ Domain) +
  apatheme_x
```

In this part, we analyzed the results from experiment 5, which included positive, neutral, and negative valence from four different domains: morality, emotion, aesthetics of human, and aesthetics of scene. We found interaction between valence and domain for both *d* prime and RT (match trials). A common pattern appeared in all four domains: each domain showed a binary results instead of gradient on both *d* prime and RT. For morality, aesthetics of human, and aesthetics of scene, the positive conditions had advantages over both neutral and negative conditions (greater *d* prime and faster RT), and neutral and negative conditions didn't differ from each other. But for the emotional stimuli, it was the positive and neutral had advantage over negative conditions, while positive and neutral conditions were not significantly different. See supplementary materials for detailed statistics. Also note that the effect size in moral domain is smaller than the aesthetic domains (beauty of people and beauty of scene). See Figure \@ref(fig:plot-exp5-effect).

## Self-reported personal distance

```{r personal distance, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# prepare questionnaire data
df.scales <- read.csv(here::here("Scale_data", "FADGS_dataset4ID_clean.csv"), header = TRUE, sep = ",",
                 stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
  dplyr::mutate(expID = derivedFactor("Exp1a" = (expID == "exp1.0"), 
                                      "Exp1b" = (expID == "exp1.1"),
                                      "Exp3a" = (expID == "exp3"),
                                      "Exp3b" = (expID == "exp3.1"),
                                      "Exp4a" = (expID == "exp4.1"),
                                      "Exp4b" = (expID == "exp4.2"),
                                      "Exp5" = (expID == "exp5.2"),
                                      "Exp6b" = (expID == "exp6.2"),
                                      "Exp7a" = (expID == "exp7.1"),
                                      "Exp7b" = (expID == "exp7r"),
                                      "Exp_dpr" = (expID == "exp6"),
                                      .method ="first", .default = NA),
                expID = as.character(expID))

## get the questionnaire names
# Self-esteem
SlfEstNames <- c("SES1","SES2","SES3","SES4","SES5","SES6","SES7","SES8","SES9","SES10")

# moral identity
mrlIdNames <- c("morId_1","morId_2","morId_3","morId_4", "morId_5","morId_6",
                "morId_7","morId_8","morId_9","morId_10","morId_11","morId_12",
                "morId_13","morId_14","morId_15","morId_16")
mrlIdIntNames <- c("morId_1","morId_2","morId_5","morId_8", "morId_10","morId_11",
                   "morId_12","morId_13","morId_14")
mrlIdExtNames <- c("morId_3","morId_4", "morId_6", "morId_7", "morId_9","morId_10",
                   "morId_15", "morId_16")

# moral self images
mrlslfImgNames <- c("morSlfImg_1","morSlfImg_2","morSlfImg_3","morSlfImg_4",
                    "morSlfImg_5","morSlfImg_6","morSlfImg_7","morSlfImg_8","morSlfImg_9")

# personal distance
perDistNames <- c("SelfSelf", 
                  "SelfGood_1", "SelfGood_2", "SelfGood_3", "SelfGood_4",
                  "SelfNeut_1", "SelfNeut_2", "SelfNeut_3", "SelfNeut_4",
                  "SelfBad_1",  "SelfBad_2",  "SelfBad_3",  "SelfBad_4",
                  "SelfStra_1", "SelfStra_2", "SelfStra_3", "SelfStra_4",
                  "GoodNeut_1", "GoodNeut_2", "GoodNeut_3", "GoodNeut_4", 
                  "GoodBad_1",  "GoodBad_2",  "GoodBad_3",  "GoodBad_4",
                  "NeutBad_1",  "NeutBad_2",  "NeutBad_3",  "NeutBad_4")

df.perdist <- df.scales %>%
  dplyr::select(c(expID, subjID),perDistNames) %>%
  #dplyr::rowwise() %>%
  dplyr::mutate(sumRaw = rowMeans(.[3:31], na.rm = T),
                SelfSelfraw = SelfSelf,
                SelfGoodraw = rowMeans(.[grep("SelfGood", names(.))], na.rm = T),
                SelfNeutraw = rowMeans(.[grep("SelfNeut", names(.))], na.rm = T),
                SelfBadraw  = rowMeans(.[grep("SelfBad", names(.))], na.rm = T),
                SelfStraraw = rowMeans(.[grep("SelfStra", names(.))], na.rm = T),
                GoodNeutraw = rowMeans(.[grep("GoodNeut", names(.))], na.rm = T),
                GoodBadraw  = rowMeans(.[grep("GoodBad", names(.))], na.rm = T),
                NeutBadraw  = rowMeans(.[grep("NeutBad", names(.))], na.rm = T)) %>%
  dplyr::select(expID, subjID, sumRaw, SelfSelfraw, 
                SelfGoodraw, SelfNeutraw, SelfBadraw,
                SelfStraraw, GoodNeutraw, GoodBadraw, NeutBadraw) %>%
  dplyr::mutate(SelfSelf = SelfSelfraw/sumRaw,
                SelfGood = SelfGoodraw/sumRaw,
                SelfNeut = SelfNeutraw/sumRaw,
                SelfBad = SelfBadraw/sumRaw,
                SelfStra = SelfStraraw/sumRaw, 
                GoodNeut = GoodNeutraw/sumRaw, 
                GoodBad = GoodBadraw/sumRaw, 
                NeutBad = NeutBadraw/sumRaw) %>%
  dplyr::select(subjID, SelfSelf, SelfGood, SelfNeut, SelfBad,
                SelfStra, GoodNeut, GoodBad, NeutBad) %>%
  tidyr::drop_na()
```

```{r plot-person-dist, fig.cap="Self-rated personal distance", fig.width=8, warning=FALSE}
df.plot <- df.perdist %>%
    dplyr::select(-c(SelfSelf, SelfStra)) %>%
    tidyr::pivot_longer(., cols = SelfGood:NeutBad, 
                        names_to = 'PerDist', 
                        values_to = "value") %>% # to longer format
    dplyr::mutate(PerDist =factor(PerDist, levels = c('SelfNeut', 'SelfGood', 'GoodNeut', 'NeutBad', 'GoodBad', 'SelfBad')),
                  # DVs = factor(DVs, levels = c('RT', 'dprime')),
                  # create an extra column for ploting the individual data cross different conditions.
                  Conds = mosaic::derivedFactor("1" = (PerDist == 'SelfNeut'), 
                                                "2" = (PerDist == 'SelfGood'),
                                                "3" = (PerDist == 'GoodNeut'),
                                                "4" = (PerDist == 'NeutBad'),
                                                "5" = (PerDist == 'GoodBad'),
                                                "6" = (PerDist == 'SelfBad'),
                                                method ="first", .default = NA),
                  Conds = as.numeric(as.character(Conds)),
    ) 
  
  df.plot$Conds_j <- jitter(df.plot$Conds, amount=.09) # add gitter to x
  
  # New facet label names for panel variable
  # https://stackoverflow.com/questions/34040376/cannot-italicize-facet-labels-with-labeller-label-parsed
  # levels(df.plot$DVs ) <- c("RT"=expression(paste("Reaction ", "times (ms)")),
  #                           "dprime"=expression(paste(italic("d"), ' prime')))
  # levels(df.plot$DVs ) <- c("RT"=expression(paste("Reaction ", "times (ms)")),
  #                           "dprime"=expression(paste(italic("d"), ' prime')))
  
  df.plot.sum_p <- summarySE(df.plot, measurevar = "value", groupvars = c('PerDist')) %>%
    dplyr::mutate(Cond_num = mosaic::derivedFactor("1" = (PerDist == 'SelfNeut'), 
                                                "2" = (PerDist == 'SelfGood'),
                                                "3" = (PerDist == 'GoodNeut'),
                                                "4" = (PerDist == 'NeutBad'),
                                                "5" = (PerDist == 'GoodBad'),
                                                "6" = (PerDist == 'SelfBad'),
                                                method ="first", .default = NA),
                  Cond_num = as.numeric(as.character(Cond_num)))
  
  pd1 <- position_dodge(0.5)
  # scaleFUN <- function(x) sprintf("%.2f", x)
  # scales_y <- list(
  #   RT = scale_y_continuous(limits = c(400, 900)),
  #   dprime = scale_y_continuous(labels=scaleFUN)
  #)
  
  df.plot  %>% # dplyr::filter(DVs== 'RT') %>%
    dplyr::rename(Subject=subjID) %>%
    ggplot(., aes(x = PerDist, y = value, colour = as.factor(Valence))) +
    geom_line(aes(x = Conds_j, y = value, group = Subject),         # link individual's points by transparent grey lines
              linetype = 1, size = 1, colour = "#000000", alpha = 0.03) + 
    geom_point(aes(x = Conds_j, y = value, group = Subject),   # plot individual points
               colour = "#000000",
               size = 1, shape = 20, alpha = 0.03) +
    geom_line(data = df.plot.sum_p, aes(x = as.numeric(PerDist), # plot the group means  
                                        y = value, 
                                        #group = Identity, 
                                        colour = PerDist,
    ), 
    linetype = 1, position = pd1, size = 2)+
    geom_point(data = df.plot.sum_p, aes(x = as.numeric(PerDist), # group mean
                                         y = value, 
                                         #group = Identity, 
                                         colour = PerDist,
    ), 
    shape = 18, position = pd1, size = 5) +
    geom_errorbar(data = df.plot.sum_p, aes(x = as.numeric(PerDist),  # group error bar.
                                            y = value, # group = Identity, 
                                            colour = PerDist,
                                            ymin = value- 1.96*se, 
                                            ymax = value+ 1.96*se), 
                  width = .05, size = 1, alpha = 0.75) +
    scale_colour_brewer(palette = "Dark2") +
    scale_x_continuous(breaks=c(1, 2, 3, 4, 5, 6),
                       labels=c("SelfNeut", "SelfGood", "GoodNeut", 'NeutBad', 'GoodBad', 'SelfBad')) +
    scale_fill_brewer(palette = "Dark2") +
    #ggtitle("A. Matching task") +
    theme_bw()+
    theme(panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.background = element_blank(),
          panel.border = element_blank(),
          text=element_text(family='Times'),
          legend.title=element_blank(),
          legend.text = element_text(size =16),
          plot.title = element_text(lineheight=.8, face="bold", size = 18, margin=margin(0,0,20,0)),
          axis.text = element_text (size = 16, color = 'black'),
          axis.title = element_text (size = 16),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          axis.line.x = element_line(color='black', size = 1),    # increase the size of font
          axis.line.y = element_line(color='black', size = 1),    # increase the size of font
          strip.text = element_text (size = 16, color = 'black'), # size of text in strips, face = "bold"
          panel.spacing = unit(3, "lines")
    ) 
```

See Figure \@ref(fig:plot-person-dist).

## Correlation analyses
The reliability of questionnaires can be found in [@Liu_2020_JOPD]. We calculated the correlation between the data from behavioral task and the questionnaire data. 

```{r correlation analysis,echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# get data for valence effect ---- 
## mean RT, SD of RT, and d prime for data without reference
tmp1 <- df.meta_rt_1 %>%
  dplyr::filter(Matchness == "Match" & Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = c(RT, RT_SD))
  
tmp2 <- df.meta_d_1 %>%
  dplyr::filter(Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = dprime) %>%
  dplyr::rename(dprime_Good = Good,
                dprime_Neut = Neutral,
                dprime_Bad = Bad)
  
df.meta_1_wide <- merge(tmp1,tmp2)
rm(tmp1,tmp2)

## parameters of HDDM for the data without reference
## read all the file name.
params.list <- list.files(here::here('HDDM'), pattern = '*_hddm_params.csv')
params.expname <- data.frame(params.list) %>%
  tidyr::separate(params.list, c('expName','B','C'),sep = '_') %>%
  dplyr::select(expName) %>%
  dplyr::pull()

df_hddm_ls_1 <- params.list[c(1:4, 9:10)]

for (indx in 1:6){
  if ((indx ==1) && exists('df_hddm_param_1')){   # if the variable already exist before the for loop start
    rm(df_hddm_param_1)
  }

  if (indx == 5 ){
    hddm_params_tmp <- read.csv(here::here("HDDM", df_hddm_ls_1[indx]), header = TRUE, sep = ",",
                   stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
      dplyr::filter(domain == "Morality") %>%
      dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val) %>%
      dplyr::select(Subject, Matchness, Valence, knode_name, mean) %>%
      tidyr::drop_na() %>%
      tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
      dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
      tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')  
    
  } else {
    hddm_params_tmp <- read.csv(here::here("HDDM", df_hddm_ls_1[indx]), header = TRUE, sep = ",",
                   stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
      dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val) %>%
      dplyr::select(Subject, Matchness, Valence, knode_name, mean) %>%
      tidyr::drop_na() %>%
      tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
      dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
      tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')   # %>%
      #dplyr::mutate(ExpID = 'Exp1a')
  }
  if (exists('df_hddm_param_1')) {
    df_hddm_param_1 <- rbind(df_hddm_param_1, hddm_params_tmp) 
  } else {
    df_hddm_param_1 <- hddm_params_tmp
  }
}

df.meta_1_wide <- merge(df.meta_1_wide, df_hddm_param_1) %>%
  dplyr::select(-c(Domain, Identity, Matchness))

## Get data for interaction between ID & Val ----
## mean RT, SD of RT, and d prime for data without reference
tmp1 <- df.meta_rt_2 %>%
  dplyr::filter(Matchness == "Match" & Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = c(RT, RT_SD))
  
tmp2 <- df.meta_d_2 %>%
  dplyr::filter(Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = dprime) %>%
  dplyr::rename(dprime_Good = Good,
                dprime_Neut = Neutral,
                dprime_Bad = Bad)
  
df.meta_2_wide <- merge(tmp1,tmp2); rm(tmp1,tmp2)

## parameters of HDDM 
## read all the file name.
df_hddm_ls_2 <- params.list[c(5, 6, 11:13)]

for (indx in 1:5){
  if ((indx ==1) && exists('df_hddm_param_1')){   # in case the variable exist in the env.
    rm(df_hddm_param_2)
  }
  hddm_params_tmp <- read.csv(paste(here::here("HDDM", df_hddm_ls_2[indx])), header = TRUE, sep = ",",
                 stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
    dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val, Identity = id) %>%
    dplyr::select(Subject, Matchness, Identity, Valence, knode_name, mean) %>%
    tidyr::drop_na() %>%
    tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
    dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
    tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')   # %>%
    #dplyr::mutate(ExpID = 'Exp1a')
  if (indx == 4 | indx == 5){
    hddm_params_tmp <- hddm_params_tmp %>%
      dplyr::mutate(Neutral_a = NA,
                    Neutral_t = NA,
                    Neutral_v = NA) %>%
      dplyr::select(Subject, Identity, Bad_a, Good_a, Neutral_a, Bad_v, Good_v, Neutral_v, Bad_t, Good_t,
                    Neutral_t)
  }
  
  if (exists('df_hddm_param_2')) {
    df_hddm_param_2 <- rbind(df_hddm_param_2, hddm_params_tmp) 
  } else {
    df_hddm_param_2 <- hddm_params_tmp
  }
}

df.meta_2_wide <- merge(df.meta_2_wide, df_hddm_param_2) %>%
  dplyr::select(-c(Domain, Matchness))

df.meta_1_wide <- df.meta_1_wide %>%
  dplyr::mutate(Identity = NA) %>%
  dplyr::select(colnames(df.meta_2_wide))

df.meta_all_wide <- df.meta_2_wide %>%
  dplyr::filter(Identity == "Self") %>%
  rbind(df.meta_1_wide, .)

# intersection between participant from behavioral task and scales and get the data
subj.common <- intersect(df.scales$subjID, unique(df.meta_all_wide$Subject))  # 253

df.scales.v <- df.scales %>% dplyr::filter(subjID %in% subj.common) %>%
  dplyr::select_if(~sum(!is.na(.)) > 0) # remove columns that only have NA.


df.mrlID <- df.scales.v[c('subjID', mrlIdIntNames, mrlIdExtNames)] %>%
  #tidyr::drop_na() %>%
  dplyr::mutate(mrlIdInt = rowMeans(.[, mrlIdIntNames], na.rm = F),
                mrlIdExt = rowMeans(.[, mrlIdExtNames], na.rm = F),)
# plot(df.mrlID$mrlIdInt)

# calculate the average score of each relevant scale
df.q_scores.v <- df.scales.v %>%
  dplyr::mutate(SlfEst = rowMeans(.[, SlfEstNames],na.rm = F),
                mrlIdInt = rowMeans(.[, mrlIdIntNames], na.rm = F),
                mrlIdExt = rowMeans(.[, mrlIdExtNames], na.rm = F),
                mrlslfImg = rowMeans(.[, mrlslfImgNames], na.rm = F),
                ) %>%
  dplyr::select(subjID, SlfEst, mrlIdInt, mrlIdExt, mrlslfImg)

df.perdist.v <- df.scales.v %>%
  dplyr::select(c(expID, subjID),perDistNames) %>%
  #dplyr::rowwise() %>%
  dplyr::mutate(sumRaw = rowMeans(.[3:31], na.rm = T),
                SelfSelfraw = SelfSelf,
                SelfGoodraw = rowMeans(.[grep("SelfGood", names(.))], na.rm = T),
                SelfNeutraw = rowMeans(.[grep("SelfNeut", names(.))], na.rm = T),
                SelfBadraw  = rowMeans(.[grep("SelfBad", names(.))], na.rm = T),
                SelfStraraw = rowMeans(.[grep("SelfStra", names(.))], na.rm = T),
                GoodNeutraw = rowMeans(.[grep("GoodNeut", names(.))], na.rm = T),
                GoodBadraw  = rowMeans(.[grep("GoodBad", names(.))], na.rm = T),
                NeutBadraw  = rowMeans(.[grep("NeutBad", names(.))], na.rm = T)) %>%
  dplyr::select(expID, subjID, sumRaw, SelfSelfraw, 
                SelfGoodraw, SelfNeutraw, SelfBadraw,
                SelfStraraw, GoodNeutraw, GoodBadraw, NeutBadraw) %>%
  dplyr::mutate(SelfSelf = SelfSelfraw/sumRaw,
                SelfGood = SelfGoodraw/sumRaw,
                SelfNeut = SelfNeutraw/sumRaw,
                SelfBad = SelfBadraw/sumRaw,
                SelfStra = SelfStraraw/sumRaw, 
                GoodNeut = GoodNeutraw/sumRaw, 
                GoodBad = GoodBadraw/sumRaw, 
                NeutBad = NeutBadraw/sumRaw) %>%
  dplyr::select(subjID, SelfSelf, SelfGood, SelfNeut, SelfBad,
                SelfStra, GoodNeut, GoodBad, NeutBad)

df.q_scores.v <- merge(df.q_scores.v, df.perdist.v)

## calculate correlation ----
df.corr <- merge(df.q_scores.v, df.meta_all_wide, by.x = 'subjID', by.y = 'Subject') %>%
  dplyr::select(-c(14)) %>%
  dplyr::select(-c(SelfSelf, SelfStra)) %>%
  dplyr::select(12, 13, 1, 14:15, everything()) %>%
  dplyr::mutate(subjID = as.character(subjID),
                Age = as.character(Age)) %>%
  dplyr::na_if("NaN")

# sapply(df.corr, class)

# standardize within each experiment.
df.corr_norm <- df.corr %>%
  dplyr::group_by(ExpID, Site) %>%
  mutate_if(is.numeric, scale) %>%
  dplyr::ungroup() %>%
  dplyr::mutate(RT_Good_Bad = RT_Good - RT_Bad,
                RT_Good_Neutral = RT_Good - RT_Neutral,
                RT_Bad_Neutral = RT_Bad -RT_Neutral,
                d_Good_Bad = dprime_Good - dprime_Bad,
                d_Good_Neutral = dprime_Good - dprime_Neut,
                d_Bad_Neutral = dprime_Bad - dprime_Neut) %>%
  dplyr::select(-contains("SD"))

# library(corrr)
res.cor_all <- df.corr_norm %>%
  dplyr::select(-c(ExpID:Sex)) %>%
  correlation::correlation(., method="pearson", p_adjust = 'none')

res.cor_1 <- df.corr_norm %>%
  dplyr::filter(ExpID %in% unique(df.meta_1_wide$ExpID)) %>%
  dplyr::select(-c(ExpID:Sex)) %>%
  correlation::correlation(., method="pearson", p_adjust = 'none')

res.cor_2 <- df.corr_norm %>%
  dplyr::filter(ExpID %in% unique(df.meta_2_wide$ExpID)) %>%
  dplyr::select(-c(ExpID:Sex)) %>%
  correlation::correlation(., method="pearson", p_adjust = 'none')

cor_pairs_all <- res.cor_all %>%
  dplyr::filter(Parameter1 %in% colnames(df.q_scores.v)) %>%
  dplyr::filter(Parameter2 %in% colnames(df.meta_all_wide[7:24])) %>%
  dplyr::filter(p <= 0.05) %>%
  dplyr::arrange(p)

cor_pairs_1 <- res.cor_1 %>%
  dplyr::filter(Parameter1 %in% colnames(df.q_scores.v)) %>%
  dplyr::filter(Parameter2 %in% colnames(df.meta_all_wide[7:24])) %>%
  dplyr::filter(p <= 0.05) %>%
  dplyr::arrange(p)

cor_pairs_2 <- res.cor_2 %>%
  dplyr::filter(Parameter1 %in% colnames(df.q_scores.v)) %>%
  dplyr::filter(Parameter2 %in% colnames(df.meta_all_wide[7:24])) %>%
  dplyr::filter(p <= 0.05) %>%
  dplyr::arrange(p)

write.csv(cor_pairs_all, 'sig_behav_quest_corr_pairs.csv', row.names = F)

```

```{r get plots of correlation, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Permutation
set.seed(12345)
permutation <- function(df) {
  v1 <- df[, 1] %>% base::sample(.)
  v2 <- df[, 2] %>% base::sample(.)
  tmp_cor <- cor(v1, v2, method = "pearson")
  tmp_cor
}

# plot for all
boot_plot_list <- list()
corr_plot_list <- list()
for (row_id in 1:nrow(cor_pairs_all)){
  
  # select variables
  var1 <- df.corr_norm %>% dplyr::select(cor_pairs_all$Parameter1[row_id])
  var2 <- df.corr_norm %>% dplyr::select(cor_pairs_all$Parameter2[row_id])
  var_tmp <- data.frame(var1, var2) %>%
    tidyr::drop_na()
  
  # boot
  boot_var <- var_tmp %>%
    bootES::bootES(., R = 5000, effect.type = 'r')
  
  cor_pairs_all$BootES_cor[row_id] <- boot_var$t0[1]
  cor_pairs_all$BootES_lb[row_id] <- boot_var$bounds[[1]]
  cor_pairs_all$BootES_ub[row_id] <- boot_var$bounds[[2]]
  
  # permutation
  per_cor <- rep(NA, 5000)
  for (i in 1:length(per_cor)){
    per_cor[i] <- permutation(var_tmp)
  }
  
  # plot scatter plot
  corr_plot_list[[row_id]] <- data.frame(var1, var2) %>%
    ggplot(., aes_string(x = cor_pairs_all$Parameter1[row_id], y = cor_pairs_all$Parameter2[row_id])) + 
    geom_point() + 
    geom_smooth(method=lm) +
    labs(title=paste(cor_pairs_all$Parameter1[row_id], '&', cor_pairs_all$Parameter2[row_id], sep = ' '), 
       x = cor_pairs_all$Parameter1[row_id], 
       y = cor_pairs_all$Parameter2[row_id]) +
    apatheme_s
  
  # plot permutation and boot 
  boot_cor <- boot_var$t %>%
    as.data.frame(.) %>%
    dplyr::arrange(V1) %>%
    dplyr::rename(corcoef = V1) %>%
    dplyr::mutate(Method = 'bootstrap')
    # dplyr::pull(V1)
  per_cor <- data.frame(per_cor) %>%
    dplyr::rename(corcoef = per_cor) %>%
    dplyr::mutate(Method = 'permutation')
  
  probs <- c(0.025, 0.975)
  quantiles <- quantile(per_cor$corcoef, prob=probs)
  
  # p_dist_df <- rbind(boot_cor, per_cor)
  
  xd <- data.frame(density(per_cor$corcoef)[c("x", "y")]) %>% dplyr::mutate(Method = 'permutation')
  yd <- data.frame(density(boot_cor$corcoef)[c("x", "y")]) %>% dplyr::mutate(Method = 'bootstrap')
  zd <- rbind(xd, yd)
  
  label_r <- paste("r = ", round(cor_pairs_all$r[row_id], 3), sep = '')
  
  boot_plot_list[[row_id]] <- ggplot(zd, aes(x, y, group=Method, colour = Method)) + 
      geom_area(data = subset(xd, x > quantiles[1] & x < quantiles[2]), fill = "grey") + # plot the 95 % area of zero
      geom_line() + 
      geom_vline(xintercept = cor_pairs_all$r[row_id], colour = 'blue') +
      geom_vline(xintercept = quantiles[1], colour = 'grey', linetype="dashed") + 
      geom_vline(xintercept = quantiles[2], colour = 'grey', linetype="dashed") + 
      geom_vline(xintercept = 0, colour = 'grey', linetype="dashed") + 
      # geom_text(aes(x = cor_pairs_all$r[row_id]*2, y = 6, label = label_r), colour = 'blue') +
      scale_color_grey() +
      apatheme_s
}
```
We focused on the task-questionnaire correlation, the results revealed that the score from three questionnaire are related to behavioral responses data. 
First, the external moral identity is positively correlated with boundary separation of moral good condition, $r = 0.194$, 95% CI [0.023 0.350]); the moral self image is positively correlated with the drift rate ($r = 0.191$, 95% CI [-0.016 0.354]) of the morally good condition. See Figure \@ref(fig:plot-corr-1).

```{r plot-corr-1, fig.cap="Correlation between moral identity and boundary separation of good condition; moral self-image and drift rate of good condition", fig.width=8, warning=FALSE}
library(patchwork)
corr_plot_list[[3]] + corr_plot_list[[5]]  + 
           boot_plot_list[[3]] + boot_plot_list[[5]] + plot_layout(ncol = 2)
```

Second, we found the personal distance between self and good is positively correlated with the boundary separation of neutral condition and the self-neutral distance is negatively correlated with the boundary separation of neutral condition. See figure \@ref(fig:plot-corr-2)


```{r plot-corr-2, fig.cap="Correlation between personal distance and boundary separation of neutral condition", fig.width=8, warning=FALSE}
library(patchwork)
corr_plot_list[[4]] + corr_plot_list[[6]] + 
           boot_plot_list[[4]] + boot_plot_list[[6]] + plot_layout(ncol = 2)
```

Third, we found the self esteem score was negative correlated with the $d'$ of bad conditions ($r = -0.16$, 95% CI [-0.277 -0.038]) and the neutral conditions ($r = -.197$, 95% CI [-0.348	-0.026]). See Figure \@ref(fig:plot-corr-3).

We also explored the correlation between behavioral data and questionnaire scores separately for experiments with and without self-referential. For experiments without self-referential (Valence effect), we found the personal distance between Good-person and self is positively correlated with boundary separation of good conditions, r = 0.292, 95% [0.071 0.485]. also personal distance between the bad and neutral person is positively correlated with non-responding time of bad and neutral conditions, r = 0.249, 0.233, respectively.

For experiments with self-referential (Valence effect for the self), we found self-esteem is negatively correlated with d prime of neutral condition, r = -0.272, [-0.468	-0.052], the self-good distance is positively correlated with d prime for Bad condition, r = 0.185, 95%CI[0.004 0.354].

```{r plot-corr-3, fig.cap="Correlation between self esteem and d prime of bad and neutral conditions", fig.width=8, warning=FALSE}
library(patchwork)
corr_plot_list[[1]] + corr_plot_list[[2]] +
  boot_plot_list[[1]] + boot_plot_list[[2]] + plot_layout(ncol = 2)
```

# Discussion

# References
```{r create_r-references, echo=FALSE,results='hide'}
#r_refs(file = "r-references.bib"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
