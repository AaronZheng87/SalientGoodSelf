---
title             : "Positive bias in perceptual matching may reflect an spontaneous self-referential processing"
shorttitle        : "Positive-bias as the spontaneous self-referential processing"

author: 
  - name          : "Hu Chuan-Peng"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Langenbeckstr. 1, Neuroimaging Center, University Medical Center Mainz, 55131 Mainz, Germany"
    email         : "hcp4715@gmail.com"
  - name          : "Kaiping Peng"
    affiliation   : "1"
  - name          : "Jie Sui"
    affiliation   : "1,3"

affiliation:
  - id            : "1"
    institution   : "Tsinghua University, 100084 Beijing, China"
  - id            : "2"
    institution   : "Leibniz Institute for Resilience Research, 55131 Mainz, Germany"
  - id            : "3"
    institution   : "University of Aberdeen, Aberdeen, Scotland"

authornote: |
  Hu Chuan-Peng, Department of Psychology, Tsinghua University, 100084 Beijing, China.
  Kaiping Peng, Department of Psychology, Tsinghua University, 100084 Beijing, China.
  Jie Sui, School of Psychology, University of Aberdeen, Aberdeen, Scotland.

  Authors contriubtion: HCP, JS, & KP design the study, HCP collected the data, HCP analyzed the data and drafted the manuscript. KP & JS supported this project.

abstract: |
  To navigate in a complex social world, individual has learnt to prioritize valuable information. Previous studies suggested the moral related stimuli was prioritized (Anderson, Siegel, et al., 2011, Science; Gantman & Van Bavel, 2014, Cognition). Using social associative learning paradigm, we found that when geometric shapes, without soical meaning, were associated with different moral valence (morally good, neutral, or bad), the shapes that associated with positive moral valence were prioritized in moral matching task. This patterns of results were robust across different procedures. Further, we tested whether this positive effect was modulated by self-relevance by manipulating the self-referential explicitly and found that the positive bias showed a large effect when positive valued stimuli were related to the self. This effect exist also when the self related information were presented as a task-irrelevant information. We also tested the specificity of the positive valence and found that this effect was not limited to moral domain. Interestingly, the better performance in reaction time is not correpsonding to self-rated psychological distance between self and a morally good-person, but with distance between self and morall bad-person. These results may suggest that our participants (College students in two different cities in China) have a positive moral self bias in perceptual processing, which drive the facilitated processing of morally good stimuli because of the spontaneous self-referential processing, and this trendency is not correlated with explicit rating of moral self.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Perceptual decision-making, Self, positive bias, morality"
wordcount         : "X"

bibliography      : 
  - r-references.bib
  - endnote.bib

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no
figsintext        : no

documentclass     : "apa6"
classoption       : "man"
output            : 
  papaja::apa6_pdf:
    latex_engine  : xelatex

---

```{r setup, include = FALSE}
#rm(list = ls())
if (.Platform$OS.type == 'windows') {
  Sys.setlocale(category = 'LC_ALL','English_United States.1250')
} else {
  Sys.setlocale(category = 'LC_ALL','en_US.UTF-8')
}
Sys.setenv(LANG = "en") # set the feedback language to English
options(scipen = 999)   # force R to output in decimal instead of scientific notion
options(digits=5)       # limit the number of reporting

pkgTest <- function(x){
        if (!require(x, character.only = TRUE)){
                install.packages(x, dep = TRUE)
                if(!require(x, character.only = TRUE)) stop("Package not found")
        }
}

pkgNeeded <- (c('here', 'mosaic', 'tidyverse', 'brms', 'tidybayes', 'patchwork', 'metafor', 'corrplot',"readr", "patchwork"))

lapply(pkgNeeded, pkgTest)
rm('pkgNeeded') # remove the variable 'pkgNeeded';

if(!"qgraph" %in% rownames(installed.packages())) install.packages("qgraph")
if(!"tidybayes" %in% rownames(installed.packages())) install.packages("tidybayes")

# Install devtools package if necessary
if(!"devtools" %in% rownames(installed.packages())) install.packages("devtools")

# Install the stable development versions from GitHub
if(!"papaja" %in% rownames(installed.packages())) devtools::install_github("crsh/papaja")

#windowsFonts(Times=windowsFont("TT Times New Roman")) # explicit mapping to "times"
apatheme = theme_bw()+
        theme(panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.background = element_blank(),
              panel.border = element_blank(),
              text=element_text(family='Times'),
              legend.title=element_blank(),
              legend.text = element_text(size =12),
              #legend.position='top',
              plot.title = element_text(lineheight=.8, face="bold", size = 16),
#              plot.title = element_text(lineheight=.8, face="bold", size = 16, hjust = 0.5),
              axis.text = element_text (size = 14, color = 'black'),
#              axis.text.x = element_text(angle = 45, vjust = 0.5),   # x-axis's label font
              axis.title = element_text (size = 14),
              axis.line.x = element_line(color='black', size = 1),   # increase the size of font
              axis.line.y = element_line(color='black', size = 1),   # increase the size of font
              axis.title.x = element_text(margin=margin(10,0,0,0)),  # increase the sapce betwen title and x axis
              axis.title.y = element_text(margin=margin(0,12,0,0)))  # increase the space between title and y axis

curDir = here::here() # dirname(rstudioapi::getActiveDocumentContext()$path)
figDir = here::here('figures')

# Seed for random number generation
set.seed(42)
options(tinytex.verbose = T) # debug the tex
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

```

# Introduction

Instead of using the mean of reaction or the Frequentist way to calculate the sensitivity under signal detection theory, We analyzed our data using Hierarchical Bayesian Drift Diffusion Model (HDDM). This was done for a few reasons: first, either mean or median of RTs are representative to the RT distribution, which is usually highly skewed; second, sequential sampling models have a concrete assumption about the generative process of RTs and accurate, provide stronger framework for testing hypothesis. Also, these models had been widely used in two alternative forced choice (TAFC) task, including perceptual matching task [van_zandt_comparison_2000].

Using HDDM to extract the dependent variable allow us to infer the perceptual process, at least better than we can infer when using mean or median-based GLM. Previous studies have showed that each parameter of the DDM is influence by different way. The initial bias is largely influenced by the proportion of each type of trial, the boundary is influenced by the instruction about the accuracy, while the drift rate is mainly related to relative perceptual salience or easiness of the perceptual decision-making.

Before analyzing the real data, we did parameter recovery to test the sanity of the model.

First, we simulated the data using the `hddm.generate.gen_rand_data` in HDDM model. To emulate the previous study on the perceptual matching task (same-faster-different-slow) and self-tagging task (manipulated effect only occur at the matched trials), we simulated six conditions. For the mismatching trials, we ....; For the matching trials, we simulated three levels, we varied drift rate ($v$) at three level: $1.5$, $2.5$, and $3.5$, while kept the other parameters fixed ($a = 1$, $t=0.4$, $z=0.5$).

XXXX
In perceptual matching, same is faster than different [@Krueger_1978; @Farell_1985].
Automatic processing [Spruyt_de_Houwer_2017]

@van_zandt_comparison_2000: A comparison of two response time models applied to perceptual matching

Yakushijin, ReikoJacobs, Robert A (2010), Are People Successful at Learning Sequential Decisions on a Perceptual Matching Task?

Schooler, L. J., Shiffrin, R. M., & Raaijmakers, J. G. W. (2001). A Bayesian model for implicit effects in perceptual identification. Psychological Review, 108(1), 257–272. https://doi.org/10.1037/0033-295X.108.1.257


Main findings:
(1) Morally good is prioritized;
(2) Moral self is prioritized;
(3) Moral self bias is relative (to neutral and bad self), especially when the task is not a speedy (time-limited)
(4) Task irrelevant coupling (weak effect)
(5) Correlation between behavioral performance and questionnaire.

What we controlled:
(1) familiarization effect (exp 1b)
(2) content is really related to moral? (exp 1c)
(3) memory effect or perceptual effect?
(4) how can we test it is the binding that matters?

# Methods
## Participants.
Most experiments (1a ~ 6b, except experiment 3b) reported in the current study were first finished between 2014 to 2016 in Tsinghua University, Beijing, China. Participants of these experiments were recruited in the local community. To increase the sample size so that each experiment has 50 or more valid data [@Simmons_2013_life], we recruited additional participants in Wenzhou University, Wenzhou, China in 2017 for experiment 1a, 1b, 4a, and 4b. Experiment 3b was finished in Wenzhou University in 2017. In the finial meta-analysis, we also included the data from two experiments (experiment 7a, 7b) that were reported in @Hu_2020_GoodSelf (See Table 1 for overview of these experiments). 
All participant received informed consent and compensated for their time. These experiments were approved by the ethic board in the Department of Tsinghua University. 

 <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Design and Procedure
This series of experiments was set out to test the effect of instantly acquired moral valence on perceptual decision-making. For this purpose, we used the social associative learning paradigm (or self-tagging paradigm)[@Sui_2012_JEPHPP], in which participants first learned the associations between geometric shapes and labels of person with different moral valence (e.g., in first three studies, the triangle, square, and circle and good person, neutral person, and bad person, respectively). The associations of the shapes and label were counterbalanced across participants. After remembered the associations, participants finished a perceptual matching task where they viewed one of the shapes upon the fixation while one of the labels below the fixation and judged whether the shape and the label matched the association they've learned. To get familiar with the task, participants practice the task and only started the experiment When their overall accuracy in the practice session was 60% or higher. 

The experiment 1a, 1b, 1c, 2, and 6a shared a 2 (matchness: matched vs. mismatched) by 3 (moral valence: good vs. neutral vs. bad) within-subject design. The experiment 1a was the first one of the whole series studies and 1b, 1c, and 2 were conducted to exclude the potential confounding factors. More specifically, experiment 1b used different Chinese words as labels to test whether the effect in experiment 1a was caused by the familiarity of the words. Experiment 1c manipulated the moral valence indirectly: participants first learn to associate different moral behaviors with different names of people, after remembered the association, they then performed the perceptual matching task by associating those names with different shapes. Experiment 2 further tested whether the way we presented the stimuli influence the effect of valence, by sequentially presenting labels and shapes. Note that part of participants of experiment 2 were from experiment 1a because we originally planned a cross task comparison. Experiment 6a, which shared the same design as experiment 2, was an EEG experiment which aimed at exploring the neural correlates of the effect. But we will focus on the behavioral results of experiment 6a in the current manuscript.

For experiment 3a, 3b, 6b, 7a, and 7b, we included self-relevance as another within-subject variable in the experimental design. For example, the experiment 3a directly extend the design of experiment 1a into a 2 (matchness: matched vs. mismatched) by 2 (reference: self vs. other) by 3 (moral valence: good vs. neutral vs. bad) within-subject design. Thus, in experiment 3a, there were six conditions (good-self, neutral-self, bad-self, good-other, neutral-other, and bad-other) and six shapes (triangle, square, circle, diamond, pentagon, and trapezoid). Experiment 3b was designed to separate the self-referential trials and other-referential trials and explore the change of task design on the effect we found in previous experiments. That is, participants finished two different blocks: in the self-referential blocks, they only response to good-self, neutral-self, and bad-self, with half of the trials was matched and half was not; for the other-reference blocks, they only responded to good-other, neutral-other, and bad-other. The experiment 6b was an EEG experiment extended from experiment 3a but presented the label and shape sequentially. Because of the relatively high working memory load (six label-shape pairs), experiment 6b were conducted in two days: the first day participants finished perceptual matching task as a practice, and the second day, they finished the task again while the EEG signals were recorded. The experiment 7a and 7b were design to test the cross task robustness of the effect. Given that we found the differences between neutral and bad conditions are negligible, we adopted a simpler experimental design in matching task of experiment 7a and 7b, i.e., with 2 (matchness: matched vs. mismatched) by 2 (reference: self vs. other) [see, @Hu_2020_GoodSelf].

Experiment 4a and 4b were design to test the implicit binding between self/other and moral valence. These two experiments were design to test whether the moral valence and self-referential will interact implicitly. For this purpose, in each experiment, only one variable was used as the explicit task and the other variable was rendered as task-irrlevant variable. In 4a, the explicit task was self-referential matching task, which two labels (self vs. other) and two shapes (circle, square) were used in the learning phase. The moral valence was manipulated by adding those labels within the shape. Participants were instructed to focus on the task and ignore the words in the shape, thus the moral valence was task-irrelevant variable. In 4b, we reversed the role of self-relevance and moral valence in the task: participants learnt three labels (good-person, neutral-person, and bad-person) and three shapes (circle, square, and triangle), and the words for self-relevance, "self" or "other", were presented in the shapes. As in 4a, participants were told to ignore the words inside the shape during the task.

Finally, experiment 5 was design to test the specificity of the moral valence. We extended the moral valence to broader domains. More specifically, besides the moral valence, we added valence from appearance of person (beautiful, neutral, ugly), apperance of a scene (beautiful, neutral, ugly), and emotion (happy, neutral, and sad). Label-shape pairs from different domains were separated into different blocks. 

E-prime 2.0 was used for presenting stimuli and collecting behavioral responses, except that experiment 7a and 7b used Matlab psychtoolbox [@Brainard_1997;@Pelli_1997]. For participants recruited in Tsinghua University, they finished the task individually in a dim-lighted chamber, stimuli were presented on 22-inch CRT monitors and their head were fixed by a chin-rest brace. The distance between participants' eyes and the screen was about 60 cm. The visual angle of geometric shapes was about 3.7º × 3.7º, the fixation cross is of (0.8º × 0.8º of visual angle) at the center of the screen. The words were of 3.6º × 1.6º visual angle. The distance between the center of the shape or the word and the fixation cross was 3.5º of visual angle. For participants recruited in Wenzhou University, they finished the experiment in a group consisted of 3 ~ 12 participants in a dim-lighted testing room. Participants were required to finished the whole experiment independently. Also, they were instructed to start the experiment at the same time, so that the distraction between participants were minimized. The stimuli were presented on 19-inch CRT monitor. The visual angles were not precisely controlled because participants’s chin were not fixed.

In most of these experiments, participant were also asked to fill a battery of questionnaire after they finish the behavioral tasks. All the questionnaire data were open [see, dataset 4 in @Liu_2020_JOPD]. See Table 1 for a summary information about all the experiments reported here. 

```{r 'Table1_exp_info', ehco = FALSE, results = 'asis'}
exp_table <- read.csv('Exp_info_all.csv') %>%
  dplyr::rename(ExpID = 1)
# knitr::kable(exp_table, caption = "Information about all experiments")
papaja::apa_table(
  exp_table
  , caption = "Information about all experiments."
  , note = "DV = dependent variables; Valence = how valence was manipulated; Shape & Label = how shapes & labels were presented."
  , escape = TRUE
)
```

## Data analysis
We reported all the measurements, analyses, and results in all the experiments in the current study. Participants whose overall accuracy lower than 60% were excluded from analysis. Also, the accurate responses with less than 200 ms reaction times were excluded from the analysis.

All data were first pre-processed using r cite_r("r-references.bib"). We first analyzed the reaction time and accuracy using Bayesian hierarchical generalized linear models, which is similar to the repeated measurement ANOVAs but can provide more information about the uncertainty. For the accuracy, we applied a signal detection theory approach. The performance in each match condition was combined with that in the non-matching condition with the same shape to form a measure of $d'$, the match trials were regarded as signal while the mismatching trials were regarded as noise [@Hu_2020_GoodSelf, @Sui_2012_JEPHPP]. Given that the matching and mismatching trials are presented in the same way and had same number of trials across all studies, we assume that participants' inner distribution of these two types of trials had equal variance but may had different means. That is, we used the equal variance Gaussian SDT model (EVSDT) here [Rouder_2005_BHM_SDT]. Trials without response were excluded from the analysis. Instead of using maximum likelihood function to estimate a single estimate for the $d'$ prime, we used the Bayesian hierarchical GLM model to estimate the sensitivity ($d'$) and criteria ($c'$) (see more details in supplementary methods). For the reaction time (RTs), we also used Bayesian hierarchical GLM to estimate the effect of our experimental manipulation. More specifically, we used the log-normal as the likelihood function for the GLM, which is equavelant to generalized linear mixed model  in Frequentists' approach, yet with more information about uncertainty in our inference. Either Bayesian generalized hierarchical model or Frequentists' generalized hierarchical model is better than taking the mean or median for each participant and then using ANOVAs [@Rousselet_2019]. 

To further understand the potential psychological processes under the perceptual decision-making task, we applied Bayesian hierarchical Drift Diffusion Model (HDDM) to the reaction times (RTs) and accuracy. We used python package HDDM [wiecki_hddm_2013] to fit the model and use the posterior of the parameters for statistical inferences. The drift diffusion model (DDM) was chosen because it's a generative model of the reaction time and accuracy, which means DDM has assumption about how the perceptual decision is made. In a simple form, DDM assume there are four psychological processes during a perceptual decision-making task, which can be captured by four parameters of the model. First, the behavior of the perceptual decision-making task consistent of non-decision process and decision process. Second, the decision process has three parameters: drift rate, which reflect the how fast the evidence is accumulated; boundary separation (a), which reflect the distance between two choices; and initial bias ($z$), which reflect how participant are bias toward each options. In the past decades, many works has been done and supported the model, from both human and animal studies.. These studies found that each parameter may reflect different psychological process. For instance, Voss & Voss (2009) found that the drift rate is modulated by the difficulty of task, the boundary separation is modulated by the criterion for each choice, the initial bias was modulated by the percentage of each options.

Previous studies using self-tagging paradigm also found that the shapes tagged by self were primarily have higher drift rate [@golubickis_self-prioritization_2017; @golubickis_valence_2019], or the self condition has both higher drift rate and higher boundary separation [reuther_does_2017].

### Valence effect
We synthesized effect size of *d* prime and RT from experiment 1a, 1b, 1c, 2, 5 and 6a for the valence effect. We reported the synthesized the effect across all experiments that tested the valence effect, using Bayesian hierarchical model. 

### Valence-self-relevance interaction
The results from experiment 3a, 3b, 6b, 7a, and 7b. These experiments explicitly included both moral valence and self-reference. 
### Implicit coupling between valence and self-relevance
In the third part, we examined the change of effect size brought by change of design, with a focus on 4a and 4b, which were designed to examine the implicit effect of the interaction between moral valence and self-referential processing. We are interested in one particular question: will self-referential and morally positive valence had a mutual facilitation effect. That is, when moral valence (experiment 4a) or self-referential (experiment 4a) was presented as task-irrelevant stimuli, whether they would facilitate self-referential or valence effect on perceptual decision-making. For experiment 4a, we report the comparisons between different valence conditions under the self-referential task, not the other-referential task; for experiment 4b, we reported the comparison between the self- vs. other-referential conditions for positive moral condition, not for the neutral or negative conditions. Note that the results were also analyzed in a standard repeated measure ANOVAs (see supplementary materials).

### Specificity of the valence effect
In this part, we reported the data from experiment 5, which included positive, neutral, and negative valence from four different domains: morality, aesthetic of person, aesthetic of scene, and emotion. This experiment was design to test whether the positive bias is specific to morality.

### Behavior-Questionnaire correlation

Finally, we explored correlation between results from behavioral results and self-reported measures. 
For the behavioral task part, we derived different indices. First, we used the mean and SD of the RT data from each participants of each condition. We included the RT variation because it has been shown to be meaningful as individual differences [Jensen, 1992; Ouyang et al., 2017]. Second, we used drift diffusion model to estimate four parameters of DDM for each participants. 
The DDM analyses were finished by HDDM, as reported in @Hu_2020_GoodSelf. That is, we used the response code approach, matched response were coded as 1 and mismatched responses were coded as 0. To fully explore all parameters, we allow all four parameters of DDM free to vary. We then extracted the estimation of all the four parameters for each participants for the correlation analyses.

For the questionnaire part, we are most interested in the self-rated distance between different person and self-evaluation related questionnaires: self-esteem, moral-self identity, and moral self-image. Other questionnaires (e.g., personality) were not planned to correlated with behavioral data were not included. Note that all data were reported in [@Liu_2020_JOPD].

```{r loadingData,echo=FALSE,results='hide'}
load("AllData.RData")
```

```{r define_funs,echo=FALSE,results='hide'}
# define a function to run the sdt GLMM for all exp with Matchness * Valence design
# for 1a, 1b, 1c, 2, 6a
fun_sdt_val <- function(exp_name) {
  df_name <- paste('df', exp_name, '.v', sep = '')
  m_name <- paste("glmmModels/exp", exp_name, "_sdt_m1_DummyCode", sep = '')
  df <- get(df_name)  # get the data by string
  
  m <- df %>%
  dplyr::filter(!is.na(RESP)) %>% # filter trials without response
  dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                saymatch = ifelse((Matchness == 'Match' & ACC == 1) | 
                                    (Matchness == 'Mismatch' & ACC == 0), 1, 0),
                Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
  brms::brm(saymatch ~ 0 + Valence + Valence:ismatch + 
              (0 + Valence + Valence:ismatch | Subject),
            family = bernoulli(link="probit"),
            data = .,
            control = list(adapt_delta = .99),
            iter = 4000,
            thin = 2,
            cores = parallel::detectCores(),
            file = here::here(m_name))
  return(m)
}

fun_plot_sdt_val <- function(m_sdt) {
    # extract c
    tmp_c <- m_sdt %>% 
      tidybayes::gather_draws(b_ValenceBad, b_ValenceNeutral, b_ValenceGood) %>%
      dplyr::rename(Valence = .variable, sdt_c = .value) %>% dplyr::ungroup() %>%
      dplyr::mutate(Valence = gsub("b_", "", Valence)) %>%
      dplyr::mutate(Valence = ifelse(stringr::str_detect(Valence, 'Bad'), 'Bad',
                                     ifelse(stringr::str_detect(Valence, 'Good'), 'Good', 'Neutral')))
    
    # dprime
    tmp_d <- m_sdt %>% 
      tidybayes::gather_draws(`b_ValenceBad:ismatch`, `b_ValenceNeutral:ismatch`, 
                              `b_ValenceGood:ismatch`) %>%
      dplyr::rename(Valence = .variable, sdt_d = .value) %>% dplyr::ungroup() %>%
      dplyr::mutate(Valence = gsub("b_", "", Valence)) %>%
      dplyr::mutate(Valence = ifelse(stringr::str_detect(Valence, 'Bad'), 'Bad',
                                     ifelse(stringr::str_detect(Valence, 'Good'), 'Good', 'Neutral')))
    
    # plot summaries with densities
    p_sdt_d_sum <- tmp_d %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      ggplot2::ggplot(aes(x = sdt_d, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(x = "sensitivity (d')", y = 'Posterior') +
      theme_classic()
    
    p_sdt_c_sum <- tmp_c %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      ggplot2::ggplot(aes(x = sdt_c, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(x = "criteria (c)", y = 'Posterior') +
      theme_classic()
    
    # plot comparison
    p_sdt_d <- tmp_d %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      tidybayes::compare_levels(sdt_d, by = Valence) %>%
      ggplot2::ggplot(aes(x = sdt_d, y = Valence, fill = stat(x > 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(x = "sensitivity (d')", y = 'Comparison') +
      theme_classic()
    
    p_sdt_c <- tmp_c %>%
      dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      tidybayes::compare_levels(sdt_c, by = Valence) %>%
      ggplot2::ggplot(aes(x = sdt_c, y = Valence, fill = stat(x > 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(x = "criteria (c)", y = 'Comparison') +
      theme_classic()
    
    return(list(p_sdt_d_sum, p_sdt_c_sum, p_sdt_d, p_sdt_c))
}

# define a function to run the RT GLMM for all exp with Matchness * Valence design
fun_rt_val <- function(exp_name) {
  df_name <- paste('df', exp_name, '.v', sep = '')
  m_name <- paste("glmmModels/exp", exp_name, "_rt_m1_DummyCode", sep = '')
  df <- get(df_name)  # get the data by string
  m <- df %>%
    dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
    dplyr::filter(ACC == 1) %>%
    dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                  Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good'))) %>%
    brms::brm(RT_sec ~ Valence*ismatch + (Valence*ismatch | Subject),
              family = shifted_lognormal(),
              data = ., control = list(adapt_delta = .99),
              iter = 4000,
              thin = 2,
              cores = parallel::detectCores(),
              file = here::here(m_name))
  return(m)
}

fun_plot_rt_val <- function(m_rt) {
    tmp_rt <- m_rt %>% 
      tidybayes::spread_draws(b_Intercept, b_ValenceBad, b_ValenceGood, 
                              b_ismatch,   `b_ValenceBad:ismatch`, `b_ValenceGood:ismatch`) %>%
      dplyr::mutate(Neut_MM = b_Intercept,
                    Bad_MM = Neut_MM + b_ValenceBad,
                    Good_MM = Neut_MM + b_ValenceGood,
                    Neut_M = Neut_MM + b_ismatch,
                    Bad_M = Neut_MM + b_ismatch + `b_ValenceBad:ismatch`,
                    Good_M = Neut_MM + b_ismatch + `b_ValenceGood:ismatch`) %>%
      dplyr::select(-contains('b_')) %>%
      tidyr::pivot_longer(cols = Neut_MM:Good_M,
                          names_to = 'cond',
                          values_to = 'logRT') %>%
      dplyr::mutate(RT = exp(logRT)*1000,
                    Matchness = dplyr::case_when(cond == 'Neut_MM' | cond == 'Bad_MM' | cond == 'Good_MM' ~ 'Mismatch',
                                                 cond == 'Neut_M'  | cond == 'Bad_M'  | cond == 'Good_M' ~ 'Match'),
                    Valence = dplyr::case_when(cond == 'Neut_MM' | cond == 'Neut_M' ~ 'Neutral',
                                               cond == 'Bad_MM'  | cond == 'Bad_M'  ~ 'Bad', 
                                               cond == 'Good_MM' | cond == 'Good_M' ~ 'Good'))
    p_exp1b_rt_m_sum <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      dplyr::filter(Matchness == 'Match') %>%
      ggplot2::ggplot(aes(x = RT, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(x = "RTs (Matching, ms)", y = 'Posterior') +
      theme_classic()
    p_exp1b_rt_mm_sum <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      dplyr::filter(Matchness == 'Mismatch') %>%
      ggplot2::ggplot(aes(x = RT, y = Valence)) +
      tidybayes::stat_halfeyeh() + 
      labs(tag = 'D', x = "RTs (Mismatching, ms)", y = 'Posterior') +
      theme_classic()
    p_exp1b_rt_m <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      dplyr::filter(Matchness == 'Match') %>%
      tidybayes::compare_levels(RT, by = Valence) %>%
      ggplot2::ggplot(aes(x = RT, y = Valence, fill = stat(x < 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(tag = 'C', x = "RTs (Matching, ms)", y = 'Comparison') +
      theme_classic()
    p_exp1b_rt_mm <- tmp_rt %>% dplyr::mutate(Valence = factor(Valence, levels = c('Bad', 'Neutral', 'Good'))) %>%
      dplyr::filter(Matchness == 'Mismatch') %>%
      tidybayes::compare_levels(RT, by = Valence) %>%
      ggplot2::ggplot(aes(x = RT, y = Valence, fill = stat(x < 0))) +
      tidybayes::stat_halfeyeh() + 
      geom_vline(xintercept =0, linetype = "dashed") +
      scale_fill_manual(values = c("gray80", "skyblue")) +
      labs(tag = 'D', x = "RTs (Mismatching, ms)", y = 'Comparison') +
      theme_classic()
    return(list(p_exp1b_rt_m_sum, p_exp1b_rt_mm_sum, p_exp1b_rt_m, p_exp1b_rt_mm))
}

```

# Experiment 1a

## Methods
### Participants
`r df1a.T.basic$N` college students (`r df1a.T.basic$Nf` female, age = `r df1a.T.basic$Age_mean` $\pm$ `r df1a.T.basic$Age_sd` years) participated. `r df1a.T.basic$N_thu` of them were recruited from Tsinghua University community in 2014; `r df1a.T.basic$N_wzu` were recruited from Wenzhou University in 2017. All participants were right-handed except one, and all had normal or corrected-to-normal vision. Informed consent was obtained from all participants prior to the experiment according to procedures approved by the local ethics committees. `r nrow(df1a.excld.sub)` participant’s data were excluded from analysis because nearly random level of accuracy, leaving `r df1a.v.basic$N` participants (`r df1a.v.basic$Nf` female, age = `r df1a.v.basic$Age_mean` $\pm$ `r df1a.v.basic$Age_sd` years).

### Stimuli and Tasks
Three geometric shapes were used in this experiment: triangle, square, and circle. These shapes were paired with three labels (bad person, good person or neutral person). The pairs were counterbalanced across participants. 

### Procedure
As we describe in general method part, this experiment had two phases. First, there was a learning stage. Participants were asked to learn the relationship between geometric shapes (triangle, square, and circle) and different person (bad person, a good person, or a neutral person). For example, a participant was told, “bad person is a circle; good person is a triangle; and a neutral person is represented by a square.” After participant remember the associations (usually in a few minutes), participants started a practicing phase of matching task which has the exact task as in the experimental task. 
In the experimental task, participants judged whether shape–label pairs, which were subsequently presented, were correct. Each trial started with the presentation of a central fixation cross for 500 ms. Subsequently, a pairing of a shape and label (good person, bad person, and neutral person) was presented for 100 ms. The pair presented could confirm to the verbal instruction for each pairing given in the training stage, or it could be a recombination of a shape with a different label, with the shape–label pairings being generated at random. The next frame showed a blank for 1100ms. Participants were expected to judge whether the shape was correctly assigned to the person by pressing one of the two response buttons as quickly and accurately as possible within this timeframe (to encourage immediate responding). Feedback (correct or incorrect) was given on the screen for 500 ms at the end of each trial, if no response detected, “too slow” was presented to remind participants to accelerate. Participants were informed of their overall accuracy at the end of each block. The practice phase finished and the experimental task began after the overall performance of accuracy during practice phase achieved 60%. 
For participants from the Tsinghua community, they completed 6 experimental blocks of 60 trials. Thus, there were 60 trials in each condition (bad-person matching, bad-person mismatching, good-person matching, good-person mismatching, neutral-person matching, and neutral-person mismatching). For the participants from Wenzhou University, they finished 6 blocks of 120 trials, therefore, 120 trials for each condition.

### Data analysis
As we describe in the general method section.

## Results
### GLM 
#### Signal detection theory analysis of accuracy
We fitted a Bayesian hierarchical GLM for SDT. The results showed that when the shapes were tagged with labels with different moral valence, the sensitivity ($d'$) and criteria ($c$) were both influence. For the $d'$, we found that the shapes tagged with morally good person (2.46, 95% CI[2.21 2.72]) is greater than shapes tagged with moral bad (2.07, 95% CI[1.83 2.32]), $P_{PosteriorComparison} = 1$. Shape tagged with morally good person is also greater than shapes tagged with neutral person (2.23, 95% CI[1.95 2.49]), $P_{PosteriorComparison} = 0.97$. Also, the shapes tagged with neutral person is greater than shapes tagged with morally bad person, $P_{PosteriorComparison} = 0.92$. 

Interesting, we also found the criteria for three conditions also differ, the shapes tagged with good person has the highest criteria (-1.01, [-1.14 -0.88]), followed by shapes tagged with neutral person(1.06, [-1.21 -0.92]), and then the shapes tagged with bad person(-1.11, [-1.25 -0.97]). However, pair-wise comparison showed that only showed strong evidence for the difference between good and bad conditions.

```{r 1a_BGLMM_sdt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp1a_sdt_m1 <- fun_sdt_val('1a')

summary(exp1a_sdt_m1)    # check summary
pp_check(exp1a_sdt_m1)  # posterior predictive check

hypothesis(exp1a_sdt_m1, "ValenceGood:ismatch > ValenceNeutral:ismatch")  # 0.97
hypothesis(exp1a_sdt_m1, "ValenceGood:ismatch > ValenceBad:ismatch")      # 1
hypothesis(exp1a_sdt_m1, "ValenceNeutral:ismatch > ValenceBad:ismatch")   # 0.91
hypothesis(exp1a_sdt_m1, "ValenceGood > ValenceNeutral")  # 0.82
hypothesis(exp1a_sdt_m1, "ValenceGood > ValenceBad")      # .95
hypothesis(exp1a_sdt_m1, "ValenceNeutral > ValenceBad")   # 0.81

exp1a_sdt_p <- fun_plot_sdt_val(exp1a_sdt_m1)
```
#### Reaction time
We fitted a Bayesian hierarchical GLM for RTs, with a log-normal distribution as the link function. We used the posterior distribution of the regression coefficient to make statistical inferences. As in previous studies, the matched conditions are much faster than the mismatched trials ($P_{PosteriorComparison} = 1$). We focused on matched trials only, and compared different conditions: Good () is faster than the neutral (), $P_{PosteriorComparison} = .99$, it was also faster than the Bad condition (), $P_{PosteriorComparison} = 1$. And the neutral condition is faster than the bad condition, $P_{PosteriorComparison} = 99$. However, the mismatched trials are largely overlapped. 

```{r 1a_BGLMM_rt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# fit a three-level hierarchical model for RT, didn't specify the prior
exp1a_rt_m1 <- fun_rt_val('1a')
#plot(exp1a_rt_m1, "b_")
summary(exp1a_rt_m1)  # n
pp_check(exp1a_rt_m1)

# Population-Level Effects: 
#                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept              -0.30      0.02    -0.33    -0.26 1.02      288      642  # baseline: mismatch:neutral = -.3
# ValenceBad              0.00      0.01    -0.01     0.01 1.00     3611     3116  # mismatch:neutral + mismatch:bad  = -.3
# ValenceGood             0.00      0.01    -0.01     0.01 1.00     2749     2691  # mismatch:neutral + mismatch:Good = -.3
# ismatch                -0.07      0.01    -0.09    -0.06 1.00     2203     2371  # mismatch:neutral + ismatch = (-0.30) + (-0.07) = -0.37
# ValenceBad:ismatch      0.02      0.01     0.00     0.05 1.00     2451     2495  # mismatch:neutral + ismatch + match:bad  = (-0.30) + (-0.07) + 0.02 = -0.35
# ValenceGood:ismatch    -0.04      0.02    -0.07    -0.01 1.00     1911     2326  # mismatch:neutral + ismatch + match:good = (-0.30) + (-0.07) +-0.04 = -0.41

hypothesis(exp1a_rt_m1, "ismatch < 0")  # Effect of matchness: Match < mis-match, p = 1
hypothesis(exp1a_rt_m1, "ValenceGood:ismatch < 0")  # Match good < Match Neutral, p = 0.99
hypothesis(exp1a_rt_m1, "ValenceBad:ismatch > 0")   # Match Bad > Match Neutral, p = 0.99
hypothesis(exp1a_rt_m1, "(ValenceGood:ismatch - ValenceBad:ismatch) < 0")   # Match Good < Match Bad, p = 1

exp1a_rt_p <- fun_plot_rt_val(exp1a_rt_m1)

exp1a_sdt_p[[1]] + exp1a_sdt_p[[2]] + exp1a_sdt_p[[3]] + exp1a_sdt_p[[4]] + exp1a_rt_p[[1]] + exp1a_rt_p[[2]] + exp1a_rt_p[[3]] + exp1a_rt_p[[4]] + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 4, byrow = FALSE)
```

### HDDM
We fitted our data with HDDM, using the response-coding [also see Hu_2020_GoodSelf]. We estimated separate drift rate ($v$), non-decision time ($T_{0}$), and boundary separation ($a$) for each condition. We found that the shapes tagged with good person has higher drift rate and higher boundary separation than shapes tagged with both neutral and bad person. Also, the shapes tagged with neutral person has a higher drift rate than shapes tagged with bad person, but not for the boundary separation. Finally, we found that shapes tagged with bad person had longer non-decision time (see figure ).

![Alternate Name](HDDM/exp1a_hddm_vta_pairplot.pdf){width=500px}

# Experiment 1b 
In this study, we aimed at excluding the potential confounding factor of the familiarity of words we used in experiment 1a, by matching the familiarity of the words.

## Method
### Participants
`r df1b.T.basic$N` college students (`r df1b.T.basic$Nf` female, age = `r df1b.T.basic$Age_mean` $\pm$ `r df1b.T.basic$Age_sd` years) participated. `r df1b.T.basic$N_thu` of them were recruited from Tsinghua University community in 2014; `r df1b.T.basic$N_wzu` were recruited from Wenzhou University in 2017. All participants were right-handed except one, and all had normal or corrected-to-normal vision. Informed consent was obtained from all participants prior to the experiment according to procedures approved by the local ethics committees. `r nrow(df1b.excld.sub)` participant’s data were excluded from analysis because nearly random level of accuracy, leaving `r df1b.v.basic$N` participants (`r df1b.v.basic$Nf` female, age = `r df1b.v.basic$Age_mean` $\pm$ `r df1b.v.basic$Age_sd` years).

### Stimuli and Tasks
Three geometric shapes (triangle, square, and circle, with 3.7º × 3.7º of visual angle) were presented above a white fixation cross subtending 0.8º × 0.8º of visual angle at the center of the screen. The three shapes were randomly assigned to three labels with different moral valence: a morally bad person (“恶人”, ERen), a morally good person (“善人”, ShanRen) or a morally neutral person (“常人”, ChangRen). The order of the associations between shapes and labels was counterbalanced across participants.
Three labels used in this experiment is selected based on the rating results from an independent survey, in which participants rated the familiarity, frequency, and concreteness of eight different words online. Of the eight words, three of them are morally positive (HaoRen, ShanRen, Junzi), two of them are morally neutral (ChangRen, FanRen), and three of them are morally negative (HuaiRen, ERen, LiuMang). An independent sample consist of 35 participants (22 females, age 20.6 ± 3.11) were recruited to rate these words. Based on the ratings (see supplementary materials Figure S1), we selected ShanRen, ChangRen, and ERen to represent morally positive, neutral, and negative person. 

### Procedure
For participants from both Tsinghua community and Wenzhou community, the procedure in the current study was exactly same as in experiment 1a. For participants in Tsinghua community, they finished a survey suite include personal distance, objective and subjective SES, belief in just world (Wu et al., 2011), disgust sensitivity scale (谭永红 et al., 2007), trait justice (Wu et al., 2014), and cognitive reflection test (Frederick, 2005). For participants from Wenzhou community, they finished exactly the same questionnaires as the participants from Wenzhou University in experiment 1a.

## Data Analysis
Data was analyzed as in experiment 1a. 

## Results
### GLM 
#### Signal detection theory analysis of accuracy
We fitted a Bayesian hierarchical GLM for SDT. The results showed that when the shapes were tagged with labels with different moral valence, the sensitivity ($d'$) and criteria ($c$) were both influence. For the $d'$, we found that the shapes tagged with morally good person (2.46, 95% CI[2.21 2.72]) is greater than shapes tagged with moral bad (2.07, 95% CI[1.83 2.32]), $P_{PosteriorComparison} = 1$. Shape tagged with morally good person is also greater than shapes tagged with neutral person (2.23, 95% CI[1.95 2.49]), $P_{PosteriorComparison} = 0.97$. Also, the shapes tagged with neutral person is greater than shapes tagged with morally bad person, $P_{PosteriorComparison} = 0.92$. 

Interesting, we also found the criteria for three conditions also differ, the shapes tagged with good person has the highest criteria (-1.01, [-1.14 -0.88]), followed by shapes tagged with neutral person(1.06, [-1.21 -0.92]), and then the shapes tagged with bad person(-1.11, [-1.25 -0.97]). However, pair-wise comparison showed that only showed strong evidence for the difference between good and bad conditions.

```{r 1b_BGLMM_sdt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp1b_sdt_m1 <- fun_sdt_val('1b')

summary(exp1b_sdt_m1)    # check summary
#pp_check(exp1b_sdt_m1)   # posterior predictive check

hypothesis(exp1b_sdt_m1, "ValenceGood:ismatch > ValenceNeutral:ismatch")  # 1
hypothesis(exp1b_sdt_m1, "ValenceGood:ismatch > ValenceBad:ismatch")      # .97
hypothesis(exp1b_sdt_m1, "ValenceNeutral:ismatch < ValenceBad:ismatch")   # 0.99
hypothesis(exp1b_sdt_m1, "ValenceGood > ValenceNeutral")  # 0.97
hypothesis(exp1b_sdt_m1, "ValenceGood > ValenceBad")      # 1
hypothesis(exp1b_sdt_m1, "ValenceNeutral > ValenceBad")   # 0.99

# extract the population level parameters
# criteria
exp1b_sdt_p <- fun_plot_sdt_val(exp1b_sdt_m1)
```

#### Reaction time
We fitted a Bayesian hierarchical GLM for RTs, with a log-normal distribution as the link function. We used the posterior distribution of the regression coefficient to make statistical inferences. As in previous studies, the matched conditions are much faster than the mismatched trials ($P_{PosteriorComparison} = 1$). We focused on matched trials only, and compared different conditions: Good () is faster than the neutral (), $P_{PosteriorComparison} = .99$, it was also faster than the Bad condition (), $P_{PosteriorComparison} = 1$. And the neutral condition is faster than the bad condition, $P_{PosteriorComparison} = 99$. However, the mismatched trials are largely overlapped. 

```{r 1b_BGLMM_rt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# fit a three-level hierarchical model for RT, didn't specify the prior
exp1b_rt_m1 <- fun_rt_val('1b')
#plot(exp1b_rt_m1, "b_")
summary(exp1b_rt_m1)  # n
pp_check(exp1b_rt_m1)

# Population-Level Effects: 
#                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept              -0.30      0.02    -0.33    -0.26 1.02      288      642  # baseline: mismatch:neutral = -.3
# ValenceBad              0.00      0.01    -0.01     0.01 1.00     3611     3116  # mismatch:neutral + mismatch:bad  = -.3
# ValenceGood             0.00      0.01    -0.01     0.01 1.00     2749     2691  # mismatch:neutral + mismatch:Good = -.3
# ismatch                -0.07      0.01    -0.09    -0.06 1.00     2203     2371  # mismatch:neutral + ismatch = (-0.30) + (-0.07) = -0.37
# ValenceBad:ismatch      0.02      0.01     0.00     0.05 1.00     2451     2495  # mismatch:neutral + ismatch + match:bad  = (-0.30) + (-0.07) + 0.02 = -0.35
# ValenceGood:ismatch    -0.04      0.02    -0.07    -0.01 1.00     1911     2326  # mismatch:neutral + ismatch + match:good = (-0.30) + (-0.07) +-0.04 = -0.41

hypothesis(exp1b_rt_m1, "ismatch < 0")  # Effect of matchness: Match < mis-match, p = 1
hypothesis(exp1b_rt_m1, "ValenceGood:ismatch < 0")  # Match good < Match Neutral, p = 0.99
hypothesis(exp1b_rt_m1, "ValenceBad:ismatch > 0")   # Match Bad > Match Neutral, p = 0.99
hypothesis(exp1b_rt_m1, "(ValenceGood:ismatch - ValenceBad:ismatch) < 0")   # Match Good < Match Bad, p = 1

exp1b_rt_p <- fun_plot_rt_val(exp1b_rt_m1)

exp1b_sdt_p[[1]] + exp1b_sdt_p[[2]] + exp1b_sdt_p[[3]] + exp1b_sdt_p[[4]] + exp1b_rt_p[[1]] + exp1b_rt_p[[2]] + exp1b_rt_p[[3]] + exp1b_rt_p[[4]] + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 4, byrow = FALSE)
```

### HDDM
We fitted our data with HDDM, using the response-coding [also see Hu_2020_GoodSelf]. We estimated separate drift rate ($v$), non-decision time ($T_{0}$), and boundary separation ($a$) for each condition. We found that the shapes tagged with good person has higher drift rate and higher boundary separation than shapes tagged with both neutral and bad person. Also, the shapes tagged with neutral person has a higher drift rate than shapes tagged with bad person, but not for the boundary separation. Finally, we found that shapes tagged with bad person had longer non-decision time (see figure ).


# Experiment 1c 
In this study, we further control the valence of words using in our experiment. Instead of using label with moral valence, we used valence-neutral names in China. Participant first learn behaviors of the different person, then, they associate the names and shapes. And then they perform a name-shape matching task.

## Method
### Participants
`r df1c.T.basic$N` college students (`r df1c.T.basic$Nf` female, age = `r df1c.T.basic$Age_mean` $\pm$ `r df1c.T.basic$Age_sd` years) participated. All of them were recruited from Tsinghua University community in 2014. Informed consent was obtained from all participants prior to the experiment according to procedures approved by the local ethics committees. No participant was excluded because they overall accuracy were above 0.6.

### Stimuli and Tasks
Three geometric shapes (triangle, square, and circle, with 3.7º × 3.7º of visual angle) were presented above a white fixation cross subtending 0.8º × 0.8º of visual angle at the center of the screen. The three most common names were chosen, which are neutral in moral valence before the manipulation.
Three names (Zhang, Wang, Li) were first paired with three paragraphs of behavioral description. Each description includes one sentence of biographic information and four sentences that describing the moral behavioral under that name. To assess the that these three descriptions represented good, neutral, and bad valence, we collected the ratings of three person on six dimensions: morality, likability, trustworthiness, dominance, competence, and aggressiveness, from an independent sample (n = 34, 18 female, age = 19.6 ± 2.05). The rating results showed that the person with morally good behavioral description has higher score on morality (M = 3.59, SD = 0.66) than neutral (M = 0.88, SD = 1.1), *t*(33) = 12.94, *p* < .001, and bad conditions (M = -3.4, SD = 1.1), *t*(33) = 30.78, *p* < .001. Neutral condition was also significant higher than bad conditions $t(33) = 13.9$, $p < .001$ (See supplementary materials).

### Procedure
After arriving the lab, participants were informed to complete two experimental tasks, first a social memory task to remember three person and their behaviors, after tested for their memory, they will finish a perceptual matching task. 
In the social memory task, the descriptions of three person were presented without time limitation. Participant self-paced to memorized the behaviors of each person. After they memorizing, a recognition task was used to test their memory effect. Each participant was required to have over 95% accuracy before preceding to matching task.
The perceptual learning task was followed, three names were randomly paired with geometric shapes. Participants were required to learn the association and perform a practicing task before they start the formal experimental blocks. They kept practicing util they reached 70% accuracy. Then, they would start the perceptual matching task as in experiment 1a. They finished 6 blocks of perceptual matching trials, each have 120 trials. 

## Data Analysis
Data was analyzed as in experiment 1a. 

## Results
### GLM 
#### Signal detection theory analysis of accuracy
We fitted a Bayesian hierarchical GLM for SDT. The results showed that when the shapes were tagged with labels with different moral valence, the sensitivity ($d'$) and criteria ($c$) were both influence. For the $d'$, we found that the shapes tagged with morally good person (2.46, 95% CI[2.21 2.72]) is greater than shapes tagged with moral bad (2.07, 95% CI[1.83 2.32]), $P_{PosteriorComparison} = 1$. Shape tagged with morally good person is also greater than shapes tagged with neutral person (2.23, 95% CI[1.95 2.49]), $P_{PosteriorComparison} = 0.97$. Also, the shapes tagged with neutral person is greater than shapes tagged with morally bad person, $P_{PosteriorComparison} = 0.92$. 

Interesting, we also found the criteria for three conditions also differ, the shapes tagged with good person has the highest criteria (-1.01, [-1.14 -0.88]), followed by shapes tagged with neutral person(1.06, [-1.21 -0.92]), and then the shapes tagged with bad person(-1.11, [-1.25 -0.97]). However, pair-wise comparison showed that only showed strong evidence for the difference between good and bad conditions.

```{r 1c_BGLMM_sdt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}

exp1c_sdt_m1 <- fun_sdt_val('1c')
summary(exp1c_sdt_m1)    # check summary
#pp_check(exp1c_sdt_m1)   # posterior predictive check

hypothesis(exp1c_sdt_m1, "ValenceGood:ismatch > ValenceNeutral:ismatch")  # 0.75
hypothesis(exp1c_sdt_m1, "ValenceGood:ismatch > ValenceBad:ismatch")      # .8
hypothesis(exp1c_sdt_m1, "ValenceNeutral:ismatch < ValenceBad:ismatch")   # 0.39
hypothesis(exp1c_sdt_m1, "ValenceGood > ValenceNeutral")  # 0.45
hypothesis(exp1c_sdt_m1, "ValenceGood > ValenceBad")      # 0.71
hypothesis(exp1c_sdt_m1, "ValenceNeutral > ValenceBad")   # 0.77

# extract the population level parameters
# criteria

exp1c_sdt_p <- fun_plot_sdt_val(exp1c_sdt_m1)
```

#### Reaction time
We fitted a Bayesian hierarchical GLM for RTs, with a log-normal distribution as the link function. We used the posterior distribution of the regression coefficient to make statistical inferences. As in previous studies, the matched conditions are much faster than the mismatched trials ($P_{PosteriorComparison} = 1$). We focused on matched trials only, and compared different conditions: Good () is faster than the neutral (), $P_{PosteriorComparison} = .99$, it was also faster than the Bad condition (), $P_{PosteriorComparison} = 1$. And the neutral condition is faster than the bad condition, $P_{PosteriorComparison} = 99$. However, the mismatched trials are largely overlapped. 

```{r 1c_BGLMM_rt, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
exp1c_rt_m1 <- fun_rt_val('1c')

#plot(exp1b_rt_m1, "b_")
summary(exp1c_rt_m1)  # n
pp_check(exp1c_rt_m1)

# Population-Level Effects: 
#                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept              -0.30      0.02    -0.33    -0.26 1.02      288      642  # baseline: mismatch:neutral = -.3
# ValenceBad              0.00      0.01    -0.01     0.01 1.00     3611     3116  # mismatch:neutral + mismatch:bad  = -.3
# ValenceGood             0.00      0.01    -0.01     0.01 1.00     2749     2691  # mismatch:neutral + mismatch:Good = -.3
# ismatch                -0.07      0.01    -0.09    -0.06 1.00     2203     2371  # mismatch:neutral + ismatch = (-0.30) + (-0.07) = -0.37
# ValenceBad:ismatch      0.02      0.01     0.00     0.05 1.00     2451     2495  # mismatch:neutral + ismatch + match:bad  = (-0.30) + (-0.07) + 0.02 = -0.35
# ValenceGood:ismatch    -0.04      0.02    -0.07    -0.01 1.00     1911     2326  # mismatch:neutral + ismatch + match:good = (-0.30) + (-0.07) +-0.04 = -0.41

hypothesis(exp1c_rt_m1, "ismatch < 0")  # Effect of matchness: Match < mis-match, p = 1
hypothesis(exp1c_rt_m1, "ValenceGood:ismatch < 0")  # Match good < Match Neutral, p = 0.5
hypothesis(exp1c_rt_m1, "ValenceBad:ismatch > 0")   # Match Bad > Match Neutral, p = 0.95
hypothesis(exp1c_rt_m1, "(ValenceGood:ismatch - ValenceBad:ismatch) < 0")   # Match Good < Match Bad, p = 0.86

exp1c_rt_p <- fun_plot_rt_val(exp1c_rt_m1)

exp1c_sdt_p[[1]] + exp1c_sdt_p[[2]] + exp1c_sdt_p[[3]] + exp1c_sdt_p[[4]] + exp1c_rt_p[[1]] + exp1c_rt_p[[2]] + exp1c_rt_p[[3]] + exp1c_rt_p[[4]] + plot_annotation(tag_levels = 'A')  + plot_layout(nrow = 4, byrow = FALSE)
```

### HDDM
We fitted our data with HDDM, using the response-coding [also see Hu_2020_GoodSelf]. We estimated separate drift rate ($v$), non-decision time ($T_{0}$), and boundary separation ($a$) for each condition. We found that the shapes tagged with good person has higher drift rate and higher boundary separation than shapes tagged with both neutral and bad person. Also, the shapes tagged with neutral person has a higher drift rate than shapes tagged with bad person, but not for the boundary separation. Finally, we found that shapes tagged with bad person had longer non-decision time (see figure ).




# Results
```{r first meta,echo=FALSE,results='hide'}
### exclude the repeated subj from the raw data (only for meta-analysis)

# No repeating subj
df1a.v_meta <- df1a.v

# No repeating subj
df1b.v_meta <- df1b.v

# exclude participant from exp 1a
df1c.v_meta <- df1c.v %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210))

# exclude participant from exp 1a
df2.v_meta <- df2.v %>% dplyr::filter(Subject > 2000)    

# exclude participants from ex1b, 1c, and 2
df3a.v_meta <- df3a.v %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) 

# No repeating subj
df3b.v_meta <- df3b.v

# No repeating subj
df4a.v_meta <- df4a.v

# exclude participants from ex1b, 1c, and 2
df4b.v_meta <- df4b.v %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201))   

# exclude participants from ex1b, 1c, and 2
df5.v_meta <- df5.v %>% dplyr::filter(!Subject %in% c(5201))   

# exclude participants from ex1b, 1c, and 2
df6a.v_meta <- df6a.v %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131))   

# exclude participants from ex1b, 1c, and 2
df6b.v_meta <- df6b_d1.v %>% dplyr::filter(!Subject %in% c(6217))   

# exclude participants from ex1b, 1c, and 2
df7a.v_meta <- df7a_m.v %>% dplyr::filter(!Subject %in% c(7020))   

# No repeating subj
df7b.v_meta <- df7b_m.v

# remove all unnecessary variables
#var_list <- c('df1a.v_meta', 'df1b.v_meta', 'df1c.v_meta', 'df2.v_meta', 'df3a.v_meta', 'df3b.v_meta',
#              'df4a.v_meta', 'df4b.v_meta', 'df5.v_meta', 'df6a.v_meta', 'df6b.v_meta', 'df7a.v_meta', 'df7b.v_meta',
#              'apatheme','exp_table', 'curDir', 'figDir')
#rm(list=ls()[! ls() %in% var_list])

df1a.v_meta$ExpID <- 'Exp1a'
df1b.v_meta$ExpID <- 'Exp1b'
df1c.v_meta$ExpID <- 'Exp1c'
df2.v_meta$ExpID <- 'Exp2'
df3a.v_meta$ExpID <- 'Exp3a'
df3b.v_meta$ExpID <- 'Exp3b'
df4a.v_meta$ExpID <- 'Exp4a'
df4b.v_meta$ExpID <- 'Exp4b'
df5.v_meta$ExpID <- 'Exp5'
df6a.v_meta$ExpID <- 'Exp6a'
df6b.v_meta$ExpID <- 'Exp6b'
df7a.v_meta$ExpID <- 'Exp7a'
df7b.v_meta$ExpID <- 'Exp7b'

### try meta-analysis 1a, 1b, 1c, 2, 5 and 6a
#selected_columns <- c('Subject','Age', 'Sex')

selected_columns <- c('ExpID', 'Site', 'Subject','Age', 'Sex', 'Matchness','Valence', 'RESP', 'ACC','RT')
df_moral <- dplyr::bind_rows(df1a.v_meta[selected_columns],
                             df1b.v_meta[selected_columns],
                             df1c.v_meta[selected_columns],
                             df2.v_meta[selected_columns],
                             df5.v_meta[selected_columns],
                             df6a.v_meta[selected_columns]) %>%
  dplyr::mutate(ExpID_new = paste(ExpID, Site, sep = "_")) %>%
  dplyr::mutate(Valence = factor(Valence, levels = c('Neutral', 'Bad', 'Good')))

df_moral_subj <- df_moral %>%
  dplyr::group_by(ExpID_new, Site) %>%
  dplyr::summarize(N = n_distinct(Subject),
                   N_trial = length(Subject),
                   Exp_conds = 6,
                   trial_per_cond = round((length(Subject)/6)/N, 0))

df_moral <- df_moral %>%
  dplyr::filter(!is.na(RESP)) %>% # filter trials without response
  dplyr::mutate(ismatch = ifelse(Matchness == 'Match', 1, 0),
                saymatch = ifelse((Matchness == 'Match' & ACC == 1) | 
                                    (Matchness == 'Mismatch' & ACC == 0), 1, 0)) %>%
  dplyr::select(ExpID_new, Subject, Valence, Matchness, RESP, ACC, RT, ismatch, saymatch) %>%
  dplyr::mutate(ismatch_num = ifelse(Matchness == 'Match', 0.5, -0.5))

# plot the nested structure of the data
with(df_moral, table(Subject, ExpID_new)) %>%
  image(
    col = grey.colors(80, start = 1, end = 0), 
    axes = TRUE, 
    xlab = "Subject", 
    ylab = "ExpID"
  )

# fit a three-level hierarchical model for SDT, didn't specify the prior; effect coding
#std_val_m1 <- brms::brm(saymatch ~ 0 + Valence + Valence:ismatch_num + 
#                         (0 + Valence + Valence:ismatch_num | ExpID_new) + 
#                         (0 + Valence + Valence:ismatch_num  | ExpID_new:Subject),
#                       family = bernoulli(link="probit"),
#                       data = df_moral,
#                       control = list(adapt_delta = .95),
#                       cores = parallel::detectCores(),
#                       file = here::here("glmmModels/sdt_val_EffectCode_3_level"))

#summary(std_val_m1)
#stancode(std_val_m1)

#plot(hypothesis(std_val_m1,
#                "ValenceBad:ismatch_num > ValenceNeutral:ismatch_num"))

#plot(hypothesis(std_val_m1,
#                "ValenceGood:ismatch_num > ValenceNeutral:ismatch_num"))

# Get the variables in the model
#var_name_m1 <- tidybayes::get_variables(std_val_m1)

#df_m1_post_sdt_exp <- std_val_m1 %>% 
#  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
#  dplyr::rename(value = r_ExpID_new)

#pop_mean <- std_val_m1 %>%
#  tidybayes::gather_draws(b_ValenceBad, b_ValenceNeutral, b_ValenceGood,
#                          `b_ValenceBad:ismatch_num`, `b_ValenceNeutral:ismatch_num`, 
#                          `b_ValenceGood:ismatch_num`) %>%
#  group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
#  tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
#tmp <- std_val_m1 %>% 
#  tidybayes::gather_draws(b_ValenceBad, b_ValenceNeutral, b_ValenceGood,
#                          `b_ValenceBad:ismatch_num`, `b_ValenceNeutral:ismatch_num`, 
#                          `b_ValenceGood:ismatch_num`) %>%
#  dplyr::rename(term = .variable,
#                pop_mean = .value) %>%
  #tidyr::separate(term, c(NA, 'term'), "_") 
#  dplyr::ungroup() %>%
#  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
#tmp2 <- merge(tmp, df_m1_post_sdt_exp, by = c('term','.chain','.iteration', '.draw'), all = T) %>%
#  dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population leve value to each experiment
#  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
#  dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
#df_m1_plot_sdt <- tmp %>%
#  dplyr::mutate(condition = 'Overall') %>%
#  dplyr::rename(value = pop_mean) %>%
#  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
#  dplyr::bind_rows(., tmp2) %>%
#  dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
#                                                         "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
#                                                         "Exp5_THU",  "Exp6a_THU","Overall")),
#                condition = forcats::fct_rev(condition), # reverse the order because the plot function auto reverse.
#                term = mosaic::derivedFactor("c_bad" = (term == "ValenceBad"),
#                                             "c_neutral" = (term == "ValenceNeutral"),
#                                             "c_good" = (term == "ValenceGood"),
#                                             "dprime_bad" = (term == "ValenceBad:ismatch_num"),
#                                             "dprime_neutral" = (term == "ValenceNeutral:ismatch_num"),
#                                             "dprime_good" = (term == "ValenceGood:ismatch_num"),
#                                             .method ="first", .default = NA),
#                term = factor(term, levels = c("c_bad", "c_neutral", "c_good",
#                                               "dprime_bad", "dprime_neutral", "dprime_good"))) %>%
#  tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
#  dplyr::mutate(diff_GB_c = c_good - c_bad,                           # calculate the differences between coditions
#                diff_GN_c = c_good - c_neutral,
#                diff_BN_c = c_bad - c_neutral,
#                diff_GB_dprm = dprime_good - dprime_bad,
#                diff_GN_dprm = dprime_good - dprime_neutral,
#                diff_BN_dprm = dprime_bad - dprime_neutral) %>%
#  dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
#               diff_GB_c,diff_GN_c, diff_BN_c,
#               diff_GB_dprm, diff_GN_dprm, diff_BN_dprm) %>%
#  tidyr::pivot_longer(cols = diff_GB_c:diff_BN_dprm, names_to = "term_diff", values_to =  "value") %>%  # wide to long
#  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_c','diff_GN_c', 'diff_BN_c',
#                                                         'diff_GB_dprm', 'diff_GN_dprm', 'diff_BN_dprm')))

#df_m1_plot_sdt %>% 
#  dplyr::group_by(condition, term_diff, .chain) %>%
#  dplyr::tally()

# plot the posterior of c
#df_m1_plot_sdt  %>%
#  dplyr::filter(str_detect(term_diff, '_c')) %>%
#  ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x > 0))) +
#  tidybayes::stat_halfeyeh() +
#  geom_vline(xintercept = 0, linetype = "dashed") +
#  scale_fill_manual(values = c('gray80', 'skyblue')) + 
#  facet_wrap( ~ term_diff,
#               scales = "free_y", nrow = 1,
#               labeller = label_parsed)

# plot the posterior of dprime
#df_m1_plot_sdt %>%
#  dplyr::filter(str_detect(term_diff, '_dprm')) %>%
#  ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x > 0))) +
#  tidybayes::stat_halfeyeh() +
#  geom_vline(xintercept = 0, linetype = "dashed") +
#  scale_fill_manual(values = c('gray80', 'skyblue')) + 
#  facet_wrap( ~ term_diff,
#               scales = "free_y", nrow = 1,
#               labeller = label_parsed)

# 
#mcmc_plot(std_val_m1, pars = 1:4, type = "dens")

# posterior predictive check
#pp_check(std_val_m1)
#pp_std_val_m1 <- 
#  brms::pp_check(std_val_m1, nsamples = 1e2) + 
#  ggtitle("PPC std_val_m1") +
#  theme_bw (base_size = 10) + 
#  theme(legend.position = "none") +
#  xlim(-0.5, 1.5)

# have a look at a few participants' data
#set.seed(123)
#random_sub <- sample(unique(df_moral$Subject), 4)
#random_sub

#df_moral %>%
#  dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
#  dplyr::filter(ACC == 1) %>%
#  dplyr::filter(Subject %in% random_sub) %>%
#  dplyr::mutate(cond = paste(Matchness, Valence, sep = "_"),
#                RT_log = log(RT_sec))%>%
#  ggplot2::ggplot(., aes(x=RT_log)) + 
#    geom_histogram(aes(fill=cond), alpha=0.5, bins=60) + 
#    facet_grid(~Subject) +  # One panel per id
#    coord_cartesian(xlim=c(-2, 1))


# fit a three-level hierarchical model for RT, didn't specify the prior
#RT_val_m1 <- df_moral %>%
#  dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
#  dplyr::filter(ACC == 1) %>%
#  brms::brm(RT_sec ~ Valence*ismatch_num + 
#              (Valence*ismatch_num | ExpID_new) +   
#              (Valence*ismatch_num  | ExpID_new:Subject),
#            family=shifted_lognormal(),
#            data = .,
#            control = list(adapt_delta = .95),
#            cores = parallel::detectCores(),
#            file = here::here("glmmModels/RT_val_EffectCode_3_level"))
#plot(RT_val_m1, "b_")
#summary(RT_val_m1)  # ndt = 0 there fore, we used lognormal.
#pp_check(RT_val_m1)

#Population-Level Effects: 
#                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
#Intercept                  -0.40      0.06    -0.52    -0.27 1.01      837     1301  # baseline: mismatch:neutral
#ValenceBad                  0.01      0.00     0.00     0.02 1.00     1752     2540  # mismatch:bad - mismatch:neutral = 0.01
#ValenceGood                -0.03      0.00    -0.04    -0.02 1.00     1237     2219  # mismatch:Good - mismatch:neutral = -0.03
#ismatch_num                -0.07      0.01    -0.09    -0.06 1.00     1638     1957  # match:neutral - mismatch:neutral = -0.07
#ValenceBad:ismatch_num      0.02      0.01     0.00     0.04 1.00     1597     2380  # match:bad - ValenceBad -ismatch_num = 0.02
#ValenceGood:ismatch_num    -0.05      0.01    -0.07    -0.03 1.00     1424     1775  # match:good - ValenceGood- ismatch_num = -0.05

# Mismatch:Neutral - Intercept = -0.4
# Mismatch:Bad     - Intercept  + ValenceBad = -0.4 + 0.01 = -0.39
# Mismatch:Good    - Intercept  + ValenceGood = -0.4 - 0.03 = -0.43
# Match: Neutral   - Intercept  + ismatch_num = -0.4 - 0.07 = -0.47
# Match: Bad       - Intercept  + ismatch_num + ValenceBad+ ValenceBad:ismatch_num = -0.4 + 0.01 + 0.02 =  -0.37 
# Match: Good      - Intercept  + ismatch_num + ValenceGood+ ValenceGood:ismatch_num = -0.4 + (-0.03) + (-0.05) = -0.48

# log normal distribution, dummy coding
#RT_val_m2 <- df_moral %>%
#  dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
#  dplyr::filter(ACC == 1) %>%
#  brms::brm(RT_sec ~ Valence*ismatch + 
#              (Valence*ismatch | ExpID_new) +   
#              (Valence*ismatch | ExpID_new:Subject),
#            family=lognormal(),
#            data = .,
#            control = list(adapt_delta = .98),
#            cores = parallel::detectCores(),
#            file = here::here("glmmModels/RT_val_EffectCode_3_level_m2"))
#summary(RT_val_m2)


# log normal distribution, with truncated distribution, , dummy coding
#RT_val_m3_trunc <- df_moral %>%
#  dplyr::mutate(RT_sec = RT/1000) %>% # log RT in seconds
#  dplyr::filter(ACC == 1) %>%
#  brms::brm(RT_sec|trunc(lb = 0.2, ub = 1.1) ~ Valence*ismatch + 
#              (Valence*ismatch | ExpID_new) +   
#              (Valence*ismatch | ExpID_new:Subject),
#            family=lognormal(),
#            data = .,
#            control = list(adapt_delta = .98),
#            cores = parallel::detectCores(),
#            file = here::here("glmmModels/RT_val_EffectCode_3_level_m3_trunc"))
#summary(RT_val_m3_trunc)
#pp_check(RT_val_m3_trunc)
#plot(RT_val_m3_trunc, "b_")

# compare three models
#loo(RT_val_m1, RT_val_m2, RT_val_m3_trunc)
#bayes_factor(RT_val_m1,RT_val_m2)
#Monte Carlo SE of elpd_loo is 0.4.

#All Pareto k estimates are good (k < 0.5).
#See help('pareto-k-diagnostic') for details.

#Model comparisons:
#                elpd_diff se_diff
#RT_val_m1          0.0       0.0 
#RT_val_m2         -5.6       3.3 
#RT_val_m3_trunc -590.6      34.8 

# Get the variables in the model 1
#RT_var_name_m1 <- tidybayes::get_variables(RT_val_m1)

#df_m1_post_rt_exp <- RT_val_m1 %>% 
#  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
#  dplyr::rename(value = r_ExpID_new)

#rt_pop_mean <- RT_val_m1 %>%
#  tidybayes::gather_draws(b_Intercept, b_ValenceBad, b_ValenceGood,
#                          `b_ismatch_num`, `b_ValenceBad:ismatch_num`, 
#                          `b_ValenceGood:ismatch_num`) %>%
#  group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
#  tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
#rt_pop_post <- RT_val_m1 %>% 
#  tidybayes::gather_draws(b_Intercept, b_ValenceBad, b_ValenceGood,
#                          `b_ismatch_num`, `b_ValenceBad:ismatch_num`, 
#                          `b_ValenceGood:ismatch_num`) %>%
#  dplyr::rename(term = .variable,
#                pop_mean = .value) %>%
  #tidyr::separate(term, c(NA, 'term'), "_") 
#  dplyr::ungroup() %>%
#  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
#rt_post_tmp <- merge(rt_pop_post, df_rt_m1_post_exp, 
#                     by = c('term','.chain','.iteration', '.draw'), all = T) %>%
#  dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population leve value to each experiment
#  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
#  dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
#df_m1_rt_plot <- rt_pop_post %>%
#  dplyr::mutate(condition = 'Overall') %>%
#  dplyr::rename(value = pop_mean) %>%
#  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
#  dplyr::bind_rows(., rt_post_tmp) %>%
#  dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
#                                                         "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
#                                                         "Exp5_THU",  "Exp6a_THU","Overall")),
#                condition = forcats::fct_rev(condition)#, # reverse the order b/c plot function auto reverse.
                #term = mosaic::derivedFactor("Neutral_NM" = (term == "Intercept"),
                #                             "diff_BN_NM" = (term == "ValenceBad"),
                #                             "diff_GN_NM" = (term == "ValenceGood"),
                #                             "Neutral_M" = (term == "ismatch_num"),
                #                             "Bad_M_int" = (term == "ValenceBad:ismatch_num"),
                #                             "Good_M_int" = (term == "ValenceGood:ismatch_num"),
                #                             .method ="first", .default = NA),
                #term = factor(term, levels = c("Bad_NM", "Neutral_NM", "Good_NM",
                 #                              "Bad_M", "Neutral_M", "Good_M"))
                ) %>%
#  tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
#  dplyr::mutate(diff_GB_NM = ValenceGood - ValenceBad,               # calculate the differences between coditions
#                diff_GN_NM = ValenceGood,
#                diff_BN_NM = ValenceBad,
#                diff_GN_M = ValenceGood + `ValenceGood:ismatch_num`,
#                diff_BN_M = ValenceBad + `ValenceBad:ismatch_num`,
#                diff_GB_M = diff_GN_M - diff_BN_M) %>%
#  dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
#               diff_GB_NM,diff_GN_NM, diff_BN_NM,
#               diff_GB_M, diff_GN_M, diff_BN_M) %>%
#  tidyr::pivot_longer(cols = diff_GB_NM:diff_BN_M, names_to = "term_diff", values_to =  "value") %>%  # wide to long
#  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_NM','diff_GN_NM', 'diff_BN_NM',
#                                                         'diff_GB_M', 'diff_GN_M', 'diff_BN_M')))

#df_m1_rt_plot %>% 
#  dplyr::group_by(condition, term_diff, .chain) %>%
#  dplyr::tally()

#df_m1_rt_mean <- df_m1_rt_plot %>%
  #tidybayes::gather_draws(b_Intercept, b_ValenceBad, b_ValenceGood,
  #                        `b_ismatch_num`, `b_ValenceBad:ismatch_num`, 
  #                        `b_ValenceGood:ismatch_num`) %>%
#  group_by(condition, term_diff) %>%       # this line not necessary (done automatically by spread_draws)
#  tidybayes::mean_hdci(value)  # get the high density continuous intervals


# plot the posterior of mismatch
#df_m1_rt_plot  %>%
#  dplyr::filter(str_detect(term_diff, '_NM')) %>%
#  ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x < 0))) +
#  tidybayes::stat_halfeyeh() +
#  geom_vline(xintercept = 0, linetype = "dashed") +
#  scale_fill_manual(values = c('gray80', 'skyblue')) + 
#  facet_wrap( ~ term_diff,
#               scales = "free_y", nrow = 1,
#               labeller = label_parsed)

# plot the posterior of matching trials
#df_m1_rt_plot %>%
#  dplyr::filter(str_detect(term_diff, '_M')) %>%
#  ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x < 0))) +
#  tidybayes::stat_halfeyeh() +
#  geom_vline(xintercept = 0, linetype = "dashed") +
#  scale_fill_manual(values = c('gray80', 'skyblue')) + 
#  facet_wrap( ~ term_diff,
#               scales = "free_y", nrow = 1,
#               labeller = label_parsed)

# Get the variables in the model 3
#RT_var_name_m3 <- tidybayes::get_variables(RT_val_m3_trunc)

#df_m3_post_rt_exp <- RT_val_m3_trunc %>% 
#  tidybayes::spread_draws(r_ExpID_new[condition, term]) %>%
#  dplyr::rename(value = r_ExpID_new)

#rt_pop_mean <- RT_val_m3_trunc %>%
#  tidybayes::gather_draws(b_Intercept, b_ValenceBad, b_ValenceGood,
#                          `b_ismatch`, `b_ValenceBad:ismatch`, 
#                          `b_ValenceGood:ismatch`) %>%
#  group_by(.variable) %>%       # this line not necessary (done automatically by spread_draws)
#  tidybayes::mean_hdci(.value)  # get the high density continuous intervals

# extract the population level parameters
#rt_pop_post <- RT_val_m3_trunc %>% 
#  tidybayes::gather_draws(b_Intercept, b_ValenceBad, b_ValenceGood,
#                          `b_ismatch`, `b_ValenceBad:ismatch`, 
#                          `b_ValenceGood:ismatch`) %>%
#  dplyr::rename(term = .variable,
#                pop_mean = .value) %>%
  #tidyr::separate(term, c(NA, 'term'), "_") 
#  dplyr::ungroup() %>%
#  dplyr::mutate(term = gsub("b_", "", term))

# Add the population mean to the experimental level and only keep the value after.
#rt_post_tmp <- merge(rt_pop_post, df_m3_post_rt_exp, 
#                     by = c('term','.chain','.iteration', '.draw'), all = T) %>%
#  dplyr::mutate(mean_value = pop_mean + value) %>%  # add the population leve value to each experiment
#  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, mean_value) %>% # select the added value
#  dplyr::rename(value = mean_value)  # rename the value

# Plot the difference between conditions 
#df_m3_rt_plot <- rt_pop_post %>%
#  dplyr::mutate(condition = 'Overall') %>%
#  dplyr::rename(value = pop_mean) %>%
#  dplyr::select(condition, term, `.chain`, `.iteration`, `.draw`, value) %>%
#  dplyr::bind_rows(., rt_post_tmp) %>%
#  dplyr::mutate(condition = factor(condition, levels = c("Exp1a_THU", "Exp1a_WZU", "Exp1b_THU", 
#                                                         "Exp1b_WZU", "Exp1c_THU", "Exp2_THU" , 
#                                                         "Exp5_THU",  "Exp6a_THU","Overall")),
#                condition = forcats::fct_rev(condition)#, # reverse the order b/c plot function auto reverse.
#                ) %>%
#  tidyr::pivot_wider(names_from = c(term), values_from = value) %>%   # long to wide
#  dplyr::mutate(diff_GB_NM = ValenceGood - ValenceBad,               # calculate the differences between coditions
#                diff_GN_NM = ValenceGood,
#                diff_BN_NM = ValenceBad,
#                diff_GN_M = ValenceGood + `ValenceGood:ismatch`,
#                diff_BN_M = ValenceBad + `ValenceBad:ismatch`,
#                diff_GB_M = diff_GN_M - diff_BN_M) %>%
#  dplyr::select(condition, `.chain`, `.iteration`, `.draw`,
#               diff_GB_NM,diff_GN_NM, diff_BN_NM,
#               diff_GB_M, diff_GN_M, diff_BN_M) %>%
#  tidyr::pivot_longer(cols = diff_GB_NM:diff_BN_M, names_to = "term_diff", values_to =  "value") %>%  # wide to long
#  dplyr::mutate(term_diff = factor(term_diff, levels = c('diff_GB_NM','diff_GN_NM', 'diff_BN_NM',
#                                                         'diff_GB_M', 'diff_GN_M', 'diff_BN_M')))

#df_m3_rt_plot %>% 
#  dplyr::group_by(condition, term_diff, .chain) %>%
#  dplyr::tally()

#df_m3_rt_mean <- df_m3_rt_plot %>%
#  group_by(condition, term_diff) %>%       # this line not necessary (done automatically by spread_draws)
#  tidybayes::mean_hdci(value)  # get the high density continuous intervals


# plot the posterior of mismatch
#df_m3_rt_plot  %>%
#  dplyr::filter(str_detect(term_diff, '_NM')) %>%
#  ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x < 0))) +
#  tidybayes::stat_halfeyeh() +
#  geom_vline(xintercept = 0, linetype = "dashed") +
#  scale_fill_manual(values = c('gray80', 'skyblue')) + 
#  facet_wrap( ~ term_diff,
#               scales = "free_y", nrow = 1,
#               labeller = label_parsed)

# plot the posterior of matching trials
# df_m3_rt_plot %>%
#   dplyr::filter(str_detect(term_diff, '_M')) %>%
#   ggplot2::ggplot(aes(y = condition, x = value, fill = stat(x < 0))) +
#   tidybayes::stat_halfeyeh() +
#   geom_vline(xintercept = 0, linetype = "dashed") +
#   scale_fill_manual(values = c('gray80', 'skyblue')) + 
#   facet_wrap( ~ term_diff,
#                scales = "free_y", nrow = 1,
#                labeller = label_parsed)

```

```{r second meta,echo=FALSE,results='hide'}
# Results part 2: with self-referential, included experiments: 3a, 3b, 6b, 7a, 7b

# Combine the data  ----
df.meta_d_2 <- rbind(df3a.meta.d, df3b.meta.d, df6b.meta.d, df7a_m.meta.d, df7b_m.meta.d) 
df.meta_rt_2 <- rbind(df3a.meta.rt, df3b.meta.rt, df6b.meta.rt, df7a_m.meta.rt, df7b_m.meta.rt)

# Calculate the mean, sd, n, and r ----
# for estimating the effect size and SE of effect size.
effectList_2 <- c('Good_Bad_S','Good_Neut_S','Neut_Bad_S',
                'Good_Bad_O','Good_Neut_O','Neut_Bad_O')

df.ES_2 <- data.frame(matrix(, nrow=length(unique(df.meta_d_2$ExpID))*length(effectList_2)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df.meta_d_2$ExpID))*length(effectList_2)),
                ExpID  = rep(rep(unique(df.meta_d_2$ExpID), each = length(effectList_2)), 2),
                Effect = rep(effectList_2, length(unique(df.meta_d_2$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df.meta_rt_2 %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df.meta_d_2 %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_2){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Good_Bad_S'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Good_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Neut_Bad_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Bad_O'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Neut_Bad_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
      }
      
      M1  <- mean(dataCond1$Value) -> df.ES_2$M1[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_2$SD1[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_2$M2[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_2$SD2[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_2$N[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_2$r[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_2$ES[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] <- tmp2[1,1]
      df.ES_2$ES.var[df.ES_2$DVtype == DVName & df.ES_2$ExpID == expName & df.ES_2$Effect == effectName] <- tmp2[1,2]
    }
  }
}

# Do the meta ----
# info about participants
df.ES_2_sum <- df.ES_2 %>% 
  dplyr::group_by(DVtype, Effect) %>% 
  tidyr::drop_na() %>% 
  dplyr::summarise(Nexp = length(unique(ExpID)), Nsubj = sum(N, na.rm = T))

df.res.meta_2 <- data.frame(matrix(, nrow= (2*3)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = (2*3)),
                Effect = rep(effectList_2, 2),
                N_exp = NA, Cohen_d = NA, se = NA, CI_low = NA, CI_upp = NA, pval = NA)

# meta -analysis
for (DVName in c('RT','dprime')){
  for (effectName in effectList_2){
    df.res.meta <- df.ES_2 %>%
      dplyr::filter(DVtype == DVName & Effect == effectName) %>%
      tidyr::drop_na()
  
    tmp.meta.res <- metafor::rma(yi = df.res.meta$ES,
                           vi = df.res.meta$ES.var,
                           slab = df.res.meta$ExpID)
    df.res.meta_2$N_exp[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$k
    df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$beta
    df.res.meta_2$se[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$se
    df.res.meta_2$CI_low[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$ci.lb
    df.res.meta_2$CI_upp[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$ci.ub
    df.res.meta_2$pval[df.res.meta_2$DVtype == DVName & df.res.meta_2$Effect == effectName] <- tmp.meta.res$pval
  }
}

# plot the effect size  ----
df.res.meta_2 <- df.res.meta_2 %>%
  dplyr::mutate(Identity = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Neut_S" | Effect == "Neut_Bad_S",
                                  "Self-Ref.", "Other-Ref."),
                EffectType = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Bad_O", "Good_Bad",
                                    ifelse(Effect == "Good_Neut_S" | Effect == "Good_Neut_O", "Good_Neut", "Neut_Bad")))

df.res_meta_pdata <- rbind(df.res.meta_1, df.res.meta_2) %>%
  dplyr::mutate(Identity = factor(Identity, levels = c("No-Ref.", "Self-Ref.", "Other-Ref.")),
                EffectType = factor(EffectType, levels = c("Good_Bad", "Good_Neut", "Neut_Bad" )))

```

## Effect of moral valence

```{r plot-all-effect, fig.cap="Effect size (Cohen's *d*) of Valence.", fig.width=12, warning=FALSE}
#p_meta_val <- 
  df.res_meta_pdata %>%
  #dplyr::filter(Identity == "Self") %>%
  ggplot(., aes(x=DVtype, y=Cohen_d, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=Cohen_d - 1.96*se, ymax = Cohen_d + 1.96*se), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  #ggtitle('Valence effect') +
  #geom_bar(stat="identity", color=NA, 
  #         position=position_dodge()) +
  #geom_errorbar(aes(ymin=Cohen_d - 1.96*se, ymax = Cohen_d + 1.96*se), width=.2,
  #               position=position_dodge(.9)) +
  coord_cartesian(ylim=c(-1.5, 1.5))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  apatheme +
  facet_wrap( ~ Identity, nrow = 1)
```
In this part, we synthesized results from experiment 1a, 1b, 1c, 2, 5 and 6a. Data from 192 participants were included in these analysis. We found differences between positive and negative conditions on RT was Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Bad']`]; on *d'* was Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Bad']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Good_Neut']`]; *d'*: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Good_Neut']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'RT' & df.res.meta_1$Effect =='Neut_Bad']`]; *d'*: Cohen's *d* = `r df.res.meta_1$Cohen_d[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']` $\pm$ `r df.res.meta_1$se[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']`, 95% CI [`r df.res.meta_1$CI_low[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']` `r df.res.meta_1$CI_upp[df.res.meta_1$DVtype == 'dprime' & df.res.meta_1$Effect =='Neut_Bad']`]. See Figure \@ref(fig:plot-all-effect) left panel.

## Interaction between valence and self-reference
In this part, we combined the experiments that explicitly manipulated the self-reference and valence, which includes 3a, 3b, 6b, 7a, and 7b. For the positive versus negative contrast, data were from five experiments whith 178 participants; for positive versus neutral and neutral versus negative contrasts, data were from three experiments with 108 participants.

In most of these experiments, the interaction between self-reference and valence was signficant (see results of each experiment in supplementary materials). In the mini-meta-analysis, we analyzed the valence effect for self-referential condition and other-referential condition separately.

For the self-referential condition, we found the same pattern as in the first part of results. That is we found significant differences between positive and neutral as well as positive and negative, but not neutral and negative. The effect size of RT between positive and negative is Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_S']`]; on *d'* was Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_S']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_S']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_S']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_S']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_S']`]. See Figure \@ref(fig:plot-all-effect) the middle panel.

For the other-referential condition, we found that only the difference between positive and negative on RT was significant, all the other conditions were not. The effect size of RT between positive and negative is Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Bad_O']`]; on *d'* was Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Bad_O']`]. The effect was also observed between positive and neutral condition, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Good_Neut_O']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Good_Neut_O']`]. And the difference between neutral and bad conditions are not significant, RT: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'RT' & df.res.meta_2$Effect =='Neut_Bad_O']`]; *d'*: Cohen's *d* = `r df.res.meta_2$Cohen_d[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']` $\pm$ `r df.res.meta_2$se[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']`, 95% CI [`r df.res.meta_2$CI_low[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']` `r df.res.meta_2$CI_upp[df.res.meta_2$DVtype == 'dprime' & df.res.meta_2$Effect =='Neut_Bad_O']`]. See Figure \@ref(fig:plot-all-effect) right panel.

## Generalizibility of the valence effect
In this part, we reported the results from experiment 4 in which either moral valence or self-reference were manipulated as task-irrelevant stimuli. 

```{r analyzing exp4a, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
df.ES_4a <- data.frame(matrix(, nrow=length(unique(df4a.meta.d$ExpID))*length(effectList_2)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df4a.meta.d$ExpID))*length(effectList_2)),
                ExpID  = rep(rep(unique(df4a.meta.d$ExpID), each = length(effectList_2)), 2),
                Effect = rep(effectList_2, length(unique(df4a.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df4a.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df4a.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_2){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Good_Bad_S'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Good_Neut_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Neut_Bad_S'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Bad_O'){
        if (!all(is.na(tmpdata$Identity))){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Good_Neut_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence )
          {
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }
      else if (effectName == 'Neut_Bad_O'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
      }
      
      M1  <- mean(dataCond1$Value) -> df.ES_4a$M1[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_4a$SD1[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_4a$M2[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_4a$SD2[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_4a$N[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_4a$r[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_4a$ES[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] <- tmp2[1,1]
      df.ES_4a$ES.var[df.ES_4a$DVtype == DVName & df.ES_4a$ExpID == expName & df.ES_4a$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_4a <- df.ES_4a %>%
  dplyr::mutate(Identity = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Neut_S" | Effect == "Neut_Bad_S",
                                  "Self-ref.", "Other-ref."),
                EffectType = ifelse(Effect == "Good_Bad_S" | Effect == "Good_Bad_O", "Good_Bad",
                                    ifelse(Effect == "Good_Neut_S" | Effect == "Good_Neut_O", "Good_Neut", "Neut_Bad")),
                Identity = factor(Identity, levels = c("Self-ref.", "Other-ref.")))

p_df_4a <- df.ES_4a %>%
  #dplyr::filter(Identity == "Self") %>%
  ggplot(., aes(x=DVtype, y=ES, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  ggtitle('A: Valence effect') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  apatheme +
  facet_wrap(~Identity, nrow = 1)

#p_df_4a
```

```{r 'plot_exp4a_effect', fig.cap="Effect size (Cohen's *d*) of Valence in Exp4a.", warning=FALSE}
#multiplot(p_meta_val_1, p_meta_val_2_self,p_meta_val_2_other, cols = 3)
p_df_4a
```
For exmperiment 4a, when self-reference was the target and moral valence was task-irrelevant, we found that only under the implicit self-referential condition, i.e., when the moral words were presented as task irrelevant stimuli, there was the main effect of valence and interaction between valence and reference for both *d* prime and RT (See supplementary resuls for the detailed statistics). For *d* prime, we found good-self condition (`r df.ES_4a$M1[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD1[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']`) had higher *d* prime than bad-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Bad_S']`); good self condition was also higher than neutral self (`r df.ES_4a$M2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Neut_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'dprime' & df.ES_4a$Effect == 'Good_Neut_S']`) but there was not statistically significant, while the neutral-self condition was higher than bad self condition and not significant neither. For reaction times, good-self condition (`r df.ES_4a$M1[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD1[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']`) were faster relative to bad-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Bad_S']`), and over neutral-self condition (`r df.ES_4a$M2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Neut_S']` $\pm$ `r df.ES_4a$SD2[df.ES_4a$DVtype == 'RT' & df.ES_4a$Effect == 'Good_Neut_S']`). The difference between neutral-self and bad-self conditions were not significant. However, for the other-referential condition, there was no significant differences between different valence conditions.

```{r analyzing exp4b, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
effectList_3 <- c('Self_Other_G','Self_Other_N', 'Self_Other_B')

df.ES_4b <- data.frame(matrix(, nrow=length(unique(df4b.meta.d$ExpID))*length(effectList_3)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df4b.meta.d$ExpID))*length(effectList_3)),
                ExpID  = rep(rep(unique(df4b.meta.d$ExpID), each = length(effectList_3)), 2),
                Effect = rep(effectList_3, length(unique(df4b.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df4b.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df4b.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_3){
      tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
      
      if (effectName == 'Self_Other_G'){
        
        if (!all(is.na(tmpdata$Identity))){
          #print(paste('processing Good_Bad_S of ', expName, sep = ''))
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Good" & Identity == 'Other')  
          }
        else{
            #print(paste('Skip Good_Bad_S of ', expName, sep = ''))
          next
          }
        }
      else if (effectName == 'Self_Other_N'){
        if (!all(is.na(tmpdata$Identity)) & 'Neutral' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral" & Identity == 'Other')  
          }
        else{
          next
          }
        }  
      else if (effectName == 'Self_Other_B'){
        if (!all(is.na(tmpdata$Identity)) & 'Bad' %in% tmpdata$Valence ){
          dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Self')
          dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad" & Identity == 'Other')  
          }
        else{
          next
          }
        }

      
      M1  <- mean(dataCond1$Value) -> df.ES_4b$M1[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_4b$SD1[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_4b$M2[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_4b$SD2[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_4b$N[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_4b$r[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_4b$ES[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] <- tmp2[1,1]
      df.ES_4b$ES.var[df.ES_4b$DVtype == DVName & df.ES_4b$ExpID == expName & df.ES_4b$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_4b <- df.ES_4b %>%
  dplyr::mutate(Val = ifelse(Effect == "Self_Other_G", "Good",
                                  ifelse(Effect == "Self_Other_N", 'Neutral', 'Bad')),
                EffectType = 'Self_Other',
                Val = factor(Val, levels = c("Good", "Neutral", "Bad")))

p_df_4b_val <- df.ES_4b %>%
  #dplyr::filter(Identity == "Self") %>%
  ggplot(., aes(x=DVtype, y=ES, color=Val, fill=Val)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  ggtitle('Self-ref effect') +
  coord_cartesian(ylim=c(-1.1, 1.1))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  apatheme

p_df_4b_val
```
For experiemnt 4b, when valence was the target and the reference was task-irrelevant, we found a strong valence effect (see supplementary results). In this experiment, the advantage of good-self conition can only be distangled by comparing the self-referential and other-referential conditions while controling the valence condition. We only found this modulation effect on RT. The RT of good-self (`r df.ES_4b$M1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']` $\pm$ `r df.ES_4b$SD1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']`) were faster relative to good-other condition (`r df.ES_4b$M2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']` $\pm$ `r df.ES_4b$SD2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']`), Cohen's *d* = `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G']`, 95% CI[`r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'] -  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'])` `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'] +  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_G'])`]. However, neutral-self (`r df.ES_4b$M1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']` $\pm$ `r df.ES_4b$SD1[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']`) were faster relative to good-other condition (`r df.ES_4b$M2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']` $\pm$ `r df.ES_4b$SD2[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']`), Cohen's *d* = `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N']`, 95% CI[`r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'] -  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'])` `r df.ES_4b$ES[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'] +  1.96*sqrt(df.ES_4b$ES.var[df.ES_4b$DVtype == 'RT' & df.ES_4b$Effect == 'Self_Other_N'])`]. The difference between bad-self and bad-other was not significant. All the differences between self-referential and other-referential were not significant for *d* prime.

## Specificity of valence effect

```{r analyzing exp5, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
effectList_exp5 <- c('Good_Bad_Mrl','Good_Neut_Mrl','Neut_Bad_Mrl',
                     'Good_Bad_BP','Good_Neut_BP','Neut_Bad_BP',
                     'Good_Bad_BS','Good_Neut_BS','Neut_Bad_BS',
                     'Good_Bad_Emo','Good_Neut_Emo','Neut_Bad_Emo')

df.ES_5 <- data.frame(matrix(, nrow=length(unique(df5.meta.d$ExpID))*length(effectList_exp5)*2, ncol=0)) %>%
  dplyr::mutate(DVtype = rep(c('RT', 'dprime'), each = length(unique(df5.meta.d$ExpID))*length(effectList_exp5)),
                ExpID  = rep(rep(unique(df5.meta.d$ExpID), each = length(effectList_exp5)), 2),
                Effect = rep(effectList_exp5, length(unique(df4a.meta.d$ExpID))*2),
                M1 = NA, SD1 = NA, M2 = NA, SD2 = NA, N = NA, r = NA, ES = NA, ES.var = NA)

for (DVName in c('RT','dprime')){
  if (DVName == 'RT'){
    metaData <- df5.meta.rt %>% dplyr::filter(Matchness == "Match") %>% dplyr::rename(Value = RT)
  } else{
    metaData <- df5.meta.d %>% dplyr::rename(Value = dprime)
  }
  
  for (expName in unique(metaData$ExpID)){
    for (effectName in effectList_exp5){
      if (effectName == 'Good_Bad_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_Mrl'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Morality')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
      }
      
      else if (effectName == 'Good_Bad_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_BP'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Person')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
      }
      
      else if (effectName == 'Good_Bad_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_BS'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Scene')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
      }
      
      else if (effectName == 'Good_Bad_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")  
        }
      else if (effectName == 'Good_Neut_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Good")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Neutral")  
        }  
      else if (effectName == 'Neut_Bad_Emo'){
        tmpdata <- metaData %>% dplyr::filter(ExpID == expName & Domain == 'Emotion')
        dataCond1 <- tmpdata %>% dplyr::filter(Valence == "Neutral")
        dataCond2 <- tmpdata %>% dplyr::filter(Valence == "Bad")
        }
      
      M1  <- mean(dataCond1$Value) -> df.ES_5$M1[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      SD1 <- sd(dataCond1$Value)   -> df.ES_5$SD1[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] 
      M2  <- mean(dataCond2$Value) -> df.ES_5$M2[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      SD2 <- sd(dataCond2$Value)   -> df.ES_5$SD2[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      N   <- length(unique(dataCond2$Subject)) -> df.ES_5$N[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName]
      r   <- cor(dataCond1$Value,dataCond2$Value) -> df.ES_5$r[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] 
      tmp2 <- d.sgpp(m.1 = M1, m.2 = M2, sd.1 = SD1, sd.2 = SD2,n = N, r = r)
      df.ES_5$ES[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] <- tmp2[1,1]
      df.ES_5$ES.var[df.ES_5$DVtype == DVName & df.ES_5$ExpID == expName & df.ES_5$Effect == effectName] <- tmp2[1,2]
    }
  }
}

df.ES_5 <- df.ES_5 %>%
  dplyr::mutate(Domain = ifelse(Effect == "Good_Bad_Mrl" | Effect == "Good_Neut_Mrl" | Effect == "Neut_Bad_Mrl",
                                  "Mrl",
                                ifelse(Effect == "Good_Bad_BP" | Effect == "Good_Neut_BP" | Effect == "Neut_Bad_BP",
                                  "AP",
                                  ifelse(Effect == "Good_Bad_BS" | Effect == "Good_Neut_BS" | Effect == "Neut_Bad_BS",
                                  "AS", 'Emo'))),
                Domain = factor(Domain, levels = c("Mrl", "AP", "AS", "Emo")),
                EffectType = ifelse(Effect == "Good_Bad_Mrl" | Effect == "Good_Bad_BP"  | Effect == "Good_Bad_BS"  | Effect == "Good_Bad_Emo", "Pos_Neg",
                                    ifelse(Effect == "Good_Neut_Mrl" | Effect == "Good_Neut_BP" | Effect == "Good_Neut_BS" | Effect == "Good_Neut_Emo", "Pos_Neut", "Neut_Neg")),
                EffectType = factor(EffectType, levels = c("Pos_Neg", "Pos_Neut", "Neut_Neg")))

p_df_5_RT <- df.ES_5 %>%
  #dplyr::filter(DVtype == "RT") %>%
  ggplot(., aes(x=DVtype, y=ES, color=EffectType, fill=EffectType)) + 
  geom_pointrange(aes(ymin=ES - 1.96*sqrt(ES.var), ymax = ES + 1.96*sqrt(ES.var)), 
                  position = position_dodge(width = 0.4),
                  shape=18, size=1) +
  geom_hline(yintercept=0, size=1, color='black') +
  ggtitle('Valence effect across different domains') +
  coord_cartesian(ylim=c(-1.5, 1.5))+
  scale_color_brewer(palette = "Dark2")+
  scale_fill_brewer(palette = "Dark2")+
  ylab(expression(paste("Cohen's ",italic("d"), sep = ' ')))+
  facet_wrap( ~ Domain, ncol = 4) +
  apatheme 
```
In this part, we analyzed the results from experiment 5, which included positive, neutral, and negative valence from four different domains: morality, emotion, aesthetics of human, and aesthetics of scene. We found interaction between valence and domain for both *d* prime and RT (matched trials). A common pattern appeared in all four domains: each domain showed a binary results instead of gradian on both *d* prime and RT. For morality, aesthetics of human, and aesthetics of scene, the positive conditions had advantages over both neutral and negative conditions (greater *d* prime and faster RT), and neutral and negative conditions didn't differ from each other. But for the emotional stimuli, it was the positive and neutral had advantage over negative conditions, while positive and neutral conditions were not significantly different. See supplementary materials for detailed statistics. Also note that the effect size in moral domain is smaller than the aesthetic domains (beauty of people and beauty of scene).

## Correlation analyses
As the reliability of the quesetionnaire can be found in [@Liu_2020_JOPD]. Then we calculated the correlation between the data from behavioral task and the questionnaire data. 

For the behavioral task part, we derived different indices. First, we used the mean and SD of the RT data from each participants of each condition. We included the RT variation because it has been shown to be meaningful as individual differences [Jensen, 1992; Ouyang et al., 2017]. Second, we used drift diffusion model to estimate four parameters of DDM for each participants. Third, we also calculated the differences between different conditions (valence effect: good-self vs. bad-self, good-self vs. neutral-self, bad-self vs. neutral-self; good-other vs. bad-other, good-other vs. neutral-other, bad-other vs. neutral-other; Self-reference effect: good-self vs. good-other, neutral-self vs. neutral-other, bad-self vs. bad-other), as indexed by Cohen's d and se of Cohen's *d*.

The DDM analyses were finished by HDDM, as reported in @Hu_2020_GoodSelf (2020). That is, we used the response code approach, matched response were coded as 1 and mismatched responses were coded as 0. To fully explore all parameters, we allow all four parameters of DDM free to vary. We then extracted the estimation of all the four parameters for each participants for the correlation analyses.

For the questionnaire part, we are most interested in the self-rated distance between different person and self-evaluation related questionnaires: self-esteem, moral-self identity, and moral self-image. Other questionnaires (e.g., personality) were not planned to correlated with behavioral data were not included.

```{r correlation analysis,echo=FALSE, results='hide', warning=FALSE, message=FALSE}
# Get the meta-analytic results:

## exclude the repeating subjects
df1c.meta.d <- df1c.meta.d %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210))
df1c.meta.rt <- df1c.meta.rt %>% dplyr::filter(!Subject %in% c(1206, 1207, 1208, 1210)) # exclude participants who participated exp1a or 1b

df2.meta.d <- df2.meta.d %>% dplyr::filter(Subject > 2000)    # exclude participant from exp 1a
df2.meta.rt <- df2.meta.rt %>% dplyr::filter(Subject > 2000)

df3a.meta.d <- df3a.meta.d %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) # exclude participants from ex1b, 1c, and 2
df3a.meta.rt <- df3a.meta.rt %>% dplyr::filter(!Subject %in% c(3013, 3012, 3043, 3046)) # exclude participants from ex1b, 1c, and 2

df4b.meta.d <- df4b.meta.d %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201))   # exclude participants from ex1b, 1c, and 2
df4b.meta.rt <- df4b.meta.rt %>% dplyr::filter(!Subject %in% c(4210, 4202, 4201)) # exclude participants from ex1b, 1c, and 2

df5.meta.d <- df5.meta.d %>% dplyr::filter(!Subject %in% c(5201))   # exclude participants from ex1b, 1c, and 2
df5.meta.rt <- df5.meta.rt %>% dplyr::filter(!Subject %in% c(5201)) # exclude participants from ex1b, 1c, and 2

df6a.meta.d <- df6a.meta.d %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131))   # exclude participants from ex1b, 1c, and 2
df6a.meta.rt <- df6a.meta.rt %>% dplyr::filter(!Subject %in% c(6118,6119,6122,6123,6131)) # exclude participants from ex1b, 1c, and 2

df6b.meta.d <- df6b.meta.d %>% dplyr::filter(!Subject %in% c(6217))   # exclude participants from ex1b, 1c, and 2
df6b.meta.rt <- df6b.meta.rt %>% dplyr::filter(!Subject %in% c(6217)) # exclude participants from ex1b, 1c, and 2

df7a_m.meta.d <- df7a_m.meta.d %>% dplyr::filter(!Subject %in% c(7020))   # exclude participants from ex1b, 1c, and 2
df7a_m.meta.rt <- df7a_m.meta.rt %>% dplyr::filter(!Subject %in% c(7020)) # exclude participants from ex1b, 1c, and 2

# Combine the data -----
df.meta_d_1 <- rbind(df1a.meta.d, df1b.meta.d, df1c.meta.d, df2.meta.d, df4b.meta.d , df5.meta.d, df6a.meta.d) 
df.meta_rt_1 <- rbind(df1a.meta.rt, df1b.meta.rt, df1c.meta.rt, df2.meta.rt, df4b.meta.rt,df5.meta.rt, df6a.meta.rt)

df.meta_d_2 <- rbind(df3a.meta.d, df3b.meta.d, df6b.meta.d, df7a_m.meta.d, df7b_m.meta.d) 
df.meta_rt_2 <- rbind(df3a.meta.rt, df3b.meta.rt, df6b.meta.rt, df7a_m.meta.rt, df7b_m.meta.rt)
# Prepare the data for meta ----

# get data for valence effect ---- 
## mean RT, SD of RT, and d prime for data without reference
tmp1 <- df.meta_rt_1 %>%
  dplyr::filter(Matchness == "Match" & Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = c(RT, RT_SD))
  
tmp2 <- df.meta_d_1 %>%
  dplyr::filter(Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = dprime) %>%
  dplyr::rename(dprime_Good = Good,
                dprime_Neut = Neutral,
                dprime_Bad = Bad)
  
df.meta_1_wide <- merge(tmp1,tmp2); rm(tmp1,tmp2)

## paramters of HDDM for the data without reference
## read all the file name.
params.list <- list.files(file.path('.', 'HDDM'), pattern = '*_hddm_params.csv')
params.expname <- data.frame(params.list) %>%
  tidyr::separate(params.list, c('expName','B','C'),sep = '_') %>%
  dplyr::select(expName) %>%
  dplyr::pull()

df_hddm_ls_1 <- params.list[c(1:4, 9:10)]

#rm(df_hddm_param_1)
for (indx in 1:6){
  #expName_tmp <- strsplit(df_hddm_ls_1[indx], '[_]')[[1]][1]
  if (indx == 5 ){
    hddm_params_tmp <- read.csv(file.path("HDDM", df_hddm_ls_1[indx]), header = TRUE, sep = ",",
                   stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
      dplyr::filter(domain == "Morality") %>%
      dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val) %>%
      dplyr::select(Subject, Matchness, Valence, knode_name, mean) %>%
      tidyr::drop_na() %>%
      tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
      dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
      tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')  
    
  } else {
    
    hddm_params_tmp <- read.csv(file.path("HDDM", df_hddm_ls_1[indx]), header = TRUE, sep = ",",
                   stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
      dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val) %>%
      dplyr::select(Subject, Matchness, Valence, knode_name, mean) %>%
      tidyr::drop_na() %>%
      tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
      dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
      tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')   # %>%
      #dplyr::mutate(ExpID = 'Exp1a')
  }
  
  if (exists('df_hddm_param_1')) {
    df_hddm_param_1 <- rbind(df_hddm_param_1, hddm_params_tmp) 
  } else {
    df_hddm_param_1 <- hddm_params_tmp
  }
  
}

df.meta_1_wide <- merge(df.meta_1_wide, df_hddm_param_1) %>%
  dplyr::select(-c(Domain, Identity, Matchness))

## Get data for interaction between ID & Val ----
## mean RT, SD of RT, and d prime for data without reference
tmp1 <- df.meta_rt_2 %>%
  dplyr::filter(Matchness == "Match" & Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = c(RT, RT_SD))
  

tmp2 <- df.meta_d_2 %>%
  dplyr::filter(Domain == 'Morality') %>%
  tidyr::pivot_wider(names_from = c(Valence), values_from = dprime) %>%
  dplyr::rename(dprime_Good = Good,
                dprime_Neut = Neutral,
                dprime_Bad = Bad)
  
df.meta_2_wide <- merge(tmp1,tmp2); rm(tmp1,tmp2)

## paramters of HDDM 
## read all the file name.
df_hddm_ls_2 <- params.list[c(5, 6, 11:13)]

#rm(df_hddm_param_2)  # in case the variable exist in the env.
for (indx in 1:5){
  #expName_tmp <- strsplit(df_hddm_ls_1[indx], '[_]')[[1]][1]
  hddm_params_tmp <- read.csv(file.path("HDDM", df_hddm_ls_2[indx]), header = TRUE, sep = ",",
                 stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
    dplyr::rename(Subject = subj_idx, Matchness = match, Valence = val, Identity = id) %>%
    dplyr::select(Subject, Matchness, Identity, Valence, knode_name, mean) %>%
    tidyr::drop_na() %>%
    tidyr::separate(knode_name, into = paste('v', 1:2, sep= '')) %>%
    dplyr::rename(param = v1)  %>% dplyr::filter(Matchness=='Match') %>% dplyr::select(- c(v2, Matchness)) %>%
    tidyr::pivot_wider(., names_from = c('Valence', 'param'), values_from = 'mean')   # %>%
    #dplyr::mutate(ExpID = 'Exp1a')
  if (indx == 4 | indx == 5){
    hddm_params_tmp <- hddm_params_tmp %>%
      dplyr::mutate(Neutral_a = NA,
                    Neutral_t = NA,
                    Neutral_v = NA) %>%
      dplyr::select(Subject, Identity, Bad_a, Good_a, Neutral_a, Bad_v, Good_v, Neutral_v, Bad_t, Good_t,
                    Neutral_t)
  }
  
  if (exists('df_hddm_param_2')) {
    df_hddm_param_2 <- rbind(df_hddm_param_2, hddm_params_tmp) 
  } else {
    df_hddm_param_2 <- hddm_params_tmp
  }
}

df.meta_2_wide <- merge(df.meta_2_wide, df_hddm_param_2) %>%
  dplyr::select(-c(Domain, Matchness))

df.meta_1_wide <- df.meta_1_wide %>%
  dplyr::mutate(Identity = NA) %>%
  dplyr::select(colnames(df.meta_2_wide))

df.meta_all_wide <- df.meta_2_wide %>%
  dplyr::filter(Identity == "Self") %>%
  rbind(df.meta_1_wide, .)

#library(mosaic) # using this library for its derivedFactor function
# prepare questionnare data
#df.scales <- read.csv(here::here("Scale_data", "FADGS_dataset4_1_clean.csv"), header = TRUE, sep = ",",
#                 stringsAsFactors=FALSE,na.strings=c("","NA")) %>%
#  dplyr::mutate(expID = mosaic::derivedFactor("Exp1a" = (expID == "exp1.0"), 
#                                      "Exp1b" = (expID == "exp1.1"),
#                                      "Exp3a" = (expID == "exp3"),
#                                      "Exp3b" = (expID == "exp3.1"),
#                                      "Exp4a" = (expID == "exp4.1"),
#                                      "Exp4b" = (expID == "exp4.2"),
#                                      "Exp5" = (expID == "exp5.2"),
#                                      "Exp6b" = (expID == "exp6.2"),
#                                      "Exp7a" = (expID == "exp7.1"),
#                                      "Exp7b" = (expID == "exp7r"),
#                                      "Exp_dpr" = (expID == "exp6"),
#                                      .method ="first", .default = NA),
#                expID = as.character(expID))

## get the questionnaire names
# Self-esteem
SlfEstNames <- c("SES1","SES2","SES3","SES4","SES5","SES6","SES7","SES8","SES9","SES10")
df.scales %>%
  dplyr::select(SlfEstNames) %>%
  psych::omega(.)

# moral identity
mrlIdNames <- c("morId_1","morId_2","morId_3","morId_4", "morId_5","morId_6",
                "morId_7","morId_8","morId_9","morId_10","morId_11","morId_12",
                "morId_13","morId_14","morId_15","morId_16")
mrlIdIntNames <- c("morId_1","morId_2","morId_5","morId_8", "morId_10","morId_11",
                   "morId_12","morId_13","morId_14")
mrlIdExtNames <- c("morId_3","morId_4", "morId_6", "morId_7", "morId_9","morId_10",
                   "morId_15", "morId_16")

df.scales %>%
  dplyr::select(mrlIdNames) %>%
  psych::omega(.)

# moral self images
mrlslfImgNames <- c("morSlfImg_1","morSlfImg_2","morSlfImg_3","morSlfImg_4",
                    "morSlfImg_5","morSlfImg_6","morSlfImg_7","morSlfImg_8","morSlfImg_9")

df.scales %>%
  dplyr::select(mrlslfImgNames) %>%
  psych::omega(.)

# personal distance
perDistNames <- c("SelfSelf", 
                  "SelfGood_1", "SelfGood_2", "SelfGood_3", "SelfGood_4",
                  "SelfNeut_1", "SelfNeut_2", "SelfNeut_3", "SelfNeut_4",
                  "SelfBad_1",  "SelfBad_2",  "SelfBad_3",  "SelfBad_4",
                  "SelfStra_1", "SelfStra_2", "SelfStra_3", "SelfStra_4",
                  "GoodNeut_1", "GoodNeut_2", "GoodNeut_3", "GoodNeut_4", 
                  "GoodBad_1",  "GoodBad_2",  "GoodBad_3",  "GoodBad_4",
                  "NeutBad_1",  "NeutBad_2",  "NeutBad_3",  "NeutBad_4")

# calculate the average score of each relevant scale
df.q_scores <- df.scales %>%
  dplyr::mutate(SlfEst = rowMeans(.[, SlfEstNames],na.rm = F),
                mrlIdInt = rowMeans(.[, mrlIdIntNames], na.rm = F),
                mrlIdExt = rowMeans(.[, mrlIdExtNames], na.rm = F),
                mrlslfImg = rowMeans(.[, mrlslfImgNames], na.rm = F),
                ) %>%
  dplyr::select(subjID, SlfEst, mrlIdInt, mrlIdExt, mrlslfImg)

df.scales %>%
  dplyr::select(c(expID, subjID),perDistNames) %>%
  head()

# normalize the personal distance, way 1: 
# (1) calculate the mean distance of all distance ratings, grand_mean
# (2) calculate the mean distance of each pair, condition_mean
# (3) normalize the condition_mean: condition_mean/grand_mean
df.perdist <- df.scales %>%
  dplyr::select(c(expID, subjID),perDistNames) %>%
  dplyr::mutate(sumRaw = rowMeans(.[3:31], na.rm = T),
                SelfSelfraw = SelfSelf,
                SelfGoodraw = rowMeans(.[grep("SelfGood", names(.))], na.rm = T),
                SelfNeutraw = rowMeans(.[grep("SelfNeut", names(.))], na.rm = T),
                SelfBadraw  = rowMeans(.[grep("SelfBad", names(.))], na.rm = T),
                SelfStraraw = rowMeans(.[grep("SelfStra", names(.))], na.rm = T),
                GoodNeutraw = rowMeans(.[grep("GoodNeut", names(.))], na.rm = T),
                GoodBadraw  = rowMeans(.[grep("GoodBad", names(.))], na.rm = T),
                NeutBadraw  = rowMeans(.[grep("NeutBad", names(.))], na.rm = T)) %>%
  dplyr::select(expID, subjID, sumRaw, SelfSelfraw, 
                SelfGoodraw, SelfNeutraw, SelfBadraw,
                SelfStraraw, GoodNeutraw, GoodBadraw, NeutBadraw) %>%
  dplyr::mutate(Self_Self = SelfSelfraw/sumRaw,
                Self_Good = SelfGoodraw/sumRaw,
                Self_Neut = SelfNeutraw/sumRaw,
                Self_Bad = SelfBadraw/sumRaw,
                Self_Stra = SelfStraraw/sumRaw, 
                Good_Neut = GoodNeutraw/sumRaw, 
                Good_Bad = GoodBadraw/sumRaw, 
                Neut_Bad = NeutBadraw/sumRaw) %>%
  dplyr::select(subjID, Self_Self, Self_Good, Self_Neut, Self_Bad,
                Self_Stra, Good_Neut, Good_Bad, Neut_Bad) %>%
  dplyr::select(-Self_Stra,-Self_Self)



#------------------- try RSA: start ------------------------------
# select data from exp1a
df.perdist_part1 <- df.perdist %>%
      dplyr::filter(subjID %in% df_moral$Subject)

# select the first participants
# personal distance matrix
#df.perdist_exp1a.sub1_dist0 <- df.perdist_exp1a %>%
#      dplyr::filter(subjID == 1051) %>%
#      dplyr::select(Good_Neut:Neut_Bad) %>%
#      tidyr::pivot_longer(cols = Good_Neut:Neut_Bad, 
#                      names_to = "condition",
#                      values_to = "distance_rate") 

dist_mental <- function(df_perdist_subj){
  df.perdist_subj <- df_perdist_subj %>%
      dplyr::select(Good_Neut:Neut_Bad) %>%
      tidyr::pivot_longer(cols = Good_Neut:Neut_Bad, 
                      names_to = "condition",
                      values_to = "distance_rate") 
}

#tmp <- dist_mental(df.perdist_exp1a[df.perdist_exp1a$subjID == 1051,])

tmp <- df.perdist_part1 %>%
  split(.$subjID) %>%
  map(., dist_mental)  %>%
  dplyr::bind_rows(., .id = "Subject")

# reaction times matrix
#df.rt_exp1a.sub1_dist0 <- df1a.v_meta %>%
#      dplyr::filter(Subject == 1051 & Matchness == 'Match' & ACC == 1) %>%
#      dplyr::mutate(SD_RT = sd(RT)) %>% 
#      dplyr::group_by(Valence, SD_RT) %>%
#      dplyr::summarise(meanRT = mean(RT)) %>%
#      dplyr::ungroup() %>%
#      tidyr::pivot_wider(names_from = Valence, values_from = meanRT) %>%
#      dplyr::mutate(Good_Neut = abs((Neutral - Good)/SD_RT),
#                    Good_Bad = abs((Bad - Good)/SD_RT),
#                    Neut_Bad = abs((Bad - Neutral)/SD_RT)) %>%
#      dplyr::select(Good_Neut:Neut_Bad) %>%
#      tidyr::pivot_longer(cols = Good_Neut:Neut_Bad, 
#                      names_to = "condition",
#                      values_to = "distance_rt") 

dist_rt <- function(df_subj) {
  dist_subj <- df_subj  %>%
      dplyr::filter(Matchness == 'Match' & ACC == 1) %>%
      dplyr::mutate(SD_RT = sd(RT)) %>% 
      dplyr::group_by(Valence, SD_RT) %>%
      dplyr::summarise(meanRT = mean(RT)) %>%
      dplyr::ungroup() %>%
      tidyr::pivot_wider(names_from = Valence, values_from = meanRT) %>%
      dplyr::mutate(Good_Neut = abs((Neutral - Good)/SD_RT),
                    Good_Bad = abs((Bad - Good)/SD_RT),
                    Neut_Bad = abs((Bad - Neutral)/SD_RT)) %>%
      dplyr::select(Good_Neut:Neut_Bad) %>%
      tidyr::pivot_longer(cols = Good_Neut:Neut_Bad, 
                      names_to = "condition",
                      values_to = "distance_rt") 
}

#tmp <- dist_rt(df1a.v_meta[df1a.v_meta$Subject == 1051,])

tmp2 <- df_moral %>%
  split(.$Subject) %>%
  map(., dist_rt) %>%
  dplyr::bind_rows(., .id = "Subject")

tmp3 <- tmp %>%
  dplyr::left_join(., tmp2, by = c('Subject','condition'))  %>%
  dplyr::group_by(Subject) %>%
  dplyr::summarise(cor_coef =  cor(distance_rt, distance_rate)) %>%
  dplyr::ungroup()

hist(tmp3$cor_coef)

rsa_sub1 <- df.rt_exp1a.sub1_dist0 %>%
      dplyr::full_join(., df.perdist_exp1a.sub1_dist0) #%>%
      
tmp <- stats::cor.test(rsa_sub1$distance_rt, rsa_sub1$distance_rate, method = 'pearson')$estimate

# visualization of the graph
df.perdist_exp1a.sub1_dist1 <- df.perdist_exp1a %>%
      dplyr::filter(subjID == 1050) %>%
      dplyr::select(Good_Neut:Neut_Bad) %>%
      tidyr::pivot_longer(cols = Good_Neut:Neut_Bad, 
                      names_to = "condition",
                      values_to = "distance") %>%
      #dplyr::filter(!is.na(distance)) %>%
      dplyr::add_row(condition = c("Good_Good",
                                "Neut_Neut","Bad_Bad")) %>%
      tidyr::separate(condition, c('var1','var2'), sep = '_') 

df.dist.mat1 <- data.frame(matrix(ncol = 3, nrow = 3))
colnames(df.dist.mat1) <- unique(df.perdist_exp1a.sub1_dist1$var1)
rownames(df.dist.mat1) <- unique(df.perdist_exp1a.sub1_dist1$var1)

for (i in 1:3){
  rname = df.perdist_exp1a.sub1_dist1$var1[i]
  cname = df.perdist_exp1a.sub1_dist1$var2[i]
  value = df.perdist_exp1a.sub1_dist1$distance[i]
  df.dist.mat1[rname, cname] = value
  df.dist.mat1[cname, rname] = value
}

library(qgraph)
qgraph::qgraph(df.dist.mat1, layout='spring', vsize=9)
#------------------- try RSA: end --------------------------------


df.perdist.sum_p <- df.perdist %>%
  tidyr::pivot_longer(cols = Self_Good:Neut_Bad, 
                      names_to = "condition",
                      values_to = "distance") %>%
  #dplyr::mutate(condition = factor(condition,
  #                                 levels = c("GoodNeut", "NeutBad", "GoodBad",
  #                                            "SelfNeut", "SelfGood", "SelfBad"))) %>%
  dplyr::filter(!is.na(distance)) %>%
  dplyr::group_by(condition) %>%
  dplyr::summarise_each(funs(mean, sd, se=sd(.)/sqrt(n())), distance) %>%
  dplyr::select(condition, mean) %>%
  dplyr::add_row(condition = c("Self_Self", "Good_Good",
                                "Neut_Neut","Bad_Bad")) %>%
  tidyr::separate(condition, c('var1','var2'), sep = '_')

# create a distance matrix
df.dist.mat <- data.frame(matrix(ncol = 4, nrow = 4))
colnames(df.dist.mat) <- unique(df.perdist.sum_p$var1)
rownames(df.dist.mat) <- unique(df.perdist.sum_p$var1)

for (i in 1:6){
  rname = df.perdist.sum_p$var1[i]
  cname = df.perdist.sum_p$var2[i]
  value = df.perdist.sum_p$mean[i]
  df.dist.mat[rname, cname] = value
  df.dist.mat[cname, rname] = value
}
df.dist.mat[is.na(df.dist.mat)] <- 0

df.dist.mat_t <- 1/df.dist.mat #/max(df.dist.mat)

library(qgraph)
qgraph(df.dist.mat_t, layout='spring', vsize=9)

pd1 <- position_dodge(0.5)
df.perdist %>%
  tidyr::pivot_longer(cols = SelfSelf:NeutBad, 
                      names_to = "condition",
                      values_to = "distance") %>%
  dplyr::mutate(condition = factor(condition,
                                   levels = c("GoodNeut", "NeutBad", "GoodBad",
                                              "SelfSelf", "SelfNeut", "SelfGood", "SelfBad", "SelfStra"))) %>%
  ggplot2::ggplot(., aes(x = condition, y = distance)) +
   geom_point(aes(x = condition, y = distance, group = subjID),   # plot individual points
              colour = "#000000",
              size = 3, shape = 20, alpha = 0.06)+
   geom_line(aes(x = condition, y = distance, group = subjID),         # link individual's points by transparent grey lines
             linetype = 1, size = 0.8, colour = "#000000", alpha = 0.06) +   
   geom_line(data = df.perdist.sum_p, aes(x = condition, # plot the group means  
                                     y = mean, 
                                     #group = Identity, 
                                     #colour = Identity
                                     ), 
             linetype = 1, position = pd1, size = 2)+
   geom_point(data = df.perdist.sum_p, aes(x = condition, # group mean
                                      y = mean, 
                                      #group = Identity, 
                                      #colour = Identity
                                      ), 
              shape = 18, position = pd1, size = 8) +
   geom_errorbar(data = df.perdist.sum_p, aes(x = condition,  # group error bar.
                                         y = mean, #group = Identity, 
                                         #colour = Identity,
                                         ymin = mean - 1.96*se, 
                                         ymax = mean + 1.96*se), 
                 width = .05, position = pd1, size = 2, alpha = 0.75) 

df.perdist.sum2_p <- df.perdist %>%
  dplyr::mutate(Rang_est_err_s = SelfGood + SelfBad - GoodBad,
                Rang_est_err_ns = GoodNeut + NeutBad - GoodBad) %>% #
  dplyr::select(subjID, Rang_est_err_s, Rang_est_err_ns) %>%
  tidyr::pivot_longer(cols = Rang_est_err_s:Rang_est_err_ns, 
                      names_to = "condition",
                      values_to = "distance") %>%
  dplyr::mutate(condition = factor(condition,
                                   levels = c('Rang_est_err_s', 'Rang_est_err_ns'))) %>%
  dplyr::filter(!is.na(distance)) %>%
  dplyr::group_by(condition) %>%
  dplyr::summarise_each(funs(mean, sd, se=sd(.)/sqrt(n())), distance) 

df.perdist %>% 
  dplyr::mutate(Rang_est_err_s = SelfGood + SelfBad - Good)

# intersection between participant from behavioral task and scales and get the data
subj.common <- intersect(df.scales$subjID, unique(df.meta_all_wide$Subject))  # 253

df.scales.v <- df.scales %>% dplyr::filter(subjID %in% subj.common) %>%
  dplyr::select_if(~sum(!is.na(.)) > 0) # remove columns that only have NA.

df.q_scores.v <- merge(df.q_scores, df.perdist)

## calculate correlation ----
df.corr <- merge(df.q_scores.v, df.meta_all_wide, by.x = 'subjID', by.y = 'Subject') %>%
  dplyr::select(-c(Identity, ExpID, Site, Age, Sex)) %>%
  #dplyr::select(-c(SelfSelf, SelfStra)) %>%
  #dplyr::select(1:5, 8,6,7,10,9,11, 18:20, 24:26, 27:29, 12:17, 21:23) %>%
  dplyr::na_if("NaN")

#library(corrr)
res.cor <- df.corr %>%
  dplyr::select(-c(subjID)) %>%
  as.matrix(.) %>%
  #dplyr::select_if(~sum(!is.na(.)) > 0) %>%
  Hmisc::rcorr(.)

corrplot::corrplot(res.cor$r, method="color", sig.level = .25, order = "FPC")

flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

cor_pairs <- flattenCorrMatrix(res.cor$r, res.cor$P) %>%
  dplyr::filter(row %in% colnames(df.q_scores.v)) %>%
  dplyr::filter(column %in% colnames(df.meta_all_wide[7:24])) %>%
  dplyr::filter(p <= 0.05) %>%
  dplyr::arrange(row, p)

write.csv(cor_pairs, 'sig_behav_quest_corr_pairs.csv', row.names = F)

```
We found that data from behavioral task are closely related, but not with self-reported questionnaire data.

# Discussion

# References
```{r create_r-references, echo=FALSE,results='hide'}
#r_refs(file = "r-references.bib"))
```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
